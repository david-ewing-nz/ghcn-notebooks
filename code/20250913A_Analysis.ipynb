{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "017705b0-281d-4565-abe1-852ebc5ca888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "global azure_account_name\n",
    "global azure_data_container_name\n",
    "global azure_user_container_name\n",
    "global azure_user_token\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "azure_user_token = r\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(f\"fs.azure.sas.{azure_user_container_name}.{azure_account_name}.blob.core.windows.net\",  azure_user_token)\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bbb004-5a22-4f65-b400-cb2bc3edf217",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e2f6d-f8f0-4150-8eec-e5bb2b0471e4",
   "metadata": {},
   "source": [
    "### Assignment 1 ###\n",
    "\n",
    "The code below demonstrates how to explore and load the data provided for the assignment from Azure Blob Storage and how to save any outputs that you generate to a separate user container.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data provided for the assignment is stored in Azure Blob Storage and outputs that you generate will be stored in Azure Blob Storage as well. Hadoop and Spark can both interact with Azure Blob Storage similar to how they interact with HDFS, but where the replication and distribution is handled by Azure instead. This makes it possible to read or write data in Azure over HTTPS where the path is prefixed by `wasbs://`.\n",
    "- There are two containers, one for the data which is read only and one for any outputs that you generate,\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/`\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`\n",
    "- You can use variable interpolation to insert your global username variable into paths automatically.\n",
    "  - This works for bash commands as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06343f5-a638-4f7f-ade5-5f564a068bc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/09/13 11:12:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/13 11:12:49 WARN Utils: Service 'sparkDriver' could not bind on port 7077. Attempting port 7078.\n",
      "25/09/13 11:12:49 WARN Utils: Service 'sparkDriver' could not bind on port 7078. Attempting port 7079.\n",
      "25/09/13 11:12:49 WARN Utils: Service 'sparkDriver' could not bind on port 7079. Attempting port 7080.\n",
      "25/09/13 11:12:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/09/13 11:12:49 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/09/13 11:12:49 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4043\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7080</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>32</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1757718768977</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1757718768874</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16</td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-177cb8984ecb4bf4b815d24b2e8d18ea</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-3acf44994033f08a</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=4, executor_cores=2, worker_memory=4, master_memory=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50828b-693b-4ed8-8082-e9590200210f",
   "metadata": {},
   "source": [
    "## NOTE  ##\n",
    "```\n",
    "The following three cell was lifted from the\n",
    "Process notebook for consistancy. It is\n",
    "unknown at the moment if they can be placed\n",
    "in a py file and included like a header file\n",
    "but we will find out. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8364b3d1-4ab6-40a8-a5c1-58268424bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "import math, os, platform, re\n",
    "import subprocess, sys, time\n",
    "\n",
    "import itertools         as it\n",
    "import matplotlib.dates  as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from time                import perf_counter\n",
    "from IPython.display     import display  # calls between environments\n",
    "from math                import acos, atan2, cos, radians, sin, sqrt\n",
    "from matplotlib.ticker   import FuncFormatter, MaxNLocator\n",
    "from pathlib             import Path\n",
    "from pyspark.sql         import DataFrame\n",
    "from pyspark.sql         import DataFrame as SparkDF\n",
    "from pyspark.sql         import functions as F, types as T\n",
    "from pyspark.sql.types   import *\n",
    "from pyspark.sql.utils   import AnalysisException\n",
    "from pyspark.sql.window  import Window\n",
    "from typing              import List, Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56336272-f34c-4ac6-9583-6d8b7d0df230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER AND DIAGNOSTIC FUNCTIONS\n",
    "notebook_run_time = time.time()\n",
    "\n",
    "def df_as_html(df, n: int = 5, right_align: bool = False, show_index: bool = False):\n",
    "    \"\"\"\n",
    "    HTML preview via pandas with no truncation. If right_align=True,\n",
    "    only numeric columns are right-justified; everything else is \n",
    "    explicitly left-aligned.\n",
    "    \"\"\"\n",
    "    \n",
    "    pdf = df.limit(n).toPandas()\n",
    "    print(\"[INFO] Converting Spark → pandas for HTML display (rows:\", len(pdf), \")\")\n",
    "    print(\"[INFO] right_align (numeric columns):\", right_align)\n",
    "\n",
    "    with pd.option_context(\n",
    "        \"display.max_colwidth\", None,   \n",
    "        \"display.max_columns\", None,    \n",
    "        \"display.width\", None            \n",
    "    ):\n",
    "        styler = pdf.style if show_index else pdf.style.hide(axis=\"index\")\n",
    "\n",
    "        #   table alignment: left for both headers and cells\n",
    "        styler = styler.set_table_styles(\n",
    "            [\n",
    "                {\"selector\": \"th\", \"props\": [(\"text-align\", \"left\")]},\n",
    "                {\"selector\": \"td\", \"props\": [(\"text-align\", \"left\")]},\n",
    "            ],\n",
    "            overwrite=True,  # make this the baseline\n",
    "        )\n",
    "         \n",
    "        if right_align:\n",
    "            numeric_cols = list(pdf.select_dtypes(include=[\"number\"]).columns)\n",
    "            print(\"[INFO] Right-aligning numeric columns:\", numeric_cols)\n",
    "            if numeric_cols:\n",
    "                styler = styler.set_properties(subset=numeric_cols,\n",
    "                                               **{\"text-align\": \"right\"})\n",
    "        display(styler)\n",
    "\n",
    "def _normalise_dir(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure trailing slash so we point to\n",
    "    the dataset directory (not a file)\n",
    "    \"\"\"\n",
    "    return s if s.endswith(\"/\") else s + \"/\"\n",
    "\n",
    "def ensure_dir(path: str) -> str:\n",
    "    \"\"\"\n",
    "    treat Parquet datasets as directories;\n",
    "    add trailing slash if needed\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        raise ValueError(\"Path is None\")\n",
    "    path = _normalise_dir(path)\n",
    "    print(\"ensure_dir -> \",path)\n",
    "    return path\n",
    "    \n",
    "def show_df(df, n: int = 10, name: str = \"\", right_align: bool = False):\n",
    "    \"\"\"\n",
    "    Print schema, \n",
    "    show an HTML sample,\n",
    "    and row count.\n",
    "    \"\"\"\n",
    "    print(\"_\"*70)\n",
    "    print(\"name : \",name)\n",
    "    df.printSchema()\n",
    "    print(\"[check] sample:\")\n",
    "    df_as_html(df, n=n, right_align=right_align)\n",
    "  \n",
    "def write_parquet(df, dir_as_path: str, df_name:str = \"\"):    \n",
    "    funct_time = time.time()\n",
    "    path = _normalise_dir(dir_as_path)\n",
    "    print(f\"[file] write_parquet  : {path}\")\n",
    "    try:      \n",
    "        show_df(df,df_name)\n",
    "    except Exception as e:\n",
    "        print(\"[cathch] sample failed:\", e)\n",
    "        os.system(f'hdfs dfs -rm -r -f \"{path}\"')   # idempotent cleanup\n",
    "    df.write.mode(\"overwrite\").format(\"parquet\").save(path)\n",
    "    os.system(f'hdfs dfs -ls -R \"{path}\"')\n",
    "    funct_time = time.time() - funct_time \n",
    "    print(f\"[time] write_parquet (min)   : {funct_time/60:5.2f}\")\n",
    "    print(f\"[time] write_parquet (sec)   : {funct_time:5.2f}\")\n",
    "  \n",
    "def has_parquet(dir_as_path: str) -> bool:\n",
    "    path   = _normalise_dir( dir_as_path)\n",
    "    marker = path + '_SUCCESS'\n",
    "    #print(\"\\n[check] dir_path:\", dir_path)\n",
    "    #print(\"\\n[check] path    :\", path)\n",
    "    print(\"\\n[check] marker  :\", marker)\n",
    "    rc = os.system(f'hdfs dfs -test -e \"{marker}\"')\n",
    "    print(\"[check] rc:\", rc, \"->\", (\"exists\" if rc == 0 else \"missing\"))\n",
    "    return (rc == 0)\n",
    "    \n",
    "def _to_spark(df_like, schema=None):\n",
    "    \"\"\"\n",
    "    Return a Spark DataFrame  .\n",
    "    \"\"\"\n",
    "    if isinstance(df_like, SparkDF):\n",
    "        return df_like\n",
    "    return spark.createDataFrame(df_like, schema=schema) if schema else spark.createDataFrame(df_like)\n",
    "\n",
    "def _success_exists(target_dir: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check for the Hadoop/Spark _SUCCESS marker;  \n",
    "    \"\"\"\n",
    "    jvm = spark._jvm\n",
    "    hconf = spark._jsc.hadoopConfiguration()\n",
    "    try:\n",
    "        uri = jvm.java.net.URI(target_dir)\n",
    "        fs = jvm.org.apache.hadoop.fs.FileSystem.get(uri, hconf)\n",
    "        success = jvm.org.apache.hadoop.fs.Path(target_dir + \"_SUCCESS\")\n",
    "        exists = fs.exists(success)\n",
    "        print(f\"[status] _SUCCESS check at: {target_dir}_SUCCESS -> {exists}\")\n",
    "        return bool(exists)\n",
    "    except Exception as e:\n",
    "        print(f\"[status] _SUCCESS check failed ({e}); attempting read-probe …\")\n",
    "        try:\n",
    "            spark.read.parquet(target_dir).limit(1).count()\n",
    "            print(f\"[dewstatus59] read-probe succeeded at: {target_dir}\")\n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"[status] read-probe failed ({e2}); treating as not existing.\")\n",
    "            return False\n",
    " \n",
    "\n",
    "def normalise_ids(df: DataFrame, col: str = \"ID\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    # Single source of truth for ID normalisation \n",
    "    Upper + trim + distinct on the given ID column.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] normalise_ids() on column: {col}\")\n",
    "    return df.select(F.upper(F.trim(F.col(col))).alias(\"ID\")).distinct()\n",
    "\n",
    "# hack \n",
    "_ids       = normalise_ids\n",
    "canon_ids  = normalise_ids\n",
    "_canon_ids = normalise_ids\n",
    " \n",
    "def probe_universe(daily_df, stations_df, inv_agg_df, tag=\"\"):\n",
    "    \"\"\"\n",
    "    DIAGNOSTIC\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"_\"*70)\n",
    "    print(f\"[PROBE] Station universe check :: {tag}\")\n",
    "    daily_ids   = _ids(daily_df)\n",
    "    station_ids = _ids(stations_df)\n",
    "    inv_ids     = _ids(inv_agg_df)\n",
    "    print(\"[COUNT] daily IDs         :\", daily_ids.count())\n",
    "    print(\"[COUNT] station IDs (cat) :\", station_ids.count())\n",
    "    print(\"[COUNT] inventory IDs     :\", inv_ids.count())\n",
    "    print(\"[DIFF ] daily  – station  :\", daily_ids.join(station_ids, \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] station – daily   :\", station_ids.join(daily_ids, \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] station – inv     :\", station_ids.join(inv_ids, \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] inv     – daily   :\", inv_ids.join(daily_ids, \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] inv     – station :\", inv_ids.join(station_ids, \"ID\", \"left_anti\").count())\n",
    "    print(\"_\"*70)\n",
    " \n",
    "\n",
    "def _count_unique_ids(df: DataFrame) -> int:\n",
    "    return normalise_ids(df).count()\n",
    "\n",
    "def pick_unfiltered_daily(preferred_path: str = None) -> DataFrame:\n",
    "    \"\"\"Return an unfiltered daily DF (~129k unique station IDs).\"\"\"\n",
    "    cand_names = [\"daily\", \"read_daily\", \"daily_df\", \"daily_all\", \"ghcnd_daily\"]\n",
    "    print(\"[INFO] Candidate DataFrames:\", [n for n in cand_names if n in globals()])\n",
    "    for name in cand_names:\n",
    "        obj = globals().get(name)\n",
    "        if isinstance(obj, DataFrame):\n",
    "            try:\n",
    "                n = normalise_ids(obj).count()\n",
    "                print(f\"[CHECK] {name} unique station IDs:\", n)\n",
    "                if n >= 120_000:\n",
    "                    print(f\"[INFO] Using {name} as the unfiltered daily.\")\n",
    "                    return obj\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not inspect {name}:\", repr(e))\n",
    "    if preferred_path:\n",
    "        print(f\"[INFO] Trying preferred_path: {preferred_path}\")\n",
    "        df = spark.read.parquet(str(preferred_path))\n",
    "        n = normalise_ids(df).count()\n",
    "        print(\"[CHECK] preferred_path unique station IDs:\", n)\n",
    "        if n >= 120_000:\n",
    "            print(\"[INFO] Using preferred_path as the unfiltered daily.\")\n",
    "            return df\n",
    "    for var in [\"DAILY_READ_NAME\",\"DAILY_WRITE_NAME\",\"daily_read_name\",\"daily_write_name\",\"DAILY_NAME\"]:\n",
    "        if var in globals():\n",
    "            path = globals()[var]\n",
    "            try:\n",
    "                print(f\"[INFO] Trying {var} = {path}\")\n",
    "                df = spark.read.parquet(str(path))\n",
    "                n = normalise_ids(df).count()\n",
    "                print(f\"[CHECK] {var} unique station IDs:\", n)\n",
    "                if n >= 120_000:\n",
    "                    print(f\"[INFO] Using {var} as the unfiltered daily.\")\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not read {var}:\", repr(e))\n",
    "    raise SystemExit(\"[FATAL] Could not find an unfiltered daily dataset (expected ~129k unique station IDs).\")\n",
    "\n",
    "def bprint(text: str=\"\", l=50):\n",
    "    n = len(text)\n",
    "    n = abs(n - l)//2\n",
    "    \n",
    "    print(\"\\n\" + \"_\" * n + text + \"_\" * n)\n",
    "\n",
    "# Back-compat aliases hack to account for non-disciplined naming\n",
    "_ids       = normalise_ids\n",
    "canon_ids  = normalise_ids\n",
    "_canon_ids = normalise_ids\n",
    "\n",
    "#print(\"[TEST] Using _canon_ids:\", _canon_ids(stations).count())\n",
    "#print(\"[TEST] Using canon_ids :\", canon_ids(stations).count())\n",
    "#print(\"[TEST] Using _ids      :\", _ids(stations).count())\n",
    "\n",
    "\n",
    "# : pairwise city distances in km using Spark built-ins \n",
    "def pairwise_city_distances_spark(cities, radius_km=6371.0):\n",
    "    \"\"\"\n",
    "    cities: list[tuple[str, float, float]] -> [(name, lat_deg, lon_deg), ...]\n",
    "    returns: Spark DataFrame with columns:\n",
    "             city_a, city_b, haversine_km, slc_km, delta_km, delta_pct\n",
    "    \"\"\"\n",
    "  #  from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        raise RuntimeError(\"No active Spark session.\")\n",
    "\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"city\", T.StringType(), False),\n",
    "        T.StructField(\"lat\",  T.DoubleType(), False),\n",
    "        T.StructField(\"lon\",  T.DoubleType(), False),\n",
    "        ])\n",
    "    df = spark.createDataFrame(cities, schema)\n",
    "\n",
    "    a, b = df.alias(\"a\"), df.alias(\"b\")\n",
    "    pairs = (a.join(b, F.col(\"a.city\") < F.col(\"b.city\"))\n",
    "               .select(F.col(\"a.city\").alias(\"city_a\"),\n",
    "                       F.col(\"b.city\").alias(\"city_b\"),\n",
    "                       F.col(\"a.lat\").alias(\"lat1\"),\n",
    "                       F.col(\"a.lon\").alias(\"lon1\"),\n",
    "                       F.col(\"b.lat\").alias(\"lat2\"),\n",
    "                       F.col(\"b.lon\").alias(\"lon2\")))\n",
    "\n",
    "    R = F.lit(float(radius_km))\n",
    "    lat1 = F.radians(F.col(\"lat1\"));  lat2 = F.radians(F.col(\"lat2\"))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = F.radians(F.col(\"lon2\") - F.col(\"lon1\"))\n",
    "\n",
    "    a_term = F.sin(dlat/2)**2 + F.cos(lat1)*F.cos(lat2)*F.sin(dlon/2)**2\n",
    "    c_term = 2*F.atan2(F.sqrt(a_term), F.sqrt(1 - a_term))\n",
    "    hav_km = R * c_term\n",
    "\n",
    "    cos_val = F.sin(lat1)*F.sin(lat2) + F.cos(lat1)*F.cos(lat2)*F.cos(dlon)\n",
    "    cos_val = F.greatest(F.lit(-1.0), F.least(F.lit(1.0), cos_val))\n",
    "    slc_km = R * F.acos(cos_val)\n",
    "\n",
    "    delta_km  = F.abs(hav_km - slc_km)\n",
    "    delta_pct = F.when(hav_km == 0, F.lit(0.0)).otherwise(delta_km / hav_km * 100.0)\n",
    "\n",
    "    out_df = (pairs\n",
    "              .withColumn(\"haversine_km\", F.round(hav_km, 2))\n",
    "              .withColumn(\"slc_km\",       F.round(slc_km, 2))\n",
    "              .withColumn(\"delta_km\",     F.round(delta_km, 4))\n",
    "              .withColumn(\"delta_pct\",    F.round(delta_pct, 6))\n",
    "              .select(\"city_a\", \"city_b\", \"haversine_km\", \"slc_km\", \"delta_km\", \"delta_pct\")\n",
    "              .orderBy(\"haversine_km\"))\n",
    "    return out_df\n",
    "\n",
    "\n",
    "# --- Timing helpers for Spark & pure Python (no extra deps)\n",
    " \n",
    "\n",
    "def benchmark_python_distances(cities, radius_km=6371.0, repeats=50000):\n",
    "    \"\"\"\n",
    "    cities: [(name, lat_deg, lon_deg), ...]  (3 cities => 3 pairs)\n",
    "    repeats: loop count to make timings stable\n",
    "    returns: dict with seconds for haversine/slc\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for i in range(len(cities)):\n",
    "        for j in range(i+1, len(cities)):\n",
    "            (_, lat1, lon1), (_, lat2, lon2) = cities[i], cities[j]\n",
    "            pairs.append((lat1, lon1, lat2, lon2))\n",
    "\n",
    "    # haversine\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        for lat1, lon1, lat2, lon2 in pairs:\n",
    "            φ1, λ1, φ2, λ2 = map(radians, (lat1, lon1, lat2, lon2))\n",
    "            dφ, dλ = (φ2 - φ1), (λ2 - λ1)\n",
    "            a = sin(dφ/2)**2 + cos(φ1)*cos(φ2)*sin(dλ/2)**2\n",
    "            c = 2*atan2(sqrt(a), sqrt(1 - a))\n",
    "            _ = radius_km * c\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # spherical law of cosines (SLC)\n",
    "    t2 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        for lat1, lon1, lat2, lon2 in pairs:\n",
    "            φ1, λ1, φ2, λ2 = map(radians, (lat1, lon1, lat2, lon2))\n",
    "            cosv = sin(φ1)*sin(φ2) + cos(φ1)*cos(φ2)*cos(λ2 - λ1)\n",
    "            cosv = max(-1.0, min(1.0, cosv))\n",
    "            _ = radius_km * acos(cosv)\n",
    "    t3 = perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"python_haversine_sec\": t1 - t0,\n",
    "        \"python_slc_sec\":       t3 - t2,\n",
    "        \"repeats\": repeats,\n",
    "        \"pairs\": len(pairs),\n",
    "    }\n",
    "\n",
    "\n",
    "def _parse_ls_bytes(line): \n",
    "    parts = line.split()\n",
    "    if len(parts) < 8:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[4])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def _parse_du_bytes(line):\n",
    "    parts = line.split()\n",
    "    if len(parts) < 2:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[0])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def du_bytes(path):\n",
    "    lines = get_ipython().getoutput(f'hdfs dfs -du \"{path}\"')\n",
    "    total = 0\n",
    "    for ln in lines:\n",
    "        parts = ln.split()\n",
    "        if len(parts) >= 2:\n",
    "            try:\n",
    "                total += int(parts[0])\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return total\n",
    "    \n",
    "def benchmark_spark_distances(cities, radius_km=6368.6, repeats=3):\n",
    "    \"\"\"\n",
    "    Uses Spark built-ins only. Measures full execution\n",
    "    time by forcing an action.\n",
    "    \n",
    "    returns: dict with seconds for haversine/slc and\n",
    "    row counts used.\n",
    "    \n",
    "    For the radius:\n",
    "    \n",
    "    The Earth is slightly flattened, so the geocentric \n",
    "    radius depends on latitude.  For context: \n",
    "    \n",
    "    * equatorial radius = 6,378.137 km; \n",
    "    * polar radius      = 6,356.752 km \n",
    "    \n",
    "    Across New Zealand’s latitudes (≈36–47°S), using the\n",
    "    WGS-84 ellipsoid, you get roughly:\n",
    "\n",
    "    Auckland (37°S):       ~6,370.4 km\n",
    "    Wellington (41°S):     ~6,369.0 km\n",
    "    Christchurch (43.5°S): ~6,368.0 km\n",
    "    Dunedin (45.9°S):      ~6,367.2 km\n",
    "    __________________________________\n",
    "    mean                  ≈ 6,368.6 km\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        from pyspark.sql import SparkSession, functions as F, types as T\n",
    "    except Exception:\n",
    "        return None  # no Spark therefore save cannot run in vs code\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        return None\n",
    "\n",
    "    # build pairs once and cache\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"city\", T.StringType(), False),\n",
    "        T.StructField(\"lat\",  T.DoubleType(), False),\n",
    "        T.StructField(\"lon\",  T.DoubleType(), False),\n",
    "    ])\n",
    "    df = spark.createDataFrame(cities, schema)\n",
    "    a, b = df.alias(\"a\"), df.alias(\"b\")\n",
    "    pairs = (a.join(b, F.col(\"a.city\") < F.col(\"b.city\"))\n",
    "               .select(F.col(\"a.lat\").alias(\"lat1\"),\n",
    "                       F.col(\"a.lon\").alias(\"lon1\"),\n",
    "                       F.col(\"b.lat\").alias(\"lat2\"),\n",
    "                       F.col(\"b.lon\").alias(\"lon2\"))\n",
    "               .cache())\n",
    "    _ = pairs.count()\n",
    "\n",
    "    R = F.lit(float(radius_km))\n",
    "    lat1 = F.radians(F.col(\"lat1\")); lat2 = F.radians(F.col(\"lat2\"))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = F.radians(F.col(\"lon2\") - F.col(\"lon1\"))\n",
    "\n",
    "    # Haversine expr\n",
    "    a_term = F.sin(dlat/2)**2 + F.cos(lat1)*F.cos(lat2)*F.sin(dlon/2)**2\n",
    "    c_term = 2*F.atan2(F.sqrt(a_term), F.sqrt(1 - a_term))\n",
    "    hav = R * c_term\n",
    "\n",
    "    # SLC expr\n",
    "    cosv = F.sin(lat1)*F.sin(lat2) + F.cos(lat1)*F.cos(lat2)*F.cos(dlon)\n",
    "    cosv = F.greatest(F.lit(-1.0), F.least(F.lit(1.0), cosv))\n",
    "    slc = R * F.acos(cosv)\n",
    "\n",
    "    # time Haversine\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(hav.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # time SLC\n",
    "    t2 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(slc.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t3 = perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"spark_pairs\": pairs.count(),\n",
    "        \"spark_repeats\": repeats,\n",
    "        \"spark_haversine_sec\": t1 - t0,\n",
    "        \"spark_slc_sec\":       t3 - t2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7512b8d8-2863-4c32-8457-78a8d3750f61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 2]"
     ]
    }
   ],
   "source": [
    "# overall time metric\n",
    "notebook_run_time = time.time()\n",
    "val               = spark.range(1).select(F.date_format(F.current_timestamp(), 'yyyy.MM.dd HH:mm').alias('t')).first()['t']\n",
    "bprint()\n",
    "print(f\"[time] current time           :  {val}\")\n",
    "#bprint()\n",
    "\n",
    "\n",
    "bprint(\"ENVIRONMENT\") \n",
    "print(\"Spark       :\", spark.version)\n",
    "print(\"Python tuple:\", sys.version_info[:3]) \n",
    "print(\"username    :\", username)\n",
    "print()\n",
    "\n",
    "bprint(\"DEEBUG LOGICALS\")\n",
    "#FORCE_OVERWRITE = False  # False means that if the file exists then we wont re-write it \n",
    "#FORCE_OVERWRITE = True   # True means overwrite all resultant files\n",
    "FORCE_REBUILD_ENRICHED  = True   #has_parquet(enriched_write_name)\n",
    "FORCE_REBUILD_STATIONS  = True    #has_parquet(stations_write_name)\n",
    "FORCE_REBUILD_INVENTORY = True    # has_parquet(inventory_write_name)\n",
    "FORCE_REBUILD_STATES    = True    #has_parquet(states_write_name)\n",
    "FORCE_REBUILD_COUNTRIES = True    #has_parquet(countries_write_name)\n",
    "\n",
    "FORCE_REBUILD_OVERLAP   = True    #has_parquet(overlap_write_name)\n",
    "FORCE_REBUILD_PRECIP    = True    #has_parquet(precip_write_path)\n",
    "\n",
    "print(f\"[status] FORCE_REBUILD_ENRICHED  :\", FORCE_REBUILD_ENRICHED)\n",
    "print(f\"[status] FORCE_REBUILD_STATIONS  :\", FORCE_REBUILD_STATIONS)\n",
    "print(f\"[status] FORCE_REBUILD_INVENTORY :\", FORCE_REBUILD_INVENTORY)\n",
    "print(f\"[status] FORCE_REBUILD_STATES    :\", FORCE_REBUILD_STATES)\n",
    "print(f\"[status] FORCE_REBUILD_COUNTRIES :\", FORCE_REBUILD_COUNTRIES)\n",
    "\n",
    "print(f\"[status] FORCE_REBUILD_OVERLAP   :\", FORCE_REBUILD_OVERLAP)\n",
    "print(f\"[status] FORCE_REBUILD_PRECIP    :\", FORCE_REBUILD_PRECIP)\n",
    "\n",
    "\n",
    "azure_account_name        = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "previous_year             = 2024  # full hear\n",
    "most_recent_year          = 2025  # currently building\n",
    "\n",
    "data_root      = f\"wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/\"\n",
    "user_root      = f\"wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/{username}/\"\n",
    " \n",
    "data_root      = ensure_dir(data_root)\n",
    "user_root      = ensure_dir(user_root) \n",
    "\n",
    "\n",
    "bprint(\"SOURCE FOLDERS\")\n",
    "print()\n",
    "print(\"data_root           :\", data_root) \n",
    "print(\"user_root           :\", user_root)\n",
    "bprint()\n",
    "\n",
    "daily_root     = ensure_dir(f\"{data_root}daily/\")\n",
    "\n",
    "print(\"daily_root          :\", daily_root)\n",
    "print()\n",
    "report_root    = ensure_dir(f\"{user_root}reports/\")\n",
    "image_root     = ensure_dir(f\"{data_root}images/\")\n",
    "figs_dir       = \"figures/\"\n",
    " \n",
    "print(\"report_root         :\", report_root)\n",
    "print(\"figs_dir (relative) :\", figs_dir)\n",
    "print(\"image_root          :\", image_root)\n",
    "\n",
    "bprint(\"SOURCE FILES\")\n",
    "stations_read_name   = f'{data_root}ghcnd-stations.txt'\n",
    "inventory_read_name  = f'{data_root}ghcnd-inventory.txt'\n",
    "countries_read_name  = f'{data_root}ghcnd-countries.txt'\n",
    "states_read_name     = f'{data_root}ghcnd-states.txt'\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"stations_read_name  :\", stations_read_name)\n",
    "print(\"inventory_read_name :\", inventory_read_name)\n",
    "print(\"countries_read_name :\", countries_read_name)\n",
    "print(\"states_read_name    :\", states_read_name)\n",
    "\n",
    "previous_csvgz_path  = f'{daily_root}2024.csv.gz' \n",
    "current_csvgz_path   = f'{daily_root}2025.csv.gz' \n",
    "\n",
    "print()\n",
    "print(\"previous_csvgz_path  :\", previous_csvgz_path)\n",
    "print(\"current_csvgz_path   :\", current_csvgz_path)\n",
    "\n",
    "bprint(\"USER FOLDERS\")\n",
    "  \n",
    "stations_write_name  =  ensure_dir(f'{user_root}stations.parquet')      #parquest file referenced by folder\n",
    "inventory_write_name =  ensure_dir(f'{user_root}inventory.parquet')\n",
    "countries_write_name =  ensure_dir(f'{user_root}countries.parquet')\n",
    "states_write_name    =  ensure_dir(f'{user_root}states.parquet')\n",
    "\n",
    "print()\n",
    "print(\"stations_write_name :\", stations_write_name)\n",
    "print(\"inventory_write_name :\", inventory_write_name)\n",
    "print(\"countries_write_name:\", countries_write_name)\n",
    "print(\"states_write_name   :\", states_write_name)\n",
    "\n",
    "#overlap_write_pathh  = ensure_dir(f'{user_root}q1b32_overlap_counts.parquet')\n",
    "#precip_write_path    = ensure_dir(f'{user_root}q2a-agg-precipitation.parquet')\n",
    " \n",
    "\n",
    "enriched_write_name  = ensure_dir(f\"{user_root}enriched_stations.parquet\" )\n",
    "station_date_element = ensure_dir(f\"{user_root}q2a_station_date_element.parquet\")\n",
    "overlap_counts_name  = ensure_dir(f'{user_root}q1b32_overlap_counts.parquet')\n",
    "overlap_write_name   = ensure_dir(f'{user_root}q1b32_overlap_counts.parquet')\n",
    "precip_write_name    = ensure_dir(f'{user_root}q2a-agg-precipitation.parquet') \n",
    "\n",
    "print()\n",
    "print(\"enriched_write_name :\", enriched_write_name)\n",
    "print(\"stations_write_name :\", stations_write_name)\n",
    "print(\"overlap_counts_name :\", overlap_counts_name)\n",
    "print(\"overlap_write_name  :\", overlap_write_name)\n",
    "print(\"precip_write_name   :\", precip_write_name) \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586fca6e-683f-4426-b354-175f8eeb5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q1(a)0\")\n",
    "cell_time = time.time()\n",
    " \n",
    "print(f\"[check] user_root: {user_root}\")   \n",
    "print(f\"[check] enriched : {enriched_write_name}\") \n",
    "\n",
    "path = str(enriched_write_name)\n",
    "print(\"[CHECK] Verifying HDFS path exists:\", path)\n",
    "\n",
    "try:\n",
    "    # -test -e returns non-zero if the file/dir does not exist\n",
    "    subprocess.run([\"hdfs\", \"dfs\", \"-test\", \"-e\", user_root], check=True)\n",
    "    print(\"[CHECK] Path exists.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"[ERROR] HDFS path missing or inaccessible. Return code:\", e.returncode)\n",
    "    # Optional: show a listing attempt for context\n",
    "    subprocess.run([\"hdfs\", \"dfs\", \"-ls\", \"-h\", path], check=False)\n",
    "    raise SystemExit(\"[FATAL] Aborting notebook: required dataset not found.\")\n",
    "\n",
    "!hdfs dfs -du -s -h {enriched_write_name} \n",
    "!hdfs dfs -ls    -h {enriched_write_name}   \n",
    "enriched = spark.read.parquet(enriched_write_name).cache()\n",
    "print(\"[check] enriched rows:\", enriched.count()); show_df(enriched, name=\"enriched\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Cell time (sec): {cell_time:6.2f}\") \n",
    "print(f\"[time] Cell time (min): {cell_time/60:6.2f}\") \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049a46ce-4a2e-4dfe-9792-1b72e647e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q1(a)1\") # Q1(a)1 – Basic counts and active in 2025 \n",
    "cell_time = time.time() \n",
    "\n",
    "total = enriched.select(\"ID\").distinct().count()\n",
    "\n",
    "active_2025 = (\n",
    "    enriched.where((F.col(\"FIRSTYEAR\") <= 2025) & (F.col(\"LASTYEAR\") >= 2025))\n",
    "    .select(\"ID\").distinct().count()\n",
    ")\n",
    "\n",
    "gsn = enriched.filter(F.col(\"GSN_FLAG\")    == \"GSN\").select(\"ID\").distinct().count()\n",
    "hcn = enriched.filter(F.col(\"HCNCRN_FLAG\") == \"HCN\").select(\"ID\").distinct().count()\n",
    "crn = enriched.filter(F.col(\"HCNCRN_FLAG\") == \"CRN\").select(\"ID\").distinct().count()\n",
    "\n",
    "print()\n",
    "bprint()\n",
    "print(f\"[result] Total stations          : {total:9,d}\")\n",
    "print(f\"[result] Active in 2025          : {active_2025:9,d}\")\n",
    "print(f\"[result] GSN stations            : {gsn:9,d}\")\n",
    "print(f\"[result] HCN stations            : {hcn:9,d}\")\n",
    "print(f\"[result] CRN stations            : {crn:9,d}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Cell time (sec)        : {cell_time:6.2f}\") \n",
    "print(f\"[time] Cell time (min)        : {cell_time/60:6.2f}\") \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c741463-9dfb-4c39-ba88-1d35afcecedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q1(a)2\") #  – Overlaps between networks \n",
    "cell_time = time.time() \n",
    "\n",
    "gsn_ids = enriched.filter(F.col(\"GSN_FLAG\")    == \"GSN\").select(\"ID\").distinct()\n",
    "hcn_ids = enriched.filter(F.col(\"HCNCRN_FLAG\") == \"HCN\").select(\"ID\").distinct()\n",
    "crn_ids = enriched.filter(F.col(\"HCNCRN_FLAG\") == \"CRN\").select(\"ID\").distinct()\n",
    "\n",
    "# Overlaps\n",
    "gsn_hcn = gsn_ids.intersect(hcn_ids).count()\n",
    "gsn_crn = gsn_ids.intersect(crn_ids).count()\n",
    "hcn_crn = hcn_ids.intersect(crn_ids).count()\n",
    "all_three = gsn_ids.intersect(hcn_ids).intersect(crn_ids).count()\n",
    "\n",
    "print()\n",
    "bprint()\n",
    "print(f\"[result] GSN ∩ HCN stations    : {gsn_hcn:5,d}\")\n",
    "print(f\"[result] GSN ∩ CRN stations    : {gsn_crn:5,d}\")\n",
    "print(f\"[result] HCN ∩ CRN stations    : {hcn_crn:5,d}\")\n",
    "print(f\"[result] All three             : {all_three:5,d}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Cell time (sec): {cell_time:6.2f}\") \n",
    "print(f\"[time] Cell time (min): {cell_time/60:6.2f}\") \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f33ee00-b659-4c18-a617-24a8c94728e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q1(a)\") # Counts from  Analysis Q1(a)\n",
    "\n",
    "# Ensure output directory exists \n",
    "os.makedirs(figs_dir, exist_ok=True)\n",
    "print(f\"[result] output directory confirmed: {os.path.abspath(figs_dir)}\")\n",
    "\n",
    "counts = {\n",
    "    \"GSN\": int(gsn),\n",
    "    \"HCN\": int(hcn),\n",
    "    \"CRN\": int(crn),\n",
    "    \"GSN ∩ HCN\": int(gsn_hcn),\n",
    "    \"GSN ∩ CRN\": int(gsn_crn),\n",
    "    \"HCN ∩ CRN\": int(hcn_crn),\n",
    "    \"All three\": int(all_three),\n",
    "}\n",
    "order = [\"GSN\", \"HCN\", \"CRN\", \"GSN ∩ HCN\", \"GSN ∩ CRN\", \"HCN ∩ CRN\", \"All three\"]\n",
    "labels = order\n",
    "values = [counts[k] for k in order]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "bars = plt.bar(labels, values)\n",
    "plt.bar(counts.keys(), counts.values())\n",
    "plt.ylabel(\"Number of stations\")\n",
    "plt.title(\"Station membership and overlaps (GSN, HCN, CRN)\")\n",
    "plt.xticks(rotation=0, ha=\"center\")\n",
    "plt.tight_layout()\n",
    "# Add the numbers on top of each bar\n",
    "for b in bars:\n",
    "    height = b.get_height()\n",
    "    plt.text(b.get_x() + b.get_width()/2,\n",
    "             height,\n",
    "             f\"{int(height)}\",\n",
    "             ha=\"center\",\n",
    "             va=\"bottom\")\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "outfile = os.path.join(figs_dir, \"barchart-station-metadata-analysis.png\")\n",
    "plt.savefig(outfile, dpi=300)\n",
    "print(f\"[file] barchart saved -> {outfile}\")\n",
    "plt.show()\n",
    "#plt.close()\n",
    "\n",
    "cell_time = time.time() - cell_time  \n",
    "print(f\"[time] Cell time (sec): {cell_time:6.2f}\") \n",
    "print(f\"[time] Cell time (min): {cell_time/60:6.2f}\") \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc5edd-f44d-41c9-8add-129638a53a8f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab232eb-ade2-4400-b722-11708d97ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q1(b)1\") # Q1(b)1 – Coverage queries \n",
    "cell_time = time.time()\n",
    "south_hem = enriched.filter(F.col(\"LATITUDE\") < 0).select(\"ID\").distinct().count()\n",
    "\n",
    "us_territories = enriched.filter(\n",
    "    (F.col(\"COUNTRY_CODE\") == \"US\") &\n",
    "    (F.col(\"STATE\").isin([\"PR\",\"GU\",\"AS\",\"VI\",\"MP\"]))\n",
    ").select(\"ID\").distinct().count()\n",
    "\n",
    "century_stations = enriched.filter(\n",
    "    (F.col(\"LASTYEAR\") - F.col(\"FIRSTYEAR\") + 1) >= 100\n",
    ").select(\"ID\").distinct().count()\n",
    "\n",
    "print()\n",
    "bprint()\n",
    "print(f\"[result] Southern Hemisphere stations : {south_hem:12,d}\")\n",
    "print(f\"[result] U.S. territory stations      : {us_territories:12,d}\")\n",
    "print(f\"[result] ≥100-year stations           : {century_stations:12,d}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Cell time (sec): {cell_time:6.2f}\") \n",
    "print(f\"[time] Cell time (min): {cell_time/60:6.2f}\") \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb35200-eb46-4924-a075-ade4228acb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\" Q1(c)1\") # – Coverage of the 5 core elements\n",
    " \n",
    "cell_time = time.time() \n",
    "# Stations with at least 1 core element\n",
    "core_stations = enriched.filter(F.col(\"CORE_ELEMENT_COUNT\") > 0).count()\n",
    "\n",
    "# Stations with all 5 core elements\n",
    "all5 = enriched.filter(F.col(\"CORE_ELEMENT_COUNT\") == 5).count()\n",
    "\n",
    "print()\n",
    "bprint()\n",
    "print(f\"[result] Stations with ≥1 core element : {core_stations:12,d}\")\n",
    "print(f\"[result] Stations with all 5 core      : {all5:12,d}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Cell time (sec): {cell_time:6.2f}\") \n",
    "print(f\"[time] Cell time (min): {cell_time/60:6.2f}\") \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa17b71-6848-4d12-942f-be6e021d6302",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc6b69-07fa-42d7-9dcc-d0a6aec617e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"TWO FIGURES\")\n",
    "# === TWO FIGURES IN ONE CELL ===\n",
    "# 1) Southern focus (assignment intent): Southern Hemisphere, US territories south of the equator, ≥100 years (global)\n",
    "# 2) Perspective: Northern Hemisphere, Southern Hemisphere, New Zealand (all)\n",
    "# Notes:\n",
    "#  - Keeps all diagnostic prints.\n",
    "#  - Comments in British English.\n",
    "#  - Filenames include 'dew59'.\n",
    " \n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "# Ensure a figures folder exists\n",
    "if 'figs_dir' not in globals():\n",
    "    figs_dir = \"figures\"\n",
    "os.makedirs(figs_dir, exist_ok=True)\n",
    "\n",
    "# ---------- COUNTS (shared) ----------\n",
    "print(\"[status] Computing station counts for requested views...\")\n",
    "\n",
    "# Hemispheres\n",
    "south_hem = (\n",
    "    enriched\n",
    "    .filter(F.col(\"LATITUDE\") < 0)\n",
    "    .select(\"ID\").distinct().count()\n",
    ")\n",
    "north_hem = (\n",
    "    enriched\n",
    "    .filter(F.col(\"LATITUDE\") > 0)\n",
    "    .select(\"ID\").distinct().count()\n",
    ")\n",
    "\n",
    "# US territories (list per your earlier cells). We compute both 'south-only' and 'all' for transparency.\n",
    "us_territories_all = (\n",
    "    enriched\n",
    "    .filter((F.col(\"COUNTRY_CODE\") == \"US\") & (F.col(\"STATE\").isin(\"PR\", \"GU\", \"AS\", \"VI\", \"MP\")))\n",
    "    .select(\"ID\").distinct().count()\n",
    ")\n",
    "us_territories_south = (\n",
    "    enriched\n",
    "    .filter(\n",
    "        (F.col(\"COUNTRY_CODE\") == \"US\") &\n",
    "        (F.col(\"STATE\").isin(\"PR\", \"GU\", \"AS\", \"VI\", \"MP\")) &\n",
    "        (F.col(\"LATITUDE\") < 0)\n",
    "    )\n",
    "    .select(\"ID\").distinct().count()\n",
    ")\n",
    "\n",
    "# ≥100-year stations (global)\n",
    "century_global = (\n",
    "    enriched\n",
    "    .filter((F.col(\"LASTYEAR\") - F.col(\"FIRSTYEAR\") + 1) >= 100)\n",
    "    .select(\"ID\").distinct().count()\n",
    ")\n",
    "\n",
    "# New Zealand\n",
    "nz_all = (\n",
    "    enriched\n",
    "    .filter(F.col(\"COUNTRY_CODE\") == \"NZ\")\n",
    "    .select(\"ID\").distinct().count()\n",
    ")\n",
    "\n",
    "print(\"[status] south_hem (all SH):\", south_hem)\n",
    "print(\"[status] north_hem (all NH):\", north_hem)\n",
    "print(\"[status] us_territories_all (global, excl. US states):\", us_territories_all)\n",
    "print(\"[status] us_territories_south (south of equator):\", us_territories_south)\n",
    "print(\"[status] century_global (≥100 years, global):\", century_global)\n",
    "print(\"[status] nz_all:\", nz_all)\n",
    "\n",
    "# ---------- FIGURE 1: Southern focus (assignment-style) ----------\n",
    "labels1 = [\n",
    "    \"Southern\\nHemisphere\",\n",
    "    \"US territories\\n(south of equator)\",\n",
    "    \"≥100 years (global)\"\n",
    "]\n",
    "values1 = [south_hem, us_territories_south, century_global]\n",
    "print(\"[status] Fig1 Labels:\", labels1)\n",
    "print(\"[status] Fig1 Values:\", values1)\n",
    "\n",
    "plt.figure(figsize=(7., 5.4))\n",
    "bars = plt.bar(labels1, values1,width=.5, color=\"skyblue\")\n",
    "plt.ylabel(\"Number of stations\")\n",
    "plt.title(\"Coverage (southern focus)\")\n",
    "\n",
    "# Numbers on top\n",
    "top_val = max(values1) if values1 else 1\n",
    "plt.ylim(0, top_val * 1.12)\n",
    "for b in bars:\n",
    "    h = b.get_height()\n",
    "    x = b.get_x() + b.get_width() / 2\n",
    "    y = h if h > 0 else 0\n",
    "    plt.text(x, y, f\"{int(h):,}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.xticks(rotation=0, ha=\"center\")\n",
    "plt.tight_layout()\n",
    "\n",
    "outfile1 = os.path.join(figs_dir, \"dew59_bar_southern_focus.png\")\n",
    "plt.savefig(outfile1, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"[status] Figure 1 saved to: {outfile1}\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ---------- FIGURE 2: Hemispheres + NZ (perspective) ----------\n",
    "labels2 = [\"Northern\\nHemisphere\", \"Southern\\nHemisphere\", \"New Zealand\\n(all)\"]\n",
    "values2 = [north_hem, south_hem, nz_all]\n",
    "print(\"[status] Fig2 Labels:\", labels2)\n",
    "print(\"[status] Fig2 Values:\", values2)\n",
    "\n",
    "plt.figure(figsize=(7., 5.4))\n",
    "bars = plt.bar(labels2, values2, width=.5, color=\"skyblue\")\n",
    "plt.ylabel(\"Number of stations\")\n",
    "plt.title(\"Coverage by hemisphere with New Zealand\")\n",
    "\n",
    "top_val = max(values2) if values2 else 1\n",
    "plt.ylim(0, top_val * 1.12)\n",
    "for b in bars:\n",
    "    h = b.get_height()\n",
    "    x = b.get_x() + b.get_width() / 2\n",
    "    y = h if h > 0 else 0\n",
    "    plt.text(x, y, f\"{int(h):,}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.xticks(rotation=0, ha=\"center\")\n",
    "plt.tight_layout()\n",
    "\n",
    "outfile2 = os.path.join(figs_dir, \"bar_hemi_plus_nz.png\")\n",
    "plt.savefig(outfile2, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"[status] Figure 2 saved to: {outfile2}\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ---------- TIMING ----------\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Cell time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] Cell time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba27ae0-ee61-407b-963d-32d23741cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"TWO FIGURES 2\") # === TWO FIGURES IN ONE CELL ===\n",
    "\n",
    "# 1) Southern focus (assignment): Southern Hemisphere, US territories south of the equator, ≥100 years (global)\n",
    "# 2) Perspective: Northern Hemisphere, Southern Hemisphere, New Zealand (all)\n",
    "# Notes:\n",
    "#  - Comments in British English; diagnostics use [status].\n",
    "#  - Single cell creates both figures and saves them under 'figures/' with 'dew59' in filenames.\n",
    "#  - Controls at top for bar thickness and true inter-bar gaps.\n",
    " \n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "# ---------- Controls ----------\n",
    "bar_width = 0.5         # thickness of each bar (0–1 relative to our custom step)\n",
    "bar_gap_factor = 0.05     # extra gap between adjacent bars (0 = default/none, larger = more spacing)\n",
    "side_margin = 0.12        # outer whitespace at chart edges (does not affect inter-bar gaps)\n",
    "print(f\"[status] bar_width -> {bar_width}\")\n",
    "print(f\"[status] bar_gap_factor -> {bar_gap_factor}\")\n",
    "print(f\"[status] side_margin -> {side_margin}\")\n",
    "\n",
    "# Ensure a figures folder exists\n",
    "if 'figs_dir' not in globals():\n",
    "    figs_dir = \"figures\"\n",
    "os.makedirs(figs_dir, exist_ok=True)\n",
    "\n",
    "# ---------- COUNTS (shared) ----------\n",
    "print(\"[status] Computing station counts for requested views...\")\n",
    "\n",
    "# Hemispheres\n",
    "south_hem = (\n",
    "    enriched\n",
    "    .filter(F.col(\"LATITUDE\") < 0)\n",
    "    .select(\"ID\").distinct().count()\n",
    ")\n",
    "north_hem = (\n",
    "    enriched\n",
    "    .filter(F.col(\"LATITUDE\") > 0)\n",
    "    .select(\"ID\").distinct().count()\n",
    ")\n",
    "\n",
    "# US territories: compute both (all vs south-only) for transparency; we use south-only in Figure 1\n",
    "us_territories_all = (\n",
    "    enriched\n",
    "    .filter((F.col(\"COUNTRY_CODE\") == \"US\") & (F.col(\"STATE\").isin(\"PR\", \"GU\", \"AS\", \"VI\", \"MP\")))\n",
    "    .select(\"ID\").distinct().count()\n",
    ")\n",
    "us_territories_south = (\n",
    "    enriched\n",
    "    .filter(\n",
    "        (F.col(\"COUNTRY_CODE\") == \"US\") &\n",
    "        (F.col(\"STATE\").isin(\"PR\", \"GU\", \"AS\", \"VI\", \"MP\")) &\n",
    "        (F.col(\"LATITUDE\") < 0)\n",
    "    )\n",
    "    .select(\"ID\").distinct().count()\n",
    ")\n",
    "\n",
    "# ≥100-year stations (global)\n",
    "century_global = (\n",
    "    enriched\n",
    "    .filter((F.col(\"LASTYEAR\") - F.col(\"FIRSTYEAR\") + 1) >= 100)\n",
    "    .select(\"ID\").distinct().count()\n",
    ")\n",
    "\n",
    "# New Zealand\n",
    "nz_all = (\n",
    "    enriched\n",
    "    .filter(F.col(\"COUNTRY_CODE\") == \"NZ\")\n",
    "    .select(\"ID\").distinct().count()\n",
    ")\n",
    "\n",
    "print(\"[status] south_hem (all SH):\", south_hem)\n",
    "print(\"[status] north_hem (all NH):\", north_hem)\n",
    "print(\"[status] us_territories_all (global, excl. US states):\", us_territories_all)\n",
    "print(\"[status] us_territories_south (south of equator):\", us_territories_south)\n",
    "print(\"[status] century_global (≥100 years, global):\", century_global)\n",
    "print(\"[status] nz_all:\", nz_all)\n",
    "\n",
    "# =====================================================\n",
    "# FIGURE 1: Southern focus (assignment-style)\n",
    "# =====================================================\n",
    "labels1 = [\n",
    "    \"Southern Hemisphere\",\n",
    "    \"US territories (south of equator)\",\n",
    "    \"≥100 years (global)\"\n",
    "]\n",
    "values1 = [south_hem, us_territories_south, century_global]\n",
    "print(\"[status] Fig1 Labels:\", labels1)\n",
    "print(\"[status] Fig1 Values:\", values1)\n",
    "\n",
    "# Numeric x positions with explicit inter-bar gaps\n",
    "x1 = np.arange(len(labels1)) * (0.5 + bar_gap_factor)\n",
    "print(\"[status] Fig1 x positions ->\", x1.tolist())\n",
    "\n",
    "plt.figure(figsize=(9.6, 5.4))\n",
    "bars = plt.bar(x1, values1, width=bar_width, color=\"skyblue\")\n",
    "plt.ylabel(\"Number of stations\")\n",
    "plt.title(\"Coverage (southern focus)\")\n",
    "\n",
    "# Numbers on top\n",
    "top_val = max(values1) if values1 else 1\n",
    "plt.ylim(0, top_val * 1.12)\n",
    "for i, b in enumerate(bars):\n",
    "    h = b.get_height()\n",
    "    x = x1[i]\n",
    "    y = h if h > 0 else 0\n",
    "    plt.text(x, y, f\"{int(h):,}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Category labels at our numeric x positions\n",
    "plt.xticks(x1, labels1, rotation=0, ha=\"center\")\n",
    "plt.margins(x=side_margin)  # outer padding only\n",
    "plt.tight_layout()\n",
    "\n",
    "outfile1 = os.path.join(figs_dir, \"dew59_bar_southern_focus.png\")\n",
    "plt.savefig(outfile1, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"[status] Figure 1 saved to: {outfile1}\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# =====================================================\n",
    "# FIGURE 2: Hemispheres + NZ (perspective)\n",
    "# =====================================================\n",
    "labels2 = [\"Northern Hemisphere\", \"Southern Hemisphere\", \"New Zealand (all)\"]\n",
    "values2 = [north_hem, south_hem, nz_all]\n",
    "print(\"[status] Fig2 Labels:\", labels2)\n",
    "print(\"[status] Fig2 Values:\", values2)\n",
    "\n",
    "x2 = np.arange(len(labels2)) * (0.5 + bar_gap_factor)\n",
    "print(\"[status] Fig2 x positions ->\", x2.tolist())\n",
    "\n",
    "plt.figure(figsize=(7., 5.4))\n",
    "bars = plt.bar(x2, values2, width=bar_width, color=\"skyblue\")\n",
    "plt.ylabel(\"Number of stations\")\n",
    "plt.title(\"Coverage by hemisphere with New Zealand\")\n",
    "\n",
    "top_val = max(values2) if values2 else 1\n",
    "plt.ylim(0, top_val * 1.12)\n",
    "for i, b in enumerate(bars):\n",
    "    h = b.get_height()\n",
    "    x = x2[i]\n",
    "    y = h if h > 0 else 0\n",
    "    plt.text(x, y, f\"{int(h):,}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.xticks(x2, labels2, rotation=0, ha=\"center\")\n",
    "plt.margins(x=side_margin)\n",
    "plt.tight_layout()\n",
    "\n",
    "outfile2 = os.path.join(figs_dir, \"dew59_bar_hemi_plus_nz.png\")\n",
    "plt.savefig(outfile2, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"[status] Figure 2 saved to: {outfile2}\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ---------- TIMING ----------\n",
    "cell_time = time.time() - cell_time  \n",
    "print(f\"[time] Cell time (sec): {cell_time:6.2f}\") \n",
    "print(f\"[time] Cell time (min): {cell_time/60:6.2f}\") \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e4d9a-68c7-4333-a7f3-b73facd70717",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"AQ2(a)2\")\n",
    "\n",
    "# This notebook does not define `daily`/`daily_df`, so we read from storage.\n",
    " \n",
    " \n",
    "cell_time = time.time()\n",
    "# Confirm daily_root exists\n",
    "\n",
    "try:\n",
    "    daily_root\n",
    "except NameError:\n",
    "    raise NameError(\"daily_root is not defined in this notebook. \"\n",
    "                    \"Please run the setup cell that defines WASBS_* paths.\")\n",
    "\n",
    "print(f\"[diag] Reading daily data from directory: {daily_root}\")\n",
    "\n",
    "# GHCN-Daily schema (as per assignment starter): \n",
    "# ID, DATE (yyyymmdd), ELEMENT, VALUE, MFLAG, QFLAG, SFLAG, OBS_TIME (optional)\n",
    "daily_schema = StructType([\n",
    "    StructField(\"ID\",       StringType(),  True),\n",
    "    StructField(\"DATE\",     StringType(),  True),   # keep as string; we parse later\n",
    "    StructField(\"ELEMENT\",  StringType(),  True),\n",
    "    StructField(\"VALUE\",    IntegerType(), True),\n",
    "    StructField(\"MFLAG\",    StringType(),  True),\n",
    "    StructField(\"QFLAG\",    StringType(),  True),\n",
    "    StructField(\"SFLAG\",    StringType(),  True),\n",
    "    StructField(\"OBS_TIME\", StringType(),  True)\n",
    "])\n",
    "\n",
    "# Read all CSV/GZ files in the daily directory; Spark handles .gz transparently\n",
    "# We do not set header=True because GHCN text files have no header row.\n",
    "daily_df = (spark.read\n",
    "                 .option(\"header\", \"false\")\n",
    "                 .schema(daily_schema)\n",
    "                 .csv(daily_root))\n",
    "\n",
    "print(\"[diag] Loaded daily_df\")\n",
    "\n",
    "show_df(daily_df,name=\"daily_df\")\n",
    "# Light sanity checks (not expensive)\n",
    "print(\"[diag] Row count sample (first action):\", daily_df.limit(5).count()) \n",
    "\n",
    " \n",
    "# Tiny preview of PRCP presence\n",
    "print(\"[diag] Distinct ELEMENT sample:\")\n",
    "daily_df.select(\"ELEMENT\").distinct().orderBy(\"ELEMENT\").show(10, truncate=False)\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Cell time (sec)        : {cell_time:6.2f}\") \n",
    "print(f\"[time] Cell time (min)        : {cell_time/60:6.2f}\") \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a067a8-2b47-4865-8505-4d81f8393cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"AQX(a)\")\n",
    "cell_time = time.time()\n",
    "core_stations = enriched.filter(F.col(\"CORE_ELEMENT_COUNT\") > 0).count()\n",
    "core_time     = time.time() - cell_time\n",
    "\n",
    "all5_time     = time.time() \n",
    "all5          = enriched.filter(F.col(\"CORE_ELEMENT_COUNT\") == 5).count()\n",
    "all5_time     = time.time() - all5_time\n",
    "core = {\n",
    "    \"≥1 core element\": core_stations,\n",
    "    \"All 5 core elements\": all5\n",
    "}\n",
    "\n",
    " \n",
    "labels = list(core.keys())\n",
    "values = [core[k] for k in labels]\n",
    "print(\"[INFO] Labels:\", labels)\n",
    "print(\"[INFO] Values:\", values)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "bars = plt.bar(labels, values, color=\"lightgreen\")\n",
    "plt.ylabel(\"Number of stations\")\n",
    "plt.title(\"Core element coverage\")\n",
    "plt.xticks(rotation=0, ha=\"center\")\n",
    "\n",
    "# Give a little headroom so the labels do not clip\n",
    "plt.ylim(0, max(values) * 1.10)\n",
    "\n",
    "# Add the numbers on top of each bar (with thousands separators)\n",
    "for b in bars:\n",
    "    height = b.get_height()\n",
    "    x = b.get_x() + b.get_width() / 2\n",
    "    y = height if height > 0 else 0\n",
    "    plt.text(x, y, f\"{int(height):,}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "outfile = os.path.join(figs_dir, \"barchart-station-metadata-analysis.png\")\n",
    "print(\"outfile: \", outfile)\n",
    "plt.savefig(outfile, dpi=300)\n",
    " \n",
    "print(f\"[INFO] Figure saved to: {outfile}\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "cell_time = time.time() - cell_time  \n",
    "print(f\"[time] core_time (sec)        : {core_time:5.2f}\") \n",
    "print(f\"[time] core_time (min)        : {core_time/60:5.2f}\")\n",
    "print(f\"[time] all5_time (sec)        : {all5_time:5.2f}\") \n",
    "print(f\"[time] all5_time (min)        : {all5_time/60:5.2f}\") \n",
    "print(f\"[time] Cell time (sec)        : {cell_time:6.2f}\") \n",
    "print(f\"[time] Cell time (min)        : {cell_time/60:6.2f}\") \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2318c1-9ef3-4db9-a5cb-7d028114864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    " \n",
    "#stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b92a3-210e-4927-aff4-c4a63a430a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"AQ2(a)\") # 3 — PRCP by year × country using Spark .agg() \n",
    "# --------------------------------------------------------------------\n",
    "# average *daily* rainfall (mm) per year & country.\n",
    "cell_time = time.time()\n",
    "\n",
    "# Be robust to column name case (ID vs id, DATE vs date, etc.)\n",
    "id_col      = 'id'      if 'id'      in daily_df.columns else 'ID'\n",
    "date_col    = 'date'    if 'date'    in daily_df.columns else 'DATE'\n",
    "elem_col    = 'element' if 'element' in daily_df.columns else 'ELEMENT'\n",
    "value_col   = 'value'   if 'value'   in daily_df.columns else 'VALUE'\n",
    "\n",
    "print(f\"[check] Using columns: id={id_col}, date={date_col}, element={elem_col}, value={value_col}\")\n",
    "filter_time = time.time()\n",
    "# Filter to precipitation only\n",
    "prcp_df = daily_df.filter(F.col(elem_col) == F.lit('PRCP'))\n",
    "print(f\"[check] PRCP record count: {prcp_df.count():,}\")\n",
    "print(\"[diag] prcp_df schema:\")\n",
    "prcp_df.printSchema()\n",
    "show_as_html(prcp_df,1)\n",
    "# Extract country code (first two characters of station ID)\n",
    "prcp_df = prcp_df.withColumn(\"country_code\", F.substring(F.col(id_col), 1, 2))\n",
    "print(\"[diag] prcp_df schema:\")\n",
    "prcp_df.printSchema()\n",
    "show_as_html(prcp_df,1)\n",
    "# Parse year from DATE (stored as yyyymmdd string/int)\n",
    "prcp_df = prcp_df.withColumn(\n",
    "    \"year\",\n",
    "    F.year(F.to_date(F.col(date_col).cast(\"string\"), \"yyyyMMdd\"))\n",
    ")\n",
    "filter_time = time.time() - filter_time \n",
    "after_time = time.time()\n",
    "show_as_html(prcp_df,1)\n",
    "# Convert PRCP from tenths of mm to mm, and drop negatives just in case\n",
    "prcp_df = prcp_df.withColumn(\"prcp_mm\", (F.col(value_col) / F.lit(10.0)).cast(\"double\"))\n",
    "print(\"[diag] prcp_df schema:\")\n",
    "prcp_df.printSchema()\n",
    "show_as_html(prcp_df,1)\n",
    "prcp_df = prcp_df.filter(F.col(\"prcp_mm\") >= 0)\n",
    "print(\"[diag] prcp_df schema:\")\n",
    "prcp_df.printSchema()\n",
    "show_as_html(prcp_df,1)\n",
    "q2a_agg_time = time.time()\n",
    "# >>> The required aggregation <<<\n",
    "q2a_agg_df = (\n",
    "    prcp_df\n",
    "    .groupBy(\"year\", \"country_code\")\n",
    "    .agg(F.avg(\"prcp_mm\").alias(\"avg_daily_prcp_mm\"))\n",
    "    .orderBy(\"year\", \"country_code\")\n",
    ")\n",
    "after_time = time.time() - after_time  \n",
    "q2a_agg_time = time.time() - q2a_agg_time   \n",
    "print(\"[diag] q2a_agg_df schema:\")\n",
    "q2a_agg_df.printSchema()\n",
    "show_as_html(q2a_agg_df,1)\n",
    "# Write the aggregated table for visualisation\n",
    " \n",
    "\n",
    "print(f\"[diag] Writing Q2(a)3 output to: {precip_write_name}\")\n",
    "(q2a_agg_df.write.mode(\"overwrite\").parquet(precip_write_name))\n",
    " \n",
    "print(f\"[time] q2a_agg_time      (sec): {q2a_agg_time:11.2f}\") \n",
    "print(f\"[time] q2a_agg_time      (min): {q2a_agg_time/60:11.2f}\")  \n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Filter time (sec): {filter_time:11.2f}\") \n",
    "print(f\"[time] Filter time (min): {filter_time/60:11.2f}\")\n",
    "print(f\"[time] After time (sec): {after_time:11.2f}\") \n",
    "print(f\"[time] After time (min): {after_time/60:11.2f}\")\n",
    "print(f\"[time] Cell time (sec): {cell_time:11.2f}\") \n",
    "print(f\"[time] Cell time (min): {cell_time/60:11.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb90335b-bbc2-4165-8aec-fd2381c284e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Ananalysis  A X (11) \") #  spark protect for vscode runs\n",
    "cell_time = time.time() \n",
    "val       = spark.range(1).select(F.date_format(F.current_timestamp(), 'yyyy.MM.dd HH:mm').alias('t')).first()['t']\n",
    "\n",
    "CITIES = [\n",
    "    (\"Auckland\",     -36.8485, 174.7633),\n",
    "    (\"Wellington\",   -41.2866, 174.7756),\n",
    "    (\"Christchurch\", -43.5321, 172.6362),\n",
    "]\n",
    "\n",
    "# Try Spark benchmark  \n",
    "spark_timings = None\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    if SparkSession.getActiveSession() is not None:\n",
    "        spark_timings = benchmark_spark_distances(CITIES, repeats=5)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Always run Python benchmark\n",
    "py_timings = benchmark_python_distances(CITIES, repeats=100000)\n",
    "\n",
    "# Print\n",
    "print(\"=== Timing (seconds) ===\")\n",
    "if spark_timings:\n",
    "    print(f\"Spark  | pairs={spark_timings['spark_pairs']}, repeats={spark_timings['spark_repeats']}\")\n",
    "    print(f\"  haversine: {spark_timings['spark_haversine_sec']:.6f}\")\n",
    "    print(f\"  slc:       {spark_timings['spark_slc_sec']:.6f}\")\n",
    "else:\n",
    "    print(\"Spark  | not detected (skipped)\")\n",
    "\n",
    "print(f\"Python | pairs={py_timings['pairs']}, repeats={py_timings['repeats']}\")\n",
    "print(f\"  haversine: {py_timings['python_haversine_sec']:.6f}\")\n",
    "print(f\"  slc:       {py_timings['python_slc_sec']:.6f}\")\n",
    "print(f\"[time] current time           :  {val}\")\n",
    "print(f\"[time] Cell time (sec)        : {cell_time:6.2f}\") \n",
    "print(f\"[time] Cell time (min)        : {cell_time/60:6.2f}\") \n",
    "print(f\"[time] notebook_run_time (min):  {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47253f6c-7c2c-47f1-a55c-a16210d00c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"stop_spark()\") # Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "cell_time = time.time()\n",
    "#stop_spark()\n",
    "\n",
    "val = spark.range(1).select(F.date_format(F.current_timestamp(), 'yyyy.MM.dd HH:mm').alias('t')).first()['t']\n",
    "cell_time = time.time() - cell_time  \n",
    "print(f\"[time] current time           :  {val}\")\n",
    "print(f\"[time] Cell time (sec)        : {cell_time:6.2f}\") \n",
    "print(f\"[time] Cell time (min)        : {cell_time/60:6.2f}\") \n",
    "print(f\"[time] notebook_run_time (min):  {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
