{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db545270-fabc-4cf7-a32a-e4f4d6fef18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "global azure_account_name\n",
    "global azure_data_container_name\n",
    "global azure_user_container_name\n",
    "global azure_user_token\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "azure_user_token = r\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(f\"fs.azure.sas.{azure_user_container_name}.{azure_account_name}.blob.core.windows.net\",  azure_user_token)\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bbb004-5a22-4f65-b400-cb2bc3edf217",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e2f6d-f8f0-4150-8eec-e5bb2b0471e4",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06343f5-a638-4f7f-ade5-5f564a068bc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/09/12 20:18:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/12 20:18:00 WARN Utils: Service 'sparkDriver' could not bind on port 7077. Attempting port 7078.\n",
      "25/09/12 20:18:01 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4045\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>32</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-0539531d162a47baaa098176b9d4b79e</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1757665080512</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-4ea0c0993d00b7c0</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1757665080402</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7078</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=4, executor_cores=2, worker_memory=4, master_memory=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7556669f-ee60-4328-8b8f-29d26d48be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "import math, os, platform, re\n",
    "import subprocess, sys, time\n",
    "\n",
    "import itertools         as it\n",
    "import matplotlib.dates  as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from IPython.display     import display  # calls between environments\n",
    "from math                import acos, atan2, cos, radians, sin, sqrt\n",
    "from matplotlib.ticker   import FuncFormatter, MaxNLocator\n",
    "from pathlib             import Path\n",
    "from pyspark.sql         import DataFrame\n",
    "from pyspark.sql         import DataFrame as SparkDF\n",
    "from pyspark.sql         import functions as F, types as T\n",
    "from pyspark.sql.types   import *\n",
    "from pyspark.sql.utils   import AnalysisException\n",
    "from pyspark.sql.window  import Window\n",
    "from typing              import List, Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cda5876-8608-4f13-8dba-fa5b7898d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER AND DIAGNOSTIC FUNCTIONS\n",
    "notebook_run_time = time.time()\n",
    "\n",
    "def df_as_html(df, n: int = 5, right_align: bool = False, show_index: bool = False):\n",
    "    \"\"\"\n",
    "    HTML preview via pandas with no truncation. If right_align=True,\n",
    "    only numeric columns are right-justified; everything else is \n",
    "    explicitly left-aligned.\n",
    "    \"\"\"\n",
    "    \n",
    "    pdf = df.limit(n).toPandas()\n",
    "    print(\"[INFO] Converting Spark → pandas for HTML display (rows:\", len(pdf), \")\")\n",
    "    print(\"[INFO] right_align (numeric columns):\", right_align)\n",
    "\n",
    "    with pd.option_context(\n",
    "        \"display.max_colwidth\", None,   \n",
    "        \"display.max_columns\", None,    \n",
    "        \"display.width\", None            \n",
    "    ):\n",
    "        styler = pdf.style if show_index else pdf.style.hide(axis=\"index\")\n",
    "\n",
    "        #   table alignment: left for both headers and cells\n",
    "        styler = styler.set_table_styles(\n",
    "            [\n",
    "                {\"selector\": \"th\", \"props\": [(\"text-align\", \"left\")]},\n",
    "                {\"selector\": \"td\", \"props\": [(\"text-align\", \"left\")]},\n",
    "            ],\n",
    "            overwrite=True,  # make this the baseline\n",
    "        )\n",
    "         \n",
    "        if right_align:\n",
    "            numeric_cols = list(pdf.select_dtypes(include=[\"number\"]).columns)\n",
    "            print(\"[INFO] Right-aligning numeric columns:\", numeric_cols)\n",
    "            if numeric_cols:\n",
    "                styler = styler.set_properties(subset=numeric_cols,\n",
    "                                               **{\"text-align\": \"right\"})\n",
    "        display(styler)\n",
    "\n",
    "def _normalise_dir(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure trailing slash so we point to\n",
    "    the dataset directory (not a file)\n",
    "    \"\"\"\n",
    "    return s if s.endswith(\"/\") else s + \"/\"\n",
    "\n",
    "def ensure_dir(path: str) -> str:\n",
    "    \"\"\"\n",
    "    treat Parquet datasets as directories;\n",
    "    add trailing slash if needed\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        raise ValueError(\"Path is None\")\n",
    "    path = _normalise_dir(path)\n",
    "    print(\"ensure_dir -> \",path)\n",
    "    return path\n",
    "    \n",
    "def show_df(df, n: int = 10, name: str = \"\", right_align: bool = False):\n",
    "    \"\"\"\n",
    "    Print schema, \n",
    "    show an HTML sample,\n",
    "    and row count.\n",
    "    \"\"\"\n",
    "    print(\"_\"*70)\n",
    "    print(\"name : \",name)\n",
    "    df.printSchema()\n",
    "    print(\"[check] sample:\")\n",
    "    df_as_html(df, n=n, right_align=right_align)\n",
    "  \n",
    "def write_parquet(df, dir_as_path: str, df_name:str = \"\"):    \n",
    "    funct_time = time.time()\n",
    "    path = _normalise_dir(dir_as_path)\n",
    "    print(f\"[file] write_parquet  : {path}\")\n",
    "    try:      \n",
    "        show_df(df,df_name)\n",
    "    except Exception as e:\n",
    "        print(\"[cathch] sample failed:\", e)\n",
    "        os.system(f'hdfs dfs -rm -r -f \"{path}\"')   # idempotent cleanup\n",
    "    df.write.mode(\"overwrite\").format(\"parquet\").save(path)\n",
    "    os.system(f'hdfs dfs -ls -R \"{path}\"')\n",
    "    funct_time = time.time() - funct_time \n",
    "    print(f\"[time] write_parquet (min)   : {funct_time/60:5.2f}\")\n",
    "    print(f\"[time] write_parquet (sec)   : {funct_time:5.2f}\")\n",
    "  \n",
    "def has_parquet(dir_as_path: str) -> bool:\n",
    "    path   = _normalise_dir( dir_as_path)\n",
    "    marker = path + '_SUCCESS'\n",
    "    #print(\"\\n[check] dir_path:\", dir_path)\n",
    "    #print(\"\\n[check] path    :\", path)\n",
    "    print(\"\\n[check] marker  :\", marker)\n",
    "    rc = os.system(f'hdfs dfs -test -e \"{marker}\"')\n",
    "    print(\"[check] rc:\", rc, \"->\", (\"exists\" if rc == 0 else \"missing\"))\n",
    "    return (rc == 0)\n",
    "    \n",
    "def _to_spark(df_like, schema=None):\n",
    "    \"\"\"\n",
    "    Return a Spark DataFrame  .\n",
    "    \"\"\"\n",
    "    if isinstance(df_like, SparkDF):\n",
    "        return df_like\n",
    "    return spark.createDataFrame(df_like, schema=schema) if schema else spark.createDataFrame(df_like)\n",
    "\n",
    "def _success_exists(target_dir: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check for the Hadoop/Spark _SUCCESS marker;  \n",
    "    \"\"\"\n",
    "    jvm = spark._jvm\n",
    "    hconf = spark._jsc.hadoopConfiguration()\n",
    "    try:\n",
    "        uri = jvm.java.net.URI(target_dir)\n",
    "        fs = jvm.org.apache.hadoop.fs.FileSystem.get(uri, hconf)\n",
    "        success = jvm.org.apache.hadoop.fs.Path(target_dir + \"_SUCCESS\")\n",
    "        exists = fs.exists(success)\n",
    "        print(f\"[status] _SUCCESS check at: {target_dir}_SUCCESS -> {exists}\")\n",
    "        return bool(exists)\n",
    "    except Exception as e:\n",
    "        print(f\"[status] _SUCCESS check failed ({e}); attempting read-probe …\")\n",
    "        try:\n",
    "            spark.read.parquet(target_dir).limit(1).count()\n",
    "            print(f\"[dewstatus59] read-probe succeeded at: {target_dir}\")\n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"[status] read-probe failed ({e2}); treating as not existing.\")\n",
    "            return False\n",
    " \n",
    "\n",
    "def normalise_ids(df: DataFrame, col: str = \"ID\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    # Single source of truth for ID normalisation \n",
    "    Upper + trim + distinct on the given ID column.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] normalise_ids() on column: {col}\")\n",
    "    return df.select(F.upper(F.trim(F.col(col))).alias(\"ID\")).distinct()\n",
    "\n",
    "# hack \n",
    "_ids       = normalise_ids\n",
    "canon_ids  = normalise_ids\n",
    "_canon_ids = normalise_ids\n",
    " \n",
    "def probe_universe(daily_df, stations_df, inv_agg_df, tag=\"\"):\n",
    "    \"\"\"\n",
    "    DIAGNOSTIC\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"_\"*70)\n",
    "    print(f\"[PROBE] Station universe check :: {tag}\")\n",
    "    daily_ids   = _ids(daily_df)\n",
    "    station_ids = _ids(stations_df)\n",
    "    inv_ids     = _ids(inv_agg_df)\n",
    "    print(\"[COUNT] daily IDs         :\", daily_ids.count())\n",
    "    print(\"[COUNT] station IDs (cat) :\", station_ids.count())\n",
    "    print(\"[COUNT] inventory IDs     :\", inv_ids.count())\n",
    "    print(\"[DIFF ] daily  – station  :\", daily_ids.join(station_ids, \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] station – daily   :\", station_ids.join(daily_ids, \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] station – inv     :\", station_ids.join(inv_ids, \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] inv     – daily   :\", inv_ids.join(daily_ids, \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] inv     – station :\", inv_ids.join(station_ids, \"ID\", \"left_anti\").count())\n",
    "    print(\"_\"*70)\n",
    " \n",
    "\n",
    "def _count_unique_ids(df: DataFrame) -> int:\n",
    "    return normalise_ids(df).count()\n",
    "\n",
    "def pick_unfiltered_daily(preferred_path: str = None) -> DataFrame:\n",
    "    \"\"\"Return an unfiltered daily DF (~129k unique station IDs).\"\"\"\n",
    "    cand_names = [\"daily\", \"read_daily\", \"daily_df\", \"daily_all\", \"ghcnd_daily\"]\n",
    "    print(\"[INFO] Candidate DataFrames:\", [n for n in cand_names if n in globals()])\n",
    "    for name in cand_names:\n",
    "        obj = globals().get(name)\n",
    "        if isinstance(obj, DataFrame):\n",
    "            try:\n",
    "                n = normalise_ids(obj).count()\n",
    "                print(f\"[CHECK] {name} unique station IDs:\", n)\n",
    "                if n >= 120_000:\n",
    "                    print(f\"[INFO] Using {name} as the unfiltered daily.\")\n",
    "                    return obj\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not inspect {name}:\", repr(e))\n",
    "    if preferred_path:\n",
    "        print(f\"[INFO] Trying preferred_path: {preferred_path}\")\n",
    "        df = spark.read.parquet(str(preferred_path))\n",
    "        n = normalise_ids(df).count()\n",
    "        print(\"[CHECK] preferred_path unique station IDs:\", n)\n",
    "        if n >= 120_000:\n",
    "            print(\"[INFO] Using preferred_path as the unfiltered daily.\")\n",
    "            return df\n",
    "    for var in [\"DAILY_READ_NAME\",\"DAILY_WRITE_NAME\",\"daily_read_name\",\"daily_write_name\",\"DAILY_NAME\"]:\n",
    "        if var in globals():\n",
    "            path = globals()[var]\n",
    "            try:\n",
    "                print(f\"[INFO] Trying {var} = {path}\")\n",
    "                df = spark.read.parquet(str(path))\n",
    "                n = normalise_ids(df).count()\n",
    "                print(f\"[CHECK] {var} unique station IDs:\", n)\n",
    "                if n >= 120_000:\n",
    "                    print(f\"[INFO] Using {var} as the unfiltered daily.\")\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not read {var}:\", repr(e))\n",
    "    raise SystemExit(\"[FATAL] Could not find an unfiltered daily dataset (expected ~129k unique station IDs).\")\n",
    "\n",
    "def bprint(text: str=\"\", l=50):\n",
    "    n = len(text)\n",
    "    n = abs(n - l)//2\n",
    "    \n",
    "    print(\"\\n\" + \"_\" * n + text + \"_\" * n)\n",
    "\n",
    "# Back-compat aliases hack to account for non-disciplined naming\n",
    "_ids       = normalise_ids\n",
    "canon_ids  = normalise_ids\n",
    "_canon_ids = normalise_ids\n",
    "\n",
    "#print(\"[TEST] Using _canon_ids:\", _canon_ids(stations).count())\n",
    "#print(\"[TEST] Using canon_ids :\", canon_ids(stations).count())\n",
    "#print(\"[TEST] Using _ids      :\", _ids(stations).count())\n",
    "\n",
    "\n",
    "# : pairwise city distances in km using Spark built-ins \n",
    "def pairwise_city_distances_spark(cities, radius_km=6371.0):\n",
    "    \"\"\"\n",
    "    cities: list[tuple[str, float, float]] -> [(name, lat_deg, lon_deg), ...]\n",
    "    returns: Spark DataFrame with columns:\n",
    "             city_a, city_b, haversine_km, slc_km, delta_km, delta_pct\n",
    "    \"\"\"\n",
    "  #  from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        raise RuntimeError(\"No active Spark session.\")\n",
    "\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"city\", T.StringType(), False),\n",
    "        T.StructField(\"lat\",  T.DoubleType(), False),\n",
    "        T.StructField(\"lon\",  T.DoubleType(), False),\n",
    "        ])\n",
    "    df = spark.createDataFrame(cities, schema)\n",
    "\n",
    "    a, b = df.alias(\"a\"), df.alias(\"b\")\n",
    "    pairs = (a.join(b, F.col(\"a.city\") < F.col(\"b.city\"))\n",
    "               .select(F.col(\"a.city\").alias(\"city_a\"),\n",
    "                       F.col(\"b.city\").alias(\"city_b\"),\n",
    "                       F.col(\"a.lat\").alias(\"lat1\"),\n",
    "                       F.col(\"a.lon\").alias(\"lon1\"),\n",
    "                       F.col(\"b.lat\").alias(\"lat2\"),\n",
    "                       F.col(\"b.lon\").alias(\"lon2\")))\n",
    "\n",
    "    R = F.lit(float(radius_km))\n",
    "    lat1 = F.radians(F.col(\"lat1\"));  lat2 = F.radians(F.col(\"lat2\"))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = F.radians(F.col(\"lon2\") - F.col(\"lon1\"))\n",
    "\n",
    "    a_term = F.sin(dlat/2)**2 + F.cos(lat1)*F.cos(lat2)*F.sin(dlon/2)**2\n",
    "    c_term = 2*F.atan2(F.sqrt(a_term), F.sqrt(1 - a_term))\n",
    "    hav_km = R * c_term\n",
    "\n",
    "    cos_val = F.sin(lat1)*F.sin(lat2) + F.cos(lat1)*F.cos(lat2)*F.cos(dlon)\n",
    "    cos_val = F.greatest(F.lit(-1.0), F.least(F.lit(1.0), cos_val))\n",
    "    slc_km = R * F.acos(cos_val)\n",
    "\n",
    "    delta_km  = F.abs(hav_km - slc_km)\n",
    "    delta_pct = F.when(hav_km == 0, F.lit(0.0)).otherwise(delta_km / hav_km * 100.0)\n",
    "\n",
    "    out_df = (pairs\n",
    "              .withColumn(\"haversine_km\", F.round(hav_km, 2))\n",
    "              .withColumn(\"slc_km\",       F.round(slc_km, 2))\n",
    "              .withColumn(\"delta_km\",     F.round(delta_km, 4))\n",
    "              .withColumn(\"delta_pct\",    F.round(delta_pct, 6))\n",
    "              .select(\"city_a\", \"city_b\", \"haversine_km\", \"slc_km\", \"delta_km\", \"delta_pct\")\n",
    "              .orderBy(\"haversine_km\"))\n",
    "    return out_df\n",
    "\n",
    "\n",
    "# --- Timing helpers for Spark & pure Python (no extra deps)\n",
    " \n",
    "\n",
    "def benchmark_python_distances(cities, radius_km=6371.0, repeats=50000):\n",
    "    \"\"\"\n",
    "    cities: [(name, lat_deg, lon_deg), ...]  (3 cities => 3 pairs)\n",
    "    repeats: loop count to make timings stable\n",
    "    returns: dict with seconds for haversine/slc\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for i in range(len(cities)):\n",
    "        for j in range(i+1, len(cities)):\n",
    "            (_, lat1, lon1), (_, lat2, lon2) = cities[i], cities[j]\n",
    "            pairs.append((lat1, lon1, lat2, lon2))\n",
    "\n",
    "    # haversine\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        for lat1, lon1, lat2, lon2 in pairs:\n",
    "            φ1, λ1, φ2, λ2 = map(radians, (lat1, lon1, lat2, lon2))\n",
    "            dφ, dλ = (φ2 - φ1), (λ2 - λ1)\n",
    "            a = sin(dφ/2)**2 + cos(φ1)*cos(φ2)*sin(dλ/2)**2\n",
    "            c = 2*atan2(sqrt(a), sqrt(1 - a))\n",
    "            _ = radius_km * c\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # spherical law of cosines (SLC)\n",
    "    t2 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        for lat1, lon1, lat2, lon2 in pairs:\n",
    "            φ1, λ1, φ2, λ2 = map(radians, (lat1, lon1, lat2, lon2))\n",
    "            cosv = sin(φ1)*sin(φ2) + cos(φ1)*cos(φ2)*cos(λ2 - λ1)\n",
    "            cosv = max(-1.0, min(1.0, cosv))\n",
    "            _ = radius_km * acos(cosv)\n",
    "    t3 = perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"python_haversine_sec\": t1 - t0,\n",
    "        \"python_slc_sec\":       t3 - t2,\n",
    "        \"repeats\": repeats,\n",
    "        \"pairs\": len(pairs),\n",
    "    }\n",
    "\n",
    "\n",
    "def _parse_ls_bytes(line): \n",
    "    parts = line.split()\n",
    "    if len(parts) < 8:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[4])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def _parse_du_bytes(line):\n",
    "    parts = line.split()\n",
    "    if len(parts) < 2:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[0])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def du_bytes(path):\n",
    "    lines = get_ipython().getoutput(f'hdfs dfs -du \"{path}\"')\n",
    "    total = 0\n",
    "    for ln in lines:\n",
    "        parts = ln.split()\n",
    "        if len(parts) >= 2:\n",
    "            try:\n",
    "                total += int(parts[0])\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return total\n",
    "    \n",
    "def benchmark_spark_distances(cities, radius_km=6368.6, repeats=3):\n",
    "    \"\"\"\n",
    "    Uses Spark built-ins only. Measures full execution\n",
    "    time by forcing an action.\n",
    "    \n",
    "    returns: dict with seconds for haversine/slc and\n",
    "    row counts used.\n",
    "    \n",
    "    For the radius:\n",
    "    \n",
    "    The Earth is slightly flattened, so the geocentric \n",
    "    radius depends on latitude.  For context: \n",
    "    \n",
    "    * equatorial radius = 6,378.137 km; \n",
    "    * polar radius      = 6,356.752 km \n",
    "    \n",
    "    Across New Zealand’s latitudes (≈36–47°S), using the\n",
    "    WGS-84 ellipsoid, you get roughly:\n",
    "\n",
    "    Auckland (37°S):       ~6,370.4 km\n",
    "    Wellington (41°S):     ~6,369.0 km\n",
    "    Christchurch (43.5°S): ~6,368.0 km\n",
    "    Dunedin (45.9°S):      ~6,367.2 km\n",
    "    __________________________________\n",
    "    mean                  ≈ 6,368.6 km\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        from pyspark.sql import SparkSession, functions as F, types as T\n",
    "    except Exception:\n",
    "        return None  # no Spark therefore save cannot run in vs code\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        return None\n",
    "\n",
    "    # build pairs once and cache\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"city\", T.StringType(), False),\n",
    "        T.StructField(\"lat\",  T.DoubleType(), False),\n",
    "        T.StructField(\"lon\",  T.DoubleType(), False),\n",
    "    ])\n",
    "    df = spark.createDataFrame(cities, schema)\n",
    "    a, b = df.alias(\"a\"), df.alias(\"b\")\n",
    "    pairs = (a.join(b, F.col(\"a.city\") < F.col(\"b.city\"))\n",
    "               .select(F.col(\"a.lat\").alias(\"lat1\"),\n",
    "                       F.col(\"a.lon\").alias(\"lon1\"),\n",
    "                       F.col(\"b.lat\").alias(\"lat2\"),\n",
    "                       F.col(\"b.lon\").alias(\"lon2\"))\n",
    "               .cache())\n",
    "    _ = pairs.count()\n",
    "\n",
    "    R = F.lit(float(radius_km))\n",
    "    lat1 = F.radians(F.col(\"lat1\")); lat2 = F.radians(F.col(\"lat2\"))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = F.radians(F.col(\"lon2\") - F.col(\"lon1\"))\n",
    "\n",
    "    # Haversine expr\n",
    "    a_term = F.sin(dlat/2)**2 + F.cos(lat1)*F.cos(lat2)*F.sin(dlon/2)**2\n",
    "    c_term = 2*F.atan2(F.sqrt(a_term), F.sqrt(1 - a_term))\n",
    "    hav = R * c_term\n",
    "\n",
    "    # SLC expr\n",
    "    cosv = F.sin(lat1)*F.sin(lat2) + F.cos(lat1)*F.cos(lat2)*F.cos(dlon)\n",
    "    cosv = F.greatest(F.lit(-1.0), F.least(F.lit(1.0), cosv))\n",
    "    slc = R * F.acos(cosv)\n",
    "\n",
    "    # time Haversine\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(hav.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # time SLC\n",
    "    t2 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(slc.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t3 = perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"spark_pairs\": pairs.count(),\n",
    "        \"spark_repeats\": repeats,\n",
    "        \"spark_haversine_sec\": t1 - t0,\n",
    "        \"spark_slc_sec\":       t3 - t2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b263df7f-2c3c-49f7-b921-8be60dc8f623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________GLOBAL VARIABLES_________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__________________________________________________\n",
      "[time] current time           :  2025.09.12 20:18\n",
      "\n",
      "__________________________________________________\n",
      "\n",
      "___________________ENVIRONMENT___________________\n",
      "Spark       : 3.5.1\n",
      "Python tuple: (3, 8, 10)\n",
      "username    : dew59\n",
      "\n",
      "\n",
      "_________________DEEBUG LOGICALS_________________\n",
      "[status] FORCE_REBUILD_ENRICHED  : True\n",
      "[status] FORCE_REBUILD_STATIONS  : True\n",
      "[status] FORCE_REBUILD_INVENTORY : True\n",
      "[status] FORCE_REBUILD_STATES    : True\n",
      "[status] FORCE_REBUILD_COUNTRIES : True\n",
      "[status] FORCE_REBUILD_OVERLAP   : True\n",
      "[status] FORCE_REBUILD_PRECIP    : True\n",
      "ensure_dir ->  wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/\n",
      "ensure_dir ->  wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/\n",
      "\n",
      "__________________SOURCE FOLDERS__________________\n",
      "\n",
      "data_root           : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/\n",
      "user_root           : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/\n",
      "\n",
      "__________________________________________________\n",
      "ensure_dir ->  wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/\n",
      "daily_root          : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/\n",
      "\n",
      "ensure_dir ->  wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/reports/\n",
      "ensure_dir ->  wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/images/\n",
      "report_root         : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/reports/\n",
      "figs_dir (relative) : figures/\n",
      "image_root          : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/images/\n",
      "\n",
      "___________________SOURCE FILES___________________\n",
      "\n",
      "stations_read_name  : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-stations.txt\n",
      "inventory_read_name : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-inventory.txt\n",
      "countries_read_name : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-countries.txt\n",
      "states_read_name    : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-states.txt\n",
      "\n",
      "previous_csvgz_path  : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2024.csv.gz\n",
      "current_csvgz_path   : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2025.csv.gz\n",
      "\n",
      "___________________USER FOLDERS___________________\n",
      "ensure_dir ->  wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations.parquet/\n",
      "ensure_dir ->  wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet/\n",
      "ensure_dir ->  wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/countries.parquet/\n",
      "ensure_dir ->  wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/states.parquet/\n",
      "\n",
      "stations_write_name : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations.parquet/\n",
      "inventory_write_name : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet/\n",
      "countries_write_name: wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/countries.parquet/\n",
      "states_write_name   : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/states.parquet/\n",
      "ensure_dir ->  wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/enriched_stations.parquet/\n",
      "ensure_dir ->  wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/q2a_station_date_element.parquet/\n",
      "ensure_dir ->  wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/q1b32_overlap_counts.parquet/\n",
      "ensure_dir ->  wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/q1b32_overlap_counts.parquet/\n",
      "ensure_dir ->  wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/q2a-agg-precipitation.parquet/\n",
      "\n",
      "enriched_write_name : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/enriched_stations.parquet/\n",
      "stations_write_name : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations.parquet/\n",
      "overlap_counts_name : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/q1b32_overlap_counts.parquet/\n",
      "overlap_write_name  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/q1b32_overlap_counts.parquet/\n",
      "precip_write_name   : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/q2a-agg-precipitation.parquet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "bprint(\"GLOBAL VARIABLES\")\n",
    "notebook_run_time = time.time()\n",
    "val               = spark.range(1).select(F.date_format(F.current_timestamp(), 'yyyy.MM.dd HH:mm').alias('t')).first()['t']\n",
    "bprint()\n",
    "print(f\"[time] current time           :  {val}\")\n",
    "bprint()\n",
    "\n",
    "\n",
    "bprint(\"ENVIRONMENT\") \n",
    "print(\"Spark       :\", spark.version)\n",
    "print(\"Python tuple:\", sys.version_info[:3]) \n",
    "print(\"username    :\", username)\n",
    "print()\n",
    "\n",
    "bprint(\"DEEBUG LOGICALS\")\n",
    "#FORCE_OVERWRITE = False  # False means that if the file exists then we wont re-write it \n",
    "#FORCE_OVERWRITE = True   # True means overwrite all resultant files\n",
    "FORCE_REBUILD_ENRICHED  = True   #has_parquet(enriched_write_name)\n",
    "FORCE_REBUILD_STATIONS  = True    #has_parquet(stations_write_name)\n",
    "FORCE_REBUILD_INVENTORY = True    # has_parquet(inventory_write_name)\n",
    "FORCE_REBUILD_STATES    = True    #has_parquet(states_write_name)\n",
    "FORCE_REBUILD_COUNTRIES = True    #has_parquet(countries_write_name)\n",
    "\n",
    "FORCE_REBUILD_OVERLAP   = True    #has_parquet(overlap_write_name)\n",
    "FORCE_REBUILD_PRECIP    = True    #has_parquet(precip_write_path)\n",
    "\n",
    "print(f\"[status] FORCE_REBUILD_ENRICHED  :\", FORCE_REBUILD_ENRICHED)\n",
    "print(f\"[status] FORCE_REBUILD_STATIONS  :\", FORCE_REBUILD_STATIONS)\n",
    "print(f\"[status] FORCE_REBUILD_INVENTORY :\", FORCE_REBUILD_INVENTORY)\n",
    "print(f\"[status] FORCE_REBUILD_STATES    :\", FORCE_REBUILD_STATES)\n",
    "print(f\"[status] FORCE_REBUILD_COUNTRIES :\", FORCE_REBUILD_COUNTRIES)\n",
    "\n",
    "print(f\"[status] FORCE_REBUILD_OVERLAP   :\", FORCE_REBUILD_OVERLAP)\n",
    "print(f\"[status] FORCE_REBUILD_PRECIP    :\", FORCE_REBUILD_PRECIP)\n",
    "\n",
    "\n",
    "azure_account_name        = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "previous_year             = 2024  # full hear\n",
    "most_recent_year          = 2025  # currently building\n",
    "\n",
    "data_root      = f\"wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/\"\n",
    "user_root      = f\"wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/{username}/\"\n",
    " \n",
    "data_root      = ensure_dir(data_root)\n",
    "user_root      = ensure_dir(user_root) \n",
    "\n",
    "\n",
    "bprint(\"SOURCE FOLDERS\")\n",
    "print()\n",
    "print(\"data_root           :\", data_root) \n",
    "print(\"user_root           :\", user_root)\n",
    "bprint()\n",
    "\n",
    "daily_root     = ensure_dir(f\"{data_root}daily/\")\n",
    "\n",
    "print(\"daily_root          :\", daily_root)\n",
    "print()\n",
    "report_root    = ensure_dir(f\"{user_root}reports/\")\n",
    "image_root     = ensure_dir(f\"{data_root}images/\")\n",
    "figs_dir       = \"figures/\"\n",
    " \n",
    "print(\"report_root         :\", report_root)\n",
    "print(\"figs_dir (relative) :\", figs_dir)\n",
    "print(\"image_root          :\", image_root)\n",
    "\n",
    "bprint(\"SOURCE FILES\")\n",
    "stations_read_name   = f'{data_root}ghcnd-stations.txt'\n",
    "inventory_read_name  = f'{data_root}ghcnd-inventory.txt'\n",
    "countries_read_name  = f'{data_root}ghcnd-countries.txt'\n",
    "states_read_name     = f'{data_root}ghcnd-states.txt'\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"stations_read_name  :\", stations_read_name)\n",
    "print(\"inventory_read_name :\", inventory_read_name)\n",
    "print(\"countries_read_name :\", countries_read_name)\n",
    "print(\"states_read_name    :\", states_read_name)\n",
    "\n",
    "previous_csvgz_path  = f'{daily_root}2024.csv.gz' \n",
    "current_csvgz_path   = f'{daily_root}2025.csv.gz' \n",
    "\n",
    "print()\n",
    "print(\"previous_csvgz_path  :\", previous_csvgz_path)\n",
    "print(\"current_csvgz_path   :\", current_csvgz_path)\n",
    "\n",
    "bprint(\"USER FOLDERS\")\n",
    "  \n",
    "stations_write_name  =  ensure_dir(f'{user_root}stations.parquet')      #parquest file referenced by folder\n",
    "inventory_write_name =  ensure_dir(f'{user_root}inventory.parquet')\n",
    "countries_write_name =  ensure_dir(f'{user_root}countries.parquet')\n",
    "states_write_name    =  ensure_dir(f'{user_root}states.parquet')\n",
    "\n",
    "print()\n",
    "print(\"stations_write_name :\", stations_write_name)\n",
    "print(\"inventory_write_name :\", inventory_write_name)\n",
    "print(\"countries_write_name:\", countries_write_name)\n",
    "print(\"states_write_name   :\", states_write_name)\n",
    "\n",
    "#overlap_write_pathh  = ensure_dir(f'{user_root}q1b32_overlap_counts.parquet')\n",
    "#precip_write_path    = ensure_dir(f'{user_root}q2a-agg-precipitation.parquet')\n",
    " \n",
    "\n",
    "enriched_write_name  = ensure_dir(f\"{user_root}enriched_stations.parquet\" )\n",
    "station_date_element = ensure_dir(f\"{user_root}q2a_station_date_element.parquet\")\n",
    "overlap_counts_name  = ensure_dir(f'{user_root}q1b32_overlap_counts.parquet')\n",
    "overlap_write_name   = ensure_dir(f'{user_root}q1b32_overlap_counts.parquet')\n",
    "precip_write_name    = ensure_dir(f'{user_root}q2a-agg-precipitation.parquet') \n",
    "\n",
    "print()\n",
    "print(\"enriched_write_name :\", enriched_write_name)\n",
    "print(\"stations_write_name :\", stations_write_name)\n",
    "print(\"overlap_counts_name :\", overlap_counts_name)\n",
    "print(\"overlap_write_name  :\", overlap_write_name)\n",
    "print(\"precip_write_name   :\", precip_write_name) \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "586fca6e-683f-4426-b354-175f8eeb5000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__________________Process A1(a)1__________________\n",
      "daily_root ->  wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/\n",
      "Found 5 items\n",
      "drwxrwxrwx   -          0 1970-01-01 12:00 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily\n",
      "-rwxrwxrwx   1      3.6 K 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-countries.txt\n",
      "-rwxrwxrwx   1     33.6 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-inventory.txt\n",
      "-rwxrwxrwx   1      1.1 K 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-states.txt\n",
      "-rwxrwxrwx   1     10.6 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-stations.txt\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Process A1(a)1\")\n",
    "#!hdfs dfs -ls -h {data_root}\n",
    "notebook_run_time = time.time() \n",
    "print(\"daily_root -> \", daily_root)\n",
    "!hdfs dfs -ls -h {data_root} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e35147-add4-42c6-83e5-7c235be0d4d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__________________Process A1(a)2__________________\n",
      "daily_root -> wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/\n",
      "13.0 G  13.0 G  wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily\n",
      "Found 264 items\n",
      "-rwxrwxrwx   1      1.3 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1750.csv.gz\n",
      "-rwxrwxrwx   1      3.3 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1763.csv.gz\n",
      "-rwxrwxrwx   1      3.2 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1764.csv.gz\n",
      "-rwxrwxrwx   1      3.3 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1765.csv.gz\n",
      "-rwxrwxrwx   1      3.3 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1766.csv.gz\n",
      "-rwxrwxrwx   1      3.3 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1767.csv.gz\n",
      "-rwxrwxrwx   1      3.2 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1768.csv.gz\n",
      "-rwxrwxrwx   1      3.3 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1769.csv.gz\n",
      "-rwxrwxrwx   1      3.3 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1770.csv.gz\n",
      "-rwxrwxrwx   1      3.3 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1771.csv.gz\n",
      "-rwxrwxrwx   1      3.3 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1772.csv.gz\n",
      "-rwxrwxrwx   1      3.3 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1773.csv.gz\n",
      "-rwxrwxrwx   1      3.3 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1774.csv.gz\n",
      "-rwxrwxrwx   1      6.2 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1775.csv.gz\n",
      "-rwxrwxrwx   1      6.3 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1776.csv.gz\n",
      "-rwxrwxrwx   1      6.3 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1777.csv.gz\n",
      "-rwxrwxrwx   1      6.1 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1778.csv.gz\n",
      "-rwxrwxrwx   1        6 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1779.csv.gz\n",
      "-rwxrwxrwx   1      6.1 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1780.csv.gz\n",
      "-rwxrwxrwx   1      7.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1781.csv.gz\n",
      "-rwxrwxrwx   1      7.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1782.csv.gz\n",
      "-rwxrwxrwx   1      7.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1783.csv.gz\n",
      "-rwxrwxrwx   1      7.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1784.csv.gz\n",
      "-rwxrwxrwx   1      7.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1785.csv.gz\n",
      "-rwxrwxrwx   1      7.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1786.csv.gz\n",
      "-rwxrwxrwx   1      6.2 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1787.csv.gz\n",
      "-rwxrwxrwx   1      6.2 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1788.csv.gz\n",
      "-rwxrwxrwx   1      7.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1789.csv.gz\n",
      "-rwxrwxrwx   1      7.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1790.csv.gz\n",
      "-rwxrwxrwx   1      7.6 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1791.csv.gz\n",
      "-rwxrwxrwx   1      7.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1792.csv.gz\n",
      "-rwxrwxrwx   1      6.2 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1793.csv.gz\n",
      "-rwxrwxrwx   1      7.6 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1794.csv.gz\n",
      "-rwxrwxrwx   1      7.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1795.csv.gz\n",
      "-rwxrwxrwx   1      7.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1796.csv.gz\n",
      "-rwxrwxrwx   1      9.1 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1797.csv.gz\n",
      "-rwxrwxrwx   1      9.1 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1798.csv.gz\n",
      "-rwxrwxrwx   1      6.2 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1799.csv.gz\n",
      "-rwxrwxrwx   1      7.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1800.csv.gz\n",
      "-rwxrwxrwx   1      7.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1801.csv.gz\n",
      "-rwxrwxrwx   1      9.0 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1802.csv.gz\n",
      "-rwxrwxrwx   1      7.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1803.csv.gz\n",
      "-rwxrwxrwx   1      8.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1804.csv.gz\n",
      "-rwxrwxrwx   1      8.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1805.csv.gz\n",
      "-rwxrwxrwx   1      8.5 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1806.csv.gz\n",
      "-rwxrwxrwx   1      8.6 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1807.csv.gz\n",
      "-rwxrwxrwx   1      8.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1808.csv.gz\n",
      "-rwxrwxrwx   1      8.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1809.csv.gz\n",
      "-rwxrwxrwx   1      8.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1810.csv.gz\n",
      "-rwxrwxrwx   1      8.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1811.csv.gz\n",
      "-rwxrwxrwx   1      8.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1812.csv.gz\n",
      "-rwxrwxrwx   1      9.1 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1813.csv.gz\n",
      "-rwxrwxrwx   1     10.6 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1814.csv.gz\n",
      "-rwxrwxrwx   1     13.6 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1815.csv.gz\n",
      "-rwxrwxrwx   1     13.6 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1816.csv.gz\n",
      "-rwxrwxrwx   1     13.4 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1817.csv.gz\n",
      "-rwxrwxrwx   1     13.5 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1818.csv.gz\n",
      "-rwxrwxrwx   1     13.4 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1819.csv.gz\n",
      "-rwxrwxrwx   1     13.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1820.csv.gz\n",
      "-rwxrwxrwx   1     13.5 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1821.csv.gz\n",
      "-rwxrwxrwx   1     13.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1822.csv.gz\n",
      "-rwxrwxrwx   1     14.4 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1823.csv.gz\n",
      "-rwxrwxrwx   1     17.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1824.csv.gz\n",
      "-rwxrwxrwx   1     17.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1825.csv.gz\n",
      "-rwxrwxrwx   1     18.1 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1826.csv.gz\n",
      "-rwxrwxrwx   1     20.4 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1827.csv.gz\n",
      "-rwxrwxrwx   1     20.5 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1828.csv.gz\n",
      "-rwxrwxrwx   1     20.6 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1829.csv.gz\n",
      "-rwxrwxrwx   1     20.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1830.csv.gz\n",
      "-rwxrwxrwx   1     20.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1831.csv.gz\n",
      "-rwxrwxrwx   1     21.9 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1832.csv.gz\n",
      "-rwxrwxrwx   1     26.4 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1833.csv.gz\n",
      "-rwxrwxrwx   1     26.2 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1834.csv.gz\n",
      "-rwxrwxrwx   1     26.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1835.csv.gz\n",
      "-rwxrwxrwx   1     28.9 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1836.csv.gz\n",
      "-rwxrwxrwx   1     28.5 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1837.csv.gz\n",
      "-rwxrwxrwx   1     30.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1838.csv.gz\n",
      "-rwxrwxrwx   1     28.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1839.csv.gz\n",
      "-rwxrwxrwx   1     35.0 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1840.csv.gz\n",
      "-rwxrwxrwx   1     36.0 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1841.csv.gz\n",
      "-rwxrwxrwx   1     38.4 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1842.csv.gz\n",
      "-rwxrwxrwx   1     42.6 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1843.csv.gz\n",
      "-rwxrwxrwx   1     48.4 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1844.csv.gz\n",
      "-rwxrwxrwx   1     56.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1845.csv.gz\n",
      "-rwxrwxrwx   1     54.3 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1846.csv.gz\n",
      "-rwxrwxrwx   1     55.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1847.csv.gz\n",
      "-rwxrwxrwx   1     54.9 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1848.csv.gz\n",
      "-rwxrwxrwx   1     56.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1849.csv.gz\n",
      "-rwxrwxrwx   1     56.6 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1850.csv.gz\n",
      "-rwxrwxrwx   1     64.0 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1851.csv.gz\n",
      "-rwxrwxrwx   1     68.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1852.csv.gz\n",
      "-rwxrwxrwx   1     69.4 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1853.csv.gz\n",
      "-rwxrwxrwx   1     69.4 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1854.csv.gz\n",
      "-rwxrwxrwx   1     75.1 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1855.csv.gz\n",
      "-rwxrwxrwx   1     85.0 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1856.csv.gz\n",
      "-rwxrwxrwx   1     93.5 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1857.csv.gz\n",
      "-rwxrwxrwx   1    124.7 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1858.csv.gz\n",
      "-rwxrwxrwx   1    141.1 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1859.csv.gz\n",
      "-rwxrwxrwx   1    149.2 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1860.csv.gz\n",
      "-rwxrwxrwx   1    156.4 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1861.csv.gz\n",
      "-rwxrwxrwx   1    150.0 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1862.csv.gz\n",
      "-rwxrwxrwx   1    165.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1863.csv.gz\n",
      "-rwxrwxrwx   1    165.2 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1864.csv.gz\n",
      "-rwxrwxrwx   1    165.4 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1865.csv.gz\n",
      "-rwxrwxrwx   1    213.6 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1866.csv.gz\n",
      "-rwxrwxrwx   1    255.1 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1867.csv.gz\n",
      "-rwxrwxrwx   1    269.2 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1868.csv.gz\n",
      "-rwxrwxrwx   1    311.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1869.csv.gz\n",
      "-rwxrwxrwx   1    357.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1870.csv.gz\n",
      "-rwxrwxrwx   1    469.2 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1871.csv.gz\n",
      "-rwxrwxrwx   1    638.3 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1872.csv.gz\n",
      "-rwxrwxrwx   1    717.2 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1873.csv.gz\n",
      "-rwxrwxrwx   1    799.8 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1874.csv.gz\n",
      "-rwxrwxrwx   1    867.5 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1875.csv.gz\n",
      "-rwxrwxrwx   1    944.5 K 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1876.csv.gz\n",
      "-rwxrwxrwx   1      1.0 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1877.csv.gz\n",
      "-rwxrwxrwx   1      1.2 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1878.csv.gz\n",
      "-rwxrwxrwx   1      1.4 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1879.csv.gz\n",
      "-rwxrwxrwx   1      1.8 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1880.csv.gz\n",
      "-rwxrwxrwx   1      2.1 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1881.csv.gz\n",
      "-rwxrwxrwx   1      2.4 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1882.csv.gz\n",
      "-rwxrwxrwx   1      2.7 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1883.csv.gz\n",
      "-rwxrwxrwx   1      3.2 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1884.csv.gz\n",
      "-rwxrwxrwx   1      3.6 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1885.csv.gz\n",
      "-rwxrwxrwx   1      3.9 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1886.csv.gz\n",
      "-rwxrwxrwx   1      4.4 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1887.csv.gz\n",
      "-rwxrwxrwx   1      4.7 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1888.csv.gz\n",
      "-rwxrwxrwx   1      5.1 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1889.csv.gz\n",
      "-rwxrwxrwx   1      5.6 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1890.csv.gz\n",
      "-rwxrwxrwx   1      5.9 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1891.csv.gz\n",
      "-rwxrwxrwx   1      6.8 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1892.csv.gz\n",
      "-rwxrwxrwx   1     12.5 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1893.csv.gz\n",
      "-rwxrwxrwx   1     13.3 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1894.csv.gz\n",
      "-rwxrwxrwx   1     14.4 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1895.csv.gz\n",
      "-rwxrwxrwx   1     15.5 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1896.csv.gz\n",
      "-rwxrwxrwx   1     16.7 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1897.csv.gz\n",
      "-rwxrwxrwx   1     17.4 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1898.csv.gz\n",
      "-rwxrwxrwx   1     18.1 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1899.csv.gz\n",
      "-rwxrwxrwx   1     19.3 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1900.csv.gz\n",
      "-rwxrwxrwx   1     24.5 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1901.csv.gz\n",
      "-rwxrwxrwx   1     25.5 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1902.csv.gz\n",
      "-rwxrwxrwx   1     26.0 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1903.csv.gz\n",
      "-rwxrwxrwx   1     26.9 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1904.csv.gz\n",
      "-rwxrwxrwx   1     28.2 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1905.csv.gz\n",
      "-rwxrwxrwx   1     28.9 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1906.csv.gz\n",
      "-rwxrwxrwx   1     29.6 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1907.csv.gz\n",
      "-rwxrwxrwx   1     30.3 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1908.csv.gz\n",
      "-rwxrwxrwx   1     31.9 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1909.csv.gz\n",
      "-rwxrwxrwx   1     33.0 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1910.csv.gz\n",
      "-rwxrwxrwx   1     34.2 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1911.csv.gz\n",
      "-rwxrwxrwx   1     35.5 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1912.csv.gz\n",
      "-rwxrwxrwx   1     36.4 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1913.csv.gz\n",
      "-rwxrwxrwx   1     37.6 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1914.csv.gz\n",
      "-rwxrwxrwx   1     38.7 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1915.csv.gz\n",
      "-rwxrwxrwx   1     40.0 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1916.csv.gz\n",
      "-rwxrwxrwx   1     40.2 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1917.csv.gz\n",
      "-rwxrwxrwx   1     39.3 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1918.csv.gz\n",
      "-rwxrwxrwx   1     38.8 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1919.csv.gz\n",
      "-rwxrwxrwx   1     39.0 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1920.csv.gz\n",
      "-rwxrwxrwx   1     39.3 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1921.csv.gz\n",
      "-rwxrwxrwx   1     40.0 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1922.csv.gz\n",
      "-rwxrwxrwx   1     40.8 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1923.csv.gz\n",
      "-rwxrwxrwx   1     41.6 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1924.csv.gz\n",
      "-rwxrwxrwx   1     41.9 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1925.csv.gz\n",
      "-rwxrwxrwx   1     43.0 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1926.csv.gz\n",
      "-rwxrwxrwx   1     43.8 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1927.csv.gz\n",
      "-rwxrwxrwx   1     44.3 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1928.csv.gz\n",
      "-rwxrwxrwx   1     45.2 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1929.csv.gz\n",
      "-rwxrwxrwx   1     46.6 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1930.csv.gz\n",
      "-rwxrwxrwx   1     48.2 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1931.csv.gz\n",
      "-rwxrwxrwx   1     49.2 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1932.csv.gz\n",
      "-rwxrwxrwx   1     49.7 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1933.csv.gz\n",
      "-rwxrwxrwx   1     50.0 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1934.csv.gz\n",
      "-rwxrwxrwx   1     50.9 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1935.csv.gz\n",
      "-rwxrwxrwx   1     54.1 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1936.csv.gz\n",
      "-rwxrwxrwx   1     55.4 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1937.csv.gz\n",
      "-rwxrwxrwx   1     56.5 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1938.csv.gz\n",
      "-rwxrwxrwx   1     58.2 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1939.csv.gz\n",
      "-rwxrwxrwx   1     60.4 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1940.csv.gz\n",
      "-rwxrwxrwx   1     62.3 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1941.csv.gz\n",
      "-rwxrwxrwx   1     64.3 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1942.csv.gz\n",
      "-rwxrwxrwx   1     65.2 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1943.csv.gz\n",
      "-rwxrwxrwx   1     66.9 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1944.csv.gz\n",
      "-rwxrwxrwx   1     69.3 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1945.csv.gz\n",
      "-rwxrwxrwx   1     69.7 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1946.csv.gz\n",
      "-rwxrwxrwx   1     71.5 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1947.csv.gz\n",
      "-rwxrwxrwx   1     84.9 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1948.csv.gz\n",
      "-rwxrwxrwx   1     96.9 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1949.csv.gz\n",
      "-rwxrwxrwx   1     99.9 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1950.csv.gz\n",
      "-rwxrwxrwx   1    103.0 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1951.csv.gz\n",
      "-rwxrwxrwx   1    104.3 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1952.csv.gz\n",
      "-rwxrwxrwx   1    105.8 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1953.csv.gz\n",
      "-rwxrwxrwx   1    107.8 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1954.csv.gz\n",
      "-rwxrwxrwx   1    110.1 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1955.csv.gz\n",
      "-rwxrwxrwx   1    112.3 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1956.csv.gz\n",
      "-rwxrwxrwx   1    114.8 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1957.csv.gz\n",
      "-rwxrwxrwx   1    116.0 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1958.csv.gz\n",
      "-rwxrwxrwx   1    118.4 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1959.csv.gz\n",
      "-rwxrwxrwx   1    120.8 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1960.csv.gz\n",
      "-rwxrwxrwx   1    124.5 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1961.csv.gz\n",
      "-rwxrwxrwx   1    127.1 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1962.csv.gz\n",
      "-rwxrwxrwx   1    130.0 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1963.csv.gz\n",
      "-rwxrwxrwx   1    131.0 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1964.csv.gz\n",
      "-rwxrwxrwx   1    135.1 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1965.csv.gz\n",
      "-rwxrwxrwx   1    137.0 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1966.csv.gz\n",
      "-rwxrwxrwx   1    138.3 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1967.csv.gz\n",
      "-rwxrwxrwx   1    137.9 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1968.csv.gz\n",
      "-rwxrwxrwx   1    139.6 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1969.csv.gz\n",
      "-rwxrwxrwx   1    140.5 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1970.csv.gz\n",
      "-rwxrwxrwx   1    135.3 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1971.csv.gz\n",
      "-rwxrwxrwx   1    134.4 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1972.csv.gz\n",
      "-rwxrwxrwx   1    140.8 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1973.csv.gz\n",
      "-rwxrwxrwx   1    142.0 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1974.csv.gz\n",
      "-rwxrwxrwx   1    141.5 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1975.csv.gz\n",
      "-rwxrwxrwx   1    141.4 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1976.csv.gz\n",
      "-rwxrwxrwx   1    141.2 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1977.csv.gz\n",
      "-rwxrwxrwx   1    141.4 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1978.csv.gz\n",
      "-rwxrwxrwx   1    141.6 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1979.csv.gz\n",
      "-rwxrwxrwx   1    142.0 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1980.csv.gz\n",
      "-rwxrwxrwx   1    145.0 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1981.csv.gz\n",
      "-rwxrwxrwx   1    146.7 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1982.csv.gz\n",
      "-rwxrwxrwx   1    148.0 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1983.csv.gz\n",
      "-rwxrwxrwx   1    146.6 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1984.csv.gz\n",
      "-rwxrwxrwx   1    145.1 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1985.csv.gz\n",
      "-rwxrwxrwx   1    144.1 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1986.csv.gz\n",
      "-rwxrwxrwx   1    144.1 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1987.csv.gz\n",
      "-rwxrwxrwx   1    144.9 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1988.csv.gz\n",
      "-rwxrwxrwx   1    145.2 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1989.csv.gz\n",
      "-rwxrwxrwx   1    145.4 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1990.csv.gz\n",
      "-rwxrwxrwx   1    146.0 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1991.csv.gz\n",
      "-rwxrwxrwx   1    146.2 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1992.csv.gz\n",
      "-rwxrwxrwx   1    145.1 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1993.csv.gz\n",
      "-rwxrwxrwx   1    144.1 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1994.csv.gz\n",
      "-rwxrwxrwx   1    143.8 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1995.csv.gz\n",
      "-rwxrwxrwx   1    144.0 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1996.csv.gz\n",
      "-rwxrwxrwx   1    142.8 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1997.csv.gz\n",
      "-rwxrwxrwx   1    145.5 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1998.csv.gz\n",
      "-rwxrwxrwx   1    148.1 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/1999.csv.gz\n",
      "-rwxrwxrwx   1    149.9 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2000.csv.gz\n",
      "-rwxrwxrwx   1    152.2 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2001.csv.gz\n",
      "-rwxrwxrwx   1    153.8 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2002.csv.gz\n",
      "-rwxrwxrwx   1    157.2 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2003.csv.gz\n",
      "-rwxrwxrwx   1    159.6 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2004.csv.gz\n",
      "-rwxrwxrwx   1    156.6 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2005.csv.gz\n",
      "-rwxrwxrwx   1    162.7 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2006.csv.gz\n",
      "-rwxrwxrwx   1    165.7 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2007.csv.gz\n",
      "-rwxrwxrwx   1    173.1 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2008.csv.gz\n",
      "-rwxrwxrwx   1    176.1 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2009.csv.gz\n",
      "-rwxrwxrwx   1    177.7 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2010.csv.gz\n",
      "-rwxrwxrwx   1    168.9 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2011.csv.gz\n",
      "-rwxrwxrwx   1    166.3 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2012.csv.gz\n",
      "-rwxrwxrwx   1    161.5 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2013.csv.gz\n",
      "-rwxrwxrwx   1    159.9 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2014.csv.gz\n",
      "-rwxrwxrwx   1    162.3 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2015.csv.gz\n",
      "-rwxrwxrwx   1    163.6 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2016.csv.gz\n",
      "-rwxrwxrwx   1    163.3 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2017.csv.gz\n",
      "-rwxrwxrwx   1    163.4 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2018.csv.gz\n",
      "-rwxrwxrwx   1    162.3 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2019.csv.gz\n",
      "-rwxrwxrwx   1    163.2 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2020.csv.gz\n",
      "-rwxrwxrwx   1    165.9 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2021.csv.gz\n",
      "-rwxrwxrwx   1    166.1 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2022.csv.gz\n",
      "-rwxrwxrwx   1    166.2 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2023.csv.gz\n",
      "-rwxrwxrwx   1    161.6 M 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2024.csv.gz\n",
      "-rwxrwxrwx   1     81.7 M 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2025.csv.gz\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Process A1(a)2\")\n",
    "#!hdfs dfs -du -s -h {daily_root} \n",
    "print(f\"daily_root -> {daily_root}\")\n",
    "!hdfs dfs -du -s -h {daily_root} \n",
    "!hdfs dfs -ls    -h {daily_root} \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c741463-9dfb-4c39-ba88-1d35afcecedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__________________Process A1(c)__________________\n",
      "\n",
      "__________________Process A1(c)__________________\n",
      "Raw result: ['13993455698  13993455698  wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily']\n",
      "\n",
      "Daily size (bytes): 13993455698\n",
      "Daily size (MB)   : 13345.199296951294\n",
      "['Found 5 items', 'drwxrwxrwx   -          0 1970-01-01 12:00 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily', '-rwxrwxrwx   1       3659 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-countries.txt', '-rwxrwxrwx   1   35272064 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-inventory.txt', '-rwxrwxrwx   1       1086 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-states.txt', '-rwxrwxrwx   1   11150502 2025-08-01 21:31 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-stations.txt']\n",
      "\n",
      "______________________________________________________________________\n",
      "[result] daily size (bytes): 13,993,455,698\n",
      "[result] daily size (MB)   : 13345.20\n",
      "[result] meta-data (bytes) : 46,427,311\n",
      "[result] meta-data (MB)    : 44.28\n",
      "[time]   Cell time (sec)   :  3.48\n",
      "[time]   Cell time (min)   :  0.06\n",
      "[time] notebook_run_time (min):  0.16\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Process A1(c)\")\n",
    "#Daily folder size, meta-data size \n",
    "bprint(\"Process A1(c)\")\n",
    "cell_time = time.time() \n",
    "result = get_ipython().getoutput(f\"hdfs dfs -du -s {daily_root}\")\n",
    "print(\"Raw result:\", result)\n",
    "print()\n",
    "daily_size_MByte = int(result[0].split()[0])\n",
    "daily_size_MByte = daily_size_MByte/ (1024**2)\n",
    "daily_size_Bytes = int(result[0].split()[0])\n",
    "print(\"Daily size (bytes):\", daily_size_Bytes)\n",
    "print(\"Daily size (MB)   :\", daily_size_MByte)\n",
    " \n",
    "lines = get_ipython().getoutput(f\"hdfs dfs -ls {data_root}\")\n",
    "print(lines)\n",
    "meta_size_Bytes  = 0\n",
    "other_size_MByte = 0\n",
    "other_size_Mbyte = meta_size_Bytes / (1024**2)\n",
    "for line in lines:\n",
    "    parts = line.split()\n",
    "    if len(parts) >= 6 and parts[0].startswith('-'):   # file, not directory\n",
    "        size = int(parts[2])                           # file size is parts[2]  \n",
    "        meta_size_Bytes += size\n",
    "        \n",
    "print()\n",
    "print(\"_\"*70) \n",
    "print(f\"[result] daily size (bytes): {daily_size_Bytes:,d}\")\n",
    "print(f\"[result] daily size (MB)   : {daily_size_MByte:.2f}\")\n",
    "print(f\"[result] meta-data (bytes) : {meta_size_Bytes:,d}\")\n",
    "print(f\"[result] meta-data (MB)    : {meta_size_Bytes / (1024**2):.2f}\")\n",
    "\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\") \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc5edd-f44d-41c9-8add-129638a53a8f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b46ac03-a6e2-4ade-aa56-be09f15807a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIAGNOSTIC 51 capture filename,lines (year,lines)\n",
      "current_csvgz_path : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2025.csv.gz\n",
      "-rwxrwxrwx   1   85656432 2025-08-01 21:30 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2025.csv.gz\n",
      "\n",
      "__________________________________________________\n",
      "Sample parsed rows: [(2025, 85656432)]\n",
      "rows : [(2025, 85656432)]\n",
      "[time] cell_time (sec):   1.51\n",
      "[time] cell_time (min):   0.03\n",
      "[time] notebook_run_time (min):  0.18\n"
     ]
    }
   ],
   "source": [
    "bprint(\"DIAGNOSTIC 51 capture filename,lines (year,lines)\")\n",
    "\n",
    "\n",
    "cell_time = time.time()  \n",
    "print(\"current_csvgz_path :\",current_csvgz_path)\n",
    "cmd        = f\"hdfs dfs -ls {current_csvgz_path       }\"\n",
    "result     = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "lines      = result.stdout.strip().split(\"\\n\")\n",
    "rows       = []\n",
    "#print(lines)\n",
    "for line in lines:\n",
    "    print(line)\n",
    "    parts = line.split()\n",
    "    #print(parts)\n",
    "    if len(parts) < 6:\n",
    "        #print(\"continue\")\n",
    "        continue\n",
    "    size = int(parts[2])\n",
    "    path = parts[-1]\n",
    "    if path.endswith(\".csv.gz\"):\n",
    "        year = int(path.split(\"/\")[-1].replace(\".csv.gz\", \"\"))\n",
    "        rows.append((year, size))\n",
    "        #print(year)\n",
    "        \n",
    " \n",
    "bprint()\n",
    "print(\"Sample parsed rows:\", rows[:5])\n",
    "print(\"rows :\",rows)\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad253ff-b9e0-45ba-a3a4-618c071edcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "______________________Q1(b)3______________________\n",
      "Schema:\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- compressed_bytes: integer (nullable = true)\n",
      "\n",
      "[time] cell_time (sec):   0.05\n",
      "[time] cell_time (min):   0.00\n",
      "[time] notebook_run_time (min):  0.19\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Q1(b)3\") \n",
    "# Build Spark DataFrame with exactly the 2 integer columns  \n",
    "\n",
    "cell_time = time.time()  \n",
    "# Define schema \n",
    "schema = StructType([\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"compressed_bytes\", IntegerType(), True)\n",
    " ])\n",
    "\n",
    "# Create Spark DataFrame with schema\n",
    "year_sizes_df = spark.createDataFrame(rows, schema)\n",
    "\n",
    "print(\"Schema:\")\n",
    "year_sizes_df.printSchema() \n",
    "\n",
    "cell_time =(time.time() - cell_time)  \n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c82399ee-c86a-4840-be91-67149fce50d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "______________________Q1(b)6______________________\n",
      "+----+----------------+\n",
      "|year|compressed_bytes|\n",
      "+----+----------------+\n",
      "|2025|85656432        |\n",
      "+----+----------------+\n",
      "\n",
      "Row count: 1\n",
      "[time] cell_time (sec):   2.04\n",
      "[time] cell_time (min):   0.03\n",
      "[time] notebook_run_time (min):  0.22\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Q1(b)6\") \n",
    "cell_time = time.time() \n",
    "year_sizes_df.show(10, truncate=False)\n",
    "print(\"Row count:\", year_sizes_df.count())\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c14ad-23d1-4adb-9919-9829c3eff9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "______________________Q1(b)6______________________\n",
      "Reading all years: wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/*.csv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                        (0 + 8) / 107]"
     ]
    }
   ],
   "source": [
    "bprint(\"Q1(b)6\") # Build FULL `daily` from all years (wildcard under daily_root)\n",
    "\n",
    "cell_time = time.time()  \n",
    "\n",
    "# Ensure the schema exists (uses your column names, incl. OBSTIME)\n",
    "if \"daily_schema\" not in globals():\n",
    "    daily_schema = T.StructType([\n",
    "        T.StructField(\"ID\",       T.StringType(), True),\n",
    "        T.StructField(\"DATE\",     T.StringType(), True),  # parsed to DateType below\n",
    "        T.StructField(\"ELEMENT\",  T.StringType(), True),\n",
    "        T.StructField(\"VALUE\",    T.IntegerType(), True),\n",
    "        T.StructField(\"MFLAG\",    T.StringType(), True),\n",
    "        T.StructField(\"QFLAG\",    T.StringType(), True),\n",
    "        T.StructField(\"SFLAG\",    T.StringType(), True),\n",
    "        T.StructField(\"OBSTIME\",  T.StringType(), True),\n",
    "    ])\n",
    "\n",
    "print(\"Reading all years:\", f\"{daily_root}*.csv.gz\")\n",
    "\n",
    "_df = spark.read.csv(\n",
    "    f\"{daily_root}*.csv.gz\",\n",
    "    schema=daily_schema,\n",
    "    header=False,            # flip to True if your files have a header row\n",
    "    mode=\"PERMISSIVE\"\n",
    ")\n",
    "\n",
    "# Some dumps use STATION instead of ID\n",
    "if \"STATION\" in _df.columns and \"ID\" not in _df.columns:\n",
    "    _df = _df.withColumnRenamed(\"STATION\", \"ID\")\n",
    "\n",
    "daily_for_overlap = (\n",
    "    _df.withColumn(\n",
    "        \"DATE\",\n",
    "        F.coalesce(F.to_date(\"DATE\", \"yyyy-MM-dd\"),\n",
    "                   F.to_date(\"DATE\", \"yyyyMMdd\"))\n",
    "    )\n",
    "    .withColumn(\"ID\", F.upper(F.trim(F.col(\"ID\"))))\n",
    "    .select(\"ID\", \"DATE\", \"ELEMENT\", \"VALUE\", \"MFLAG\", \"QFLAG\", \"SFLAG\", \"OBSTIME\")\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "# Touch to materialise cache\n",
    "_ = daily_for_overlap.limit(1).count()\n",
    "\n",
    "show_df(daily_for_overlap.limit(10), name=\"daily (full, wildcard)\")\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8cade-6a39-48f2-a050-dffecad17f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q1(b) NEW -ER\") # \n",
    "# Build  meta-data dataframe\n",
    "\n",
    "cell_time = time.time() \n",
    "rows        = []\n",
    "# NOTE: use -du with a files-only glob so size + path are stable (behaves like the GOOD run)\n",
    "lines       = get_ipython().getoutput(f'hdfs dfs -du \"{data_root}/ghcnd-*.txt\"')\n",
    "print(lines)\n",
    "for line in lines:                 # <-- was lines[15:] (skipped everything)\n",
    "    #print()\n",
    "    parts = line.split()\n",
    "    #print(line)\n",
    "    #print(parts)\n",
    "    #print(len(parts))\n",
    "    #print(parts[0])\n",
    "\n",
    "    if len(parts) >= 2:\n",
    "        size = int(parts[0])                 # bytes from `hdfs dfs -du`\n",
    "        path = parts[-1].strip()             # full path\n",
    "        #print(\"size:\",size)\n",
    "        print(path)\n",
    "        # if not path.startswith(daily_root):   # files-only glob excludes /daily already\n",
    "        rows.append((path, size))             # not compressed\n",
    "\n",
    "print(\"\\nMetadata file count:\", len(rows))\n",
    "print(\"Sample parsed rows:\", rows[:5])\n",
    "# Spark schema\n",
    "schema = StructType([\n",
    "    StructField(\"path\", StringType(), False),\n",
    "    StructField(\"uncompressed_bytes\", LongType(), False),\n",
    "])\n",
    "\n",
    "metadata_files_df = spark.createDataFrame(rows, schema)\n",
    "\n",
    "print(\"\\n[spark] other_files_df schema:\")\n",
    "metadata_files_df.printSchema()\n",
    "print(\"[spark] sample:\")\n",
    "metadata_files_df.show( truncate=False)\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a6fb6-ac8c-46c5-a30c-cee9f136b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"q1(bb)\") #  q1(bb)  — dataset sizes from HDFS + estimated uncompressed for daily\n",
    "\n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "# \n",
    "print(f\"daily_root          : {daily_root}\")\n",
    "print(f\"inventory_read_name : {inventory_read_name}\")\n",
    "print(f\"stations_read_name  : {stations_read_name}\")\n",
    "print(f\"countries_read_name : {countries_read_name}\")\n",
    "print(f\"states_read_name    : {states_read_name}\")\n",
    "sizes = {\n",
    "    \"daily (folder)\":      du_bytes(daily_root),\n",
    "    \"ghcnd-inventory.txt\": du_bytes(inventory_read_name),\n",
    "    \"ghcnd-stations.txt\":  du_bytes(stations_read_name),\n",
    "    \"ghcnd-countries.txt\": du_bytes(countries_read_name),\n",
    "    \"ghcnd-states.txt\":    du_bytes(states_read_name),\n",
    "}\n",
    "total_bytes = sum(sizes.values())\n",
    "\n",
    "# Simple gzip expansion estimate  \n",
    "gzip_expansion_factor = 3.3\n",
    "est_uncomp_daily = int(sizes[\"daily (folder)\"] * gzip_expansion_factor)\n",
    "\n",
    "print(\"[status] gzip_expansion_factor ->\", gzip_expansion_factor)\n",
    "print(\"[status] sizes (bytes) ->\", sizes)\n",
    "print(\"[status] total (bytes)  ->\", total_bytes)\n",
    "\n",
    "# Present as a small Spark table (sizes in MB for readability)\n",
    "to_mb = 1024**2\n",
    "rows = []\n",
    "for k, v in sizes.items():\n",
    "    rows.append((k, round(v/to_mb, 2)))\n",
    "rows.append((\"TOTAL\", round(total_bytes/to_mb, 2)))\n",
    "\n",
    "sizes_df = spark.createDataFrame(rows, [\"dataset\", \"size_mb\"])\n",
    "sizes_df.show(truncate=False)\n",
    "\n",
    "print(f\"[status] estimated uncompressed daily (MB): {est_uncomp_daily/to_mb:,.2f}\")\n",
    "\n",
    " \n",
    "user_out = f\"wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/processing\"\n",
    "spark.createDataFrame(\n",
    "    [(k, v, round(v/to_mb,2)) for k, v in sizes.items()] + [(\"TOTAL\", total_bytes, round(total_bytes/to_mb, 2))]\n",
    "    , [\"dataset\",\"size_bytes\",\"size_mb\"]\n",
    ").coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(f\"{user_out}/dew59_sizes_mb_csv\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1707a3f4-7854-4820-924f-e07441473389",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process A X (1)\") \n",
    "# One pie: All other years (≤2023) vs 2024 vs 2025 vs Metadata\n",
    "# De-overlapped balloons with stronger vertical separation and horizontal staggering.\n",
    "# Comments in British English; diagnostics use [status].\n",
    "\n",
    "\n",
    "cell_time = time.time()  \n",
    "\n",
    "print(\"previous_csvgz_path ->\", previous_csvgz_path)\n",
    "\n",
    "# --- Locate the directory that holds the yearly CSV.GZ files\n",
    "daily_dir = previous_csvgz_path.rsplit(\"/\", 1)[0]\n",
    "print(\"[status] daily_dir ->\", daily_dir)\n",
    "\n",
    "\n",
    "\n",
    "# --- Prefer `ls`; fall back to `du` if needed\n",
    "ls_cmd = f'hdfs dfs -ls {daily_dir}'\n",
    "ls_lines = get_ipython().getoutput(ls_cmd)\n",
    "print(\"[status] ls CMD ->\", ls_cmd)\n",
    "print(\"[status] ls returned\", len(ls_lines), \"lines\")\n",
    "\n",
    "csvgz_records = []\n",
    "for ln in ls_lines:\n",
    "    if \".csv.gz\" not in ln:\n",
    "        continue\n",
    "    size, path = _parse_ls_bytes(ln)\n",
    "    if size is None:\n",
    "        continue\n",
    "    csvgz_records.append((size, path))\n",
    "\n",
    "if not csvgz_records:\n",
    "    glob_path = f\"{daily_dir}/*.csv.gz\"\n",
    "    du_cmd = f'hdfs dfs -du {glob_path}'\n",
    "    du_lines = get_ipython().getoutput(du_cmd)\n",
    "    print(\"[status] du CMD (fallback) ->\", du_cmd)\n",
    "    for ln in du_lines:\n",
    "        size, path = _parse_du_bytes(ln)\n",
    "        if size is None or \".csv.gz\" not in (path or \"\"):\n",
    "            continue\n",
    "        csvgz_records.append((size, path))\n",
    "\n",
    "print(f\"[status] matched *.csv.gz files -> {len(csvgz_records)}\")\n",
    "if csvgz_records[:3]:\n",
    "    print(\"[status] first 3 matches ->\", [p for _, p in csvgz_records[:3]])\n",
    "\n",
    "# --- Year buckets\n",
    "sum_by_year = {}\n",
    "total_csvgz_bytes = 0\n",
    "for size, path in csvgz_records:\n",
    "    base = path.rsplit(\"/\", 1)[-1].replace(\".csv.gz\", \"\")\n",
    "    try:\n",
    "        yr = int(base)\n",
    "        sum_by_year[yr] = sum_by_year.get(yr, 0) + size\n",
    "    except ValueError:\n",
    "        print(f\"[status] Non-numeric year stem for CSV.GZ, included in total: {base}\")\n",
    "    total_csvgz_bytes += size\n",
    "\n",
    "sum_2024_bytes = sum_by_year.get(2024, 0)\n",
    "sum_2025_bytes = sum_by_year.get(2025, 0)\n",
    "sum_pre2024_bytes = max(total_csvgz_bytes - sum_2024_bytes - sum_2025_bytes, 0)\n",
    "\n",
    "print(\"[status] sum_pre2024 (bytes) ->\", sum_pre2024_bytes)\n",
    "print(\"[status] sum_2024    (bytes) ->\", sum_2024_bytes)\n",
    "print(\"[status] sum_2025    (bytes) ->\", sum_2025_bytes)\n",
    "\n",
    "# --- Metadata (separate folder)\n",
    "metadata_total = 0\n",
    "if 'metadata_files_df' in globals():\n",
    "    metadata_total = metadata_files_df.agg({\"uncompressed_bytes\": \"sum\"}).collect()[0][0]\n",
    "else:\n",
    "    print(\"[status] WARNING: metadata_files_df is not defined; setting metadata_total to 0.\")\n",
    "print(\"[status] metadata_total (bytes) ->\", metadata_total)\n",
    "\n",
    "# --- Bytes → MiB\n",
    "to_mb = 1024**2\n",
    "other_mb = sum_pre2024_bytes / to_mb\n",
    "y2024_mb = sum_2024_bytes    / to_mb\n",
    "y2025_mb = sum_2025_bytes    / to_mb\n",
    "meta_mb  = metadata_total    / to_mb\n",
    "\n",
    "print(f\"[status] other_mb (≤2023) = {other_mb:,.1f} MB\")\n",
    "print(f\"[status] 2024_mb          = {y2024_mb:,.1f} MB\")\n",
    "print(f\"[status] 2025_mb          = {y2025_mb:,.1f} MB\")\n",
    "print(f\"[status] meta_mb          = {meta_mb:,.1f} MB\")\n",
    "\n",
    "# --- Pie inputs\n",
    "labels_full = [\n",
    "    \"All other years (compressed)\\n≤2023\",\n",
    "    \"Daily 2024 (compressed)\",\n",
    "    \"Daily 2025 (compressed)\",\n",
    "    \"Metadata (.txt files)\"\n",
    "]\n",
    "sizes = [other_mb, y2024_mb, y2025_mb, meta_mb]\n",
    "\n",
    "# Only the big slice keeps its label/percentage on the wedge\n",
    "pie_labels = [\"All other years (compressed)\\n≤2023\", \"\", \"\", \"\"]\n",
    "explode = [0.00, 0.07, 0.07, 0.07]\n",
    "\n",
    "# --- Plot\n",
    "rotation_deg = -90\n",
    "print(f\"[status] applying rotation: {rotation_deg} degrees (clockwise)\")\n",
    "fig, ax = plt.subplots(figsize=(10.4, 8.8))  # a little wider again for spacing\n",
    "start_angle = 90 + rotation_deg\n",
    "ax.margins(x=0.42)  # extra breathing room for call-outs on the right\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    sizes,\n",
    "    labels=pie_labels,\n",
    "    explode=explode,\n",
    "    startangle=start_angle,\n",
    "    autopct=\"%1.2f%%\",\n",
    "    labeldistance=1.05,\n",
    "    pctdistance=0.68\n",
    ")\n",
    "\n",
    "ax.set_title(\"Proportion of GHCN’s Daily (compressed): ≤2023 vs 2024 vs 2025 vs Metadata\")\n",
    "ax.set_aspect(\"equal\")\n",
    "\n",
    "# --- Style: big slice shows label and yellow percentage; small slices show neither on the wedge\n",
    "big_idx = 0\n",
    "small_indices = [1, 2, 3]\n",
    "texts[big_idx].set_color(\"black\")\n",
    "texts[big_idx].set_fontweight(\"bold\")\n",
    "autotexts[big_idx].set_color(\"yellow\")\n",
    "autotexts[big_idx].set_fontweight(\"bold\")\n",
    "for si in small_indices:\n",
    "    texts[si].set_text(\"\")\n",
    "    autotexts[si].set_text(\"\")\n",
    "\n",
    "# --- Balloon call-outs for the three small slices with stronger de-overlap\n",
    "callouts = [\n",
    "    (1, labels_full[1], y2024_mb),\n",
    "    (2, labels_full[2], y2025_mb),\n",
    "    (3, labels_full[3], meta_mb),\n",
    "]\n",
    "\n",
    "# Compute anchor points on the pie for each slice\n",
    "anchors = {}\n",
    "for idx, _, _ in callouts:\n",
    "    w = wedges[idx]\n",
    "    theta_mid = (w.theta1 + w.theta2) / 2.0\n",
    "    r = getattr(w, \"r\", 1.0)\n",
    "    x0 = r * np.cos(np.deg2rad(theta_mid))\n",
    "    y0 = r * np.sin(np.deg2rad(theta_mid))\n",
    "    anchors[idx] = (x0, y0, theta_mid)\n",
    "\n",
    "# Prepare target positions (sorted by y) and enforce minimum separation\n",
    "y_targets = []\n",
    "for idx, _, _ in callouts:\n",
    "    x0, y0, _ = anchors[idx]\n",
    "    y_targets.append([idx, y0])\n",
    "\n",
    "# Sort by vertical position\n",
    "y_targets.sort(key=lambda t: t[1])\n",
    "\n",
    "# Strengthened separation and horizontal staggering\n",
    "min_sep = 0.32      # increase vertical gap between neighbours\n",
    "base_x  = 1.70      # starting x for the leftmost of the three call-outs\n",
    "x_step  = 0.22      # horizontal stagger step\n",
    "print(f\"[status] balloon min_sep -> {min_sep}\")\n",
    "print(f\"[status] balloon base_x/x_step -> {base_x} / {x_step}\")\n",
    "\n",
    "# Enforce vertical spacing\n",
    "for i in range(1, len(y_targets)):\n",
    "    prev = y_targets[i-1]\n",
    "    curr = y_targets[i]\n",
    "    if curr[1] - prev[1] < min_sep:\n",
    "        curr[1] = prev[1] + min_sep\n",
    "\n",
    "# Assign final positions with staggered x\n",
    "final_positions = []\n",
    "for i, (idx, ytext) in enumerate(y_targets):\n",
    "    xtext = base_x + i * x_step\n",
    "    final_positions.append((idx, xtext, ytext))\n",
    "\n",
    "# Use gentle, varied curvature to reduce crossings\n",
    "arc_rads = {final_positions[0][0]: -0.25,\n",
    "            final_positions[1][0]:  0.00,\n",
    "            final_positions[2][0]:  0.25}\n",
    "\n",
    "# Now annotate using adjusted positions\n",
    "total = max(sum(sizes), 1e-12)\n",
    "for idx, xtext, ytext in final_positions:\n",
    "    label = labels_full[idx]\n",
    "    value_mb = sizes[idx]\n",
    "    x0, y0, theta_mid = anchors[idx]\n",
    "    pct_text = f\"{(value_mb / total) * 100:0.2f}%\"\n",
    "    annot_text = f\"{label}\\n~{pct_text}\\n{value_mb:,.2f} MB\"\n",
    "\n",
    "    ax.annotate(\n",
    "        annot_text,\n",
    "        xy=(x0, y0),\n",
    "        xytext=(xtext, ytext),\n",
    "        ha=\"left\", va=\"center\",\n",
    "        arrowprops=dict(\n",
    "            arrowstyle=\"->\",\n",
    "            linewidth=1,\n",
    "            connectionstyle=f\"arc3,rad={arc_rads.get(idx, 0.2)}\"\n",
    "        ),\n",
    "        bbox=dict(boxstyle=\"round,pad=0.35\", edgecolor=\"black\", facecolor=\"white\"),\n",
    "        zorder=15,\n",
    "        annotation_clip=False\n",
    "    )\n",
    "    print(f\"[status] balloon -> idx={idx}, theta_mid={theta_mid:.2f}, target=({xtext:.2f},{ytext:.2f})\")\n",
    "\n",
    "# --- Save figure  \n",
    "out_dir = \"figures\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"dew59_pie_all-upto2023_vs_2024_2025_metadata.png\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(out_path, dpi=300)\n",
    "print(f\"[status] pie chart saved -> {out_path}\")\n",
    "\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aac052-de7e-4506-bad7-cfe7fa5cae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process A X (2) \") \n",
    "# One pie: All other years (≤2023) vs 2024 vs 2025 vs Metadata\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "print(\"previous_csvgz_path ->\", previous_csvgz_path)\n",
    "\n",
    "# --- Locate the directory that holds the yearly CSV.GZ files\n",
    "daily_dir = previous_csvgz_path.rsplit(\"/\", 1)[0]\n",
    "print(\"[status] daily_dir ->\", daily_dir)\n",
    "\n",
    "\n",
    "\n",
    "# --- Prefer `ls`; fall back to `du` if needed\n",
    "ls_cmd = f'hdfs dfs -ls {daily_dir}'\n",
    "ls_lines = get_ipython().getoutput(ls_cmd)\n",
    "print(\"[status] ls CMD ->\", ls_cmd)\n",
    "print(\"[status] ls returned\", len(ls_lines), \"lines\")\n",
    "\n",
    "csvgz_records = []\n",
    "for ln in ls_lines:\n",
    "    if \".csv.gz\" not in ln:\n",
    "        continue\n",
    "    size, path = _parse_ls_bytes(ln)\n",
    "    if size is None:\n",
    "        continue\n",
    "    csvgz_records.append((size, path))\n",
    "\n",
    "if not csvgz_records:\n",
    "    glob_path = f\"{daily_dir}/*.csv.gz\"\n",
    "    du_cmd = f'hdfs dfs -du {glob_path}'\n",
    "    du_lines = get_ipython().getoutput(du_cmd)\n",
    "    print(\"[status] du CMD (fallback) ->\", du_cmd)\n",
    "    for ln in du_lines:\n",
    "        size, path = _parse_du_bytes(ln)\n",
    "        if size is None or \".csv.gz\" not in (path or \"\"):\n",
    "            continue\n",
    "        csvgz_records.append((size, path))\n",
    "\n",
    "print(f\"[status] matched *.csv.gz files -> {len(csvgz_records)}\")\n",
    "if csvgz_records[:3]:\n",
    "    print(\"[status] first 3 matches ->\", [p for _, p in csvgz_records[:3]])\n",
    "\n",
    "# --- Year buckets\n",
    "sum_by_year = {}\n",
    "total_csvgz_bytes = 0\n",
    "for size, path in csvgz_records:\n",
    "    base = path.rsplit(\"/\", 1)[-1].replace(\".csv.gz\", \"\")\n",
    "    try:\n",
    "        yr = int(base)\n",
    "        sum_by_year[yr] = sum_by_year.get(yr, 0) + size\n",
    "    except ValueError:\n",
    "        print(f\"[status] Non-numeric year stem for CSV.GZ, included in total: {base}\")\n",
    "    total_csvgz_bytes += size\n",
    "\n",
    "sum_2024_bytes = sum_by_year.get(2024, 0)\n",
    "sum_2025_bytes = sum_by_year.get(2025, 0)\n",
    "sum_pre2024_bytes = max(total_csvgz_bytes - sum_2024_bytes - sum_2025_bytes, 0)\n",
    "\n",
    "print(\"[status] sum_pre2024 (bytes) ->\", sum_pre2024_bytes)\n",
    "print(\"[status] sum_2024    (bytes) ->\", sum_2024_bytes)\n",
    "print(\"[status] sum_2025    (bytes) ->\", sum_2025_bytes)\n",
    "\n",
    "# --- Metadata (separate folder)\n",
    "metadata_total = 0\n",
    "if 'metadata_files_df' in globals():\n",
    "    metadata_total = metadata_files_df.agg({\"uncompressed_bytes\": \"sum\"}).collect()[0][0]\n",
    "else:\n",
    "    print(\"[status] WARNING: metadata_files_df is not defined; setting metadata_total to 0.\")\n",
    "print(\"[status] metadata_total (bytes) ->\", metadata_total)\n",
    "\n",
    "# --- Bytes → MiB\n",
    "to_mb = 1024**2\n",
    "other_mb = sum_pre2024_bytes / to_mb\n",
    "y2024_mb = sum_2024_bytes    / to_mb\n",
    "y2025_mb = sum_2025_bytes    / to_mb\n",
    "meta_mb  = metadata_total    / to_mb\n",
    "\n",
    "print(f\"[status] other_mb (≤2023) = {other_mb:,.1f} MB\")\n",
    "print(f\"[status] 2024_mb          = {y2024_mb:,.1f} MB\")\n",
    "print(f\"[status] 2025_mb          = {y2025_mb:,.1f} MB\")\n",
    "print(f\"[status] meta_mb          = {meta_mb:,.1f} MB\")\n",
    "\n",
    "# --- Pie inputs\n",
    "labels_full = [\n",
    "    \"All other years (compressed)\\n≤2023\",\n",
    "    \"Daily 2024 (compressed)\",\n",
    "    \"Daily 2025 (compressed)\",\n",
    "    \"Metadata (.txt files)\"\n",
    "]\n",
    "sizes = [other_mb, y2024_mb, y2025_mb, meta_mb]\n",
    "\n",
    "# Only show the big segment label on the pie; the three small ones will use balloons\n",
    "pie_labels = [\"All other years (compressed)\\n≤2023\", \"\", \"\", \"\"]\n",
    "explode = [0.00, 0.07, 0.07, 0.07]\n",
    "\n",
    "# --- Plot\n",
    "rotation_deg = -90\n",
    "print(f\"[status] applying rotation: {rotation_deg} degrees (clockwise)\")\n",
    "fig, ax = plt.subplots(figsize=(9.6, 8.8))  # a touch wider for balloon space\n",
    "start_angle = 90 + rotation_deg\n",
    "ax.margins(x=0.35)  # breathing room for call-outs on the right\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    sizes,\n",
    "    labels=pie_labels,\n",
    "    explode=explode,\n",
    "    startangle=start_angle,\n",
    "    autopct=\"%1.2f%%\",\n",
    "    labeldistance=1.05,\n",
    "    pctdistance=0.68\n",
    ")\n",
    "\n",
    "ax.set_title(\"Proportion of GHCN’s Daily (compressed): ≤2023 vs 2024 vs 2025 vs Metadata\")\n",
    "ax.set_aspect(\"equal\")\n",
    "\n",
    "# --- Style: keep label + yellow % only for the big slice; clear small-slice texts\n",
    "big_idx = 0\n",
    "small_indices = [1, 2, 3]\n",
    "# Big slice styling\n",
    "texts[big_idx].set_color(\"black\")\n",
    "texts[big_idx].set_fontweight(\"bold\")\n",
    "autotexts[big_idx].set_color(\"yellow\")\n",
    "autotexts[big_idx].set_fontweight(\"bold\")\n",
    "# Suppress on-slice text for small slices (avoid overlap)\n",
    "for si in small_indices:\n",
    "    texts[si].set_text(\"\")       # no label on the wedge\n",
    "    autotexts[si].set_text(\"\")   # no percentage on the wedge\n",
    "\n",
    "# --- Balloon call-outs for the three small slices with simple de-overlap\n",
    "callouts = [\n",
    "    (1, labels_full[1], y2024_mb),\n",
    "    (2, labels_full[2], y2025_mb),\n",
    "    (3, labels_full[3], meta_mb),\n",
    "]\n",
    "\n",
    "# Compute anchor points on the pie for each slice\n",
    "anchors = {}\n",
    "for idx, _, _ in callouts:\n",
    "    w = wedges[idx]\n",
    "    theta_mid = (w.theta1 + w.theta2) / 2.0\n",
    "    r = getattr(w, \"r\", 1.0)\n",
    "    x0 = r * np.cos(np.deg2rad(theta_mid))\n",
    "    y0 = r * np.sin(np.deg2rad(theta_mid))\n",
    "    anchors[idx] = (x0, y0, theta_mid)\n",
    "\n",
    "# Candidate text positions to the right; we will adjust y to prevent overlap\n",
    "xtext_fixed = 1.60   # place text boxes well to the right of the unit circle\n",
    "y_cands = []\n",
    "for idx, _, _ in callouts:\n",
    "    x0, y0, _ = anchors[idx]\n",
    "    ytext = y0  # initial guess centred on the slice\n",
    "    y_cands.append([idx, xtext_fixed, ytext])\n",
    "\n",
    "# Sort by y then ensure a minimum vertical separation\n",
    "y_cands.sort(key=lambda t: t[2])\n",
    "min_sep = 0.18\n",
    "for i in range(1, len(y_cands)):\n",
    "    prev = y_cands[i-1]\n",
    "    curr = y_cands[i]\n",
    "    if curr[2] - prev[2] < min_sep:\n",
    "        curr[2] = prev[2] + min_sep\n",
    "\n",
    "# Now annotate using adjusted positions\n",
    "for idx, xtext, ytext in y_cands:\n",
    "    label = labels_full[idx]\n",
    "    value_mb = sizes[idx]\n",
    "    x0, y0, theta_mid = anchors[idx]\n",
    "    pct_text = f\"{(sizes[idx] / sum(sizes)) * 100:0.2f}%\"\n",
    "    annot_text = f\"{label}\\n~{pct_text}\\n{value_mb:,.2f} MB\"\n",
    "\n",
    "    ax.annotate(\n",
    "        annot_text,\n",
    "        xy=(x0, y0),\n",
    "        xytext=(xtext, ytext),\n",
    "        ha=\"left\", va=\"center\",\n",
    "        arrowprops=dict(\n",
    "            arrowstyle=\"->\",\n",
    "            linewidth=1,\n",
    "            connectionstyle=\"arc3,rad=0.2\"  # gentle curve to reduce crossings\n",
    "        ),\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"white\"),\n",
    "        zorder=15,\n",
    "        annotation_clip=False\n",
    "    )\n",
    "    print(f\"[status] balloon -> idx={idx}, theta_mid={theta_mid:.2f}, target=({xtext:.2f},{ytext:.2f})\")\n",
    "\n",
    "# --- Save figure  \n",
    "out_dir = \"figures\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"dew59_pie_all-upto2023_vs_2024_2025_metadata.png\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(out_path, dpi=300)\n",
    "print(f\"[status] pie chart saved -> {out_path}\")\n",
    "\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775f510-22ab-4b42-a576-7ce05580e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q1(b)13\") \n",
    "print(\"type(daily)\\n \",type(daily_for_overlap))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa17b71-6848-4d12-942f-be6e021d6302",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a0fb5-ba12-4adf-9e11-00ac4c510218",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q1(b)15\") \n",
    "print(\"daily\")\n",
    "print(daily_for_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b005f-3e52-4b50-97d9-756d8e502244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1(b)16\n",
    "print(\"daily_for_overlap.show(20, False)\")\n",
    "daily_for_overlap.show(20, False)\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866f8f3-6962-44b0-9d17-9c381eeee470",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q1(b)17\") \n",
    "# Load the stations metadata into Spark from Azure Blob Storage using spark.read.text without any other processing\n",
    "\n",
    "read_stations  = spark.read.text(stations_read_name).limit(100)\n",
    "read_inventory = spark.read.text(inventory_read_name).limit(100)\n",
    "read_countries = spark.read.text(countries_read_name).limit(100)\n",
    "read_states    = spark.read.text(states_read_name).limit(100)\n",
    "\n",
    "print(\"type(read_stations)\")\n",
    "print(type(read_stations))\n",
    "print(\"type(read_inventory)\")\n",
    "print(type(read_inventory))\n",
    "print(\"type(read_countries)\")\n",
    "print(type(read_countries))\n",
    "print(\"type(read_states)\")\n",
    "print(type(read_states))\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e14fd2-733a-49bd-a948-24f85c02b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q1(b)18\") \n",
    "print(\"stations.printSchema()\")\n",
    "read_stations.printSchema()\n",
    "print(\"inventory.printSchema()\")\n",
    "read_inventory.printSchema()\n",
    "print(\"countries.printSchema()\")\n",
    "read_countries.printSchema()\n",
    "print(\"states.printSchema()\")\n",
    "read_states.printSchema()\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f060e427-4152-4bcd-b9ec-ab0fcf401da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q1(b)19\")\n",
    "print(\"read_stations\")\n",
    "print(read_stations)\n",
    "read_stations.show(20, False)\n",
    "print(\"read_inventory\")\n",
    "print(read_inventory)\n",
    "read_inventory.show(20, False)\n",
    "print(\"read_countries\")\n",
    "print(read_countries)\n",
    "read_countries.show(20, False)\n",
    "print(\"read_states\")\n",
    "print(read_states)\n",
    "read_states.show(100, False)\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777255a-e6d1-46bc-8413-83de590650e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bprint(\"Q1(b)20\")\n",
    "# ok now read the full file\n",
    "read_stations = spark.read.text(stations_read_name)\n",
    "\n",
    "stations = (\n",
    "    read_stations.select(\n",
    "        F.trim(F.substring(\"value\",  1, 11)).alias(\"ID\"),                 # 1–11\n",
    "        F.trim(F.substring(\"value\", 13,  8)).cast(\"double\").alias(\"LATITUDE\"),   # 13–20\n",
    "        F.trim(F.substring(\"value\", 22,  9)).cast(\"double\").alias(\"LONGITUDE\"),  # 22–30\n",
    "        F.trim(F.substring(\"value\", 32,  6)).cast(\"double\").alias(\"ELEVATION\"),  # 32–37\n",
    "        F.trim(F.substring(\"value\", 39,  2)).alias(\"STATE\"),                     # 39–40\n",
    "        F.trim(F.substring(\"value\", 42, 30)).alias(\"NAME\"),                      # 42–71\n",
    "        F.trim(F.substring(\"value\", 73,  3)).alias(\"GSN_FLAG\"),                  # 73–75\n",
    "        F.trim(F.substring(\"value\", 77,  3)).alias(\"HCNCRN_FLAG\"),               # 77–79\n",
    "        F.trim(F.substring(\"value\", 81,  5)).alias(\"WMO_ID\")                     # 81–85\n",
    "    )\n",
    ")\n",
    "print(\"stations\")\n",
    "stations.printSchema()\n",
    "stations.show(10, truncate=False)\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb52e415-e5fc-434f-a73b-70d3513db1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q3(a–c)1\")\n",
    "# countries\n",
    "read_countries = spark.read.text(countries_read_name)\n",
    "countries = (\n",
    "    read_countries.select(\n",
    "        F.substring(\"value\", 1, 2).alias(\"CODE\"),                # 1–2\n",
    "        F.trim(F.substring(\"value\", 4, 61)).alias(\"COUNTRY_NAME\")# 4–64\n",
    "    )\n",
    ")\n",
    "countries.show()\n",
    "# derive country code \n",
    "stations_cc = stations.withColumn(\"COUNTRY_CODE\", F.substring(\"ID\", 1, 2))\n",
    "# join country code \n",
    "stn_countries = (\n",
    "    stations_cc\n",
    "    .join(countries, stations_cc.COUNTRY_CODE == countries.CODE, \"left\")\n",
    "    .drop(countries.CODE)   # keep COUNTRY_CODE from stations, drop duplicate\n",
    ")\n",
    "stations_cc.show()\n",
    "stn_countries.show()\n",
    "stn_countries.select(\"ID\",\"NAME\",\"COUNTRY_CODE\",\"COUNTRY_NAME\").show(20, False)\n",
    "\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ccb34-1c8a-4478-b0ea-14154463ee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q3(a-c)2\")\n",
    "# states\n",
    "cell_time = time.time()  \n",
    "read_states = spark.read.text(states_read_name)\n",
    "\n",
    "states = (\n",
    "    read_states.select(\n",
    "        F.substring(\"value\", 1, 2).alias(\"CODE\"),                 # 1–2\n",
    "        F.trim(F.substring(\"value\", 4, 47)).alias(\"STATE_NAME\")   # 4–50  (length = 47)\n",
    "    )\n",
    ")\n",
    "\n",
    "states.printSchema()\n",
    "states.show(20, truncate=False)\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e313a575-ad27-4a3b-bc91-9d2cf0c58126",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0e93a-329a-43be-b71c-ad795a38ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q3(d)1\")\n",
    "\n",
    "cell_time = time.time()   \n",
    "read_inventory = spark.read.text(inventory_read_name)\n",
    "\n",
    "inventory = (\n",
    "    read_inventory.select(\n",
    "        F.substring(\"value\",  1, 11).alias(\"ID\"),                  # 1–11\n",
    "        F.trim(F.substring(\"value\", 13,  8)).cast(\"double\").alias(\"LATITUDE\"),   # 13–20\n",
    "        F.trim(F.substring(\"value\", 22,  9)).cast(\"double\").alias(\"LONGITUDE\"),  # 22–30\n",
    "        F.substring(\"value\", 32,  4).alias(\"ELEMENT\"),             # 32–35\n",
    "        F.substring(\"value\", 37,  4).cast(\"int\").alias(\"FIRSTYEAR\"),# 37–40\n",
    "        F.substring(\"value\", 42,  4).cast(\"int\").alias(\"LASTYEAR\")  # 42–45\n",
    "    )\n",
    ")\n",
    "\n",
    "inventory.printSchema()\n",
    "inventory.show(20, truncate=False)\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a08cc0-cce7-44d5-87d4-c456848ade4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q2(d)\")#  — row counts (+ previews & schemas) for metadata + daily; save counts to dew59\n",
    "\n",
    "import time\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"[status] counting rows for metadata + daily subset...\")\n",
    "\n",
    "# Pass 1: counts\n",
    "counts = []\n",
    "for name, df in [\n",
    "    (\"stations\",  stations),\n",
    "    (\"states\",    states),\n",
    "    (\"countries\", countries),\n",
    "    (\"inventory\", inventory),\n",
    "    (\"daily_for_overlap\", daily_for_overlap),\n",
    "]:\n",
    "    n = df.count()\n",
    "    counts.append((name, n))\n",
    "    print(f\"[status] rows -> {name}: {n:,}\")\n",
    "\n",
    "# Show the counts as a tiny table\n",
    "counts_df = spark.createDataFrame(counts, [\"dataset\", \"rows\"])\n",
    "counts_df.show(truncate=False)\n",
    "\n",
    "# Save the counts table as a small CSV artefact for the report\n",
    "out_counts = \"wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/q2d_row_counts_csv\"\n",
    "print(f\"[status] writing counts_df -> {out_counts}\")\n",
    "counts_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(out_counts)\n",
    "print(\"[status] wrote counts_df\")\n",
    "\n",
    "# Pass 2: previews (same list again)\n",
    "print(\"\\n[status] previewing 3 records per dataset...\")\n",
    "for name, df in [\n",
    "    (\"stations\",  stations),\n",
    "    (\"states\",    states),\n",
    "    (\"countries\", countries),\n",
    "    (\"inventory\", inventory),\n",
    "    (\"daily_for_overlap\", daily_for_overlap),\n",
    "]:\n",
    "    print(f\"\\n[status] head(3) -> {name}:\")\n",
    "    df.show(3, truncate=False)\n",
    "\n",
    "# Pass 3: schema prints\n",
    "print(\"\\n[status] schemas:\")\n",
    "for name, df in [\n",
    "    (\"stations\",  stations),\n",
    "    (\"states\",    states),\n",
    "    (\"countries\", countries),\n",
    "    (\"inventory\", inventory),\n",
    "    (\"daily_for_overlap\", daily_for_overlap),\n",
    "]:\n",
    "    print(f\"[status] schema -> {name}:\")\n",
    "    df.printSchema()\n",
    "\n",
    "# timing\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\")\n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e85b719-2210-442a-b1a6-e5a448208983",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q3(d)1b\") #  – Row counts check (including inventory)\n",
    "\n",
    "bprint()\n",
    "print(\"Row counts (with inventory):\")\n",
    "print(f\"[result] stations      : {stations.count() :12,d}\")\n",
    "print(f\"[result] countries     : {countries.count():12,d}\")\n",
    "print(f\"[result] states        : {states.count()   :12,d}\")\n",
    "print(f\"[result] inventory     : {inventory.count():12,d}\")\n",
    "print(f\"[check ] stations_cc   : {inventory.count():12,d}\")\n",
    "print(f\"[check ] stn_countries : {inventory.count():12,d}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8a496-6ef3-4a5d-be3f-28cfdeca10a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3(d)2 – Aggregate inventory per station\n",
    "cell_time = time.time() \n",
    "\n",
    "core_elements = [\"PRCP\", \"SNOW\", \"SNWD\", \"TMAX\", \"TMIN\"]\n",
    "\n",
    "inv_agg = (inventory\n",
    "           .groupBy(\"ID\")\n",
    "           .agg(\n",
    "               F.min(\"FIRSTYEAR\").alias(\"FIRSTYEAR\"),\n",
    "               F.max(\"LASTYEAR\").alias(\"LASTYEAR\"),\n",
    "               F.countDistinct(\"ELEMENT\").alias(\"ELEMENT_COUNT\"),\n",
    "               F.countDistinct(\n",
    "                   F.when(F.col(\"ELEMENT\").isin(core_elements), F.col(\"ELEMENT\"))\n",
    "               ).alias(\"CORE_ELEMENT_COUNT\"),\n",
    "               F.countDistinct(\n",
    "                   F.when(~F.col(\"ELEMENT\").isin(core_elements), F.col(\"ELEMENT\"))\n",
    "              ).alias(\"OTHER_ELEMENT_COUNT\")\n",
    "                   ).orderBy(F.col(\"CORE_ELEMENT_COUNT\").desc(),\n",
    "                        F.col(\"ELEMENT_COUNT\").desc(),\n",
    "                        F.col(\"ID\").asc())\n",
    "                        )\n",
    "print()\n",
    "print(\"_\"*70)\n",
    "inv_agg.printSchema()\n",
    "inv_agg.show(20, truncate=False)\n",
    "\n",
    "print(f\"[result] Aggregated inventory rows : {inv_agg.count():12,d}\")\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5debf8de-1d3d-4907-8943-ea1eb144336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q3(e)1\") #  – Join aggregated inventory into enriched stations\n",
    "\n",
    "cell_time = time.time()\n",
    "enriched = (stn_countries   # already has station + country info\n",
    "            .join(states, stn_countries.STATE == states.CODE, \"left\")\n",
    "            .join(inv_agg, on=\"ID\", how=\"left\")\n",
    "           # ---- order the result (adjust) ----\n",
    "             .orderBy(F.col(\"ID\").asc(), F.col(\"LASTYEAR\").asc(), F.col(\"ELEMENT_COUNT\").asc())\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"_\"*70)\n",
    "print(f\"[result] Enriched stations rows : {enriched.count():12,d}\")\n",
    "enriched.select(\n",
    "                \"ID\"           ,\"NAME\"    ,\"COUNTRY_NAME\" ,\"STATE_NAME\",\n",
    "                \"FIRSTYEAR\"    ,\"LASTYEAR\",\"ELEMENT_COUNT\",\"CORE_ELEMENT_COUNT\" ,\"OTHER_ELEMENT_COUNT\"\n",
    "               ).show(20, truncate=False)\n",
    "\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43ec8e7-dda9-4a79-b7d8-54aaf11f5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q3(e)2\") #  – Save enriched stations table\n",
    "\n",
    "cell_time = time.time() \n",
    "\n",
    "print()\n",
    "print(\"_\"*70)\n",
    "print(f\"[written] enriched_write_name: {enriched_write_name}\")\n",
    "\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01ebdbb-1eb5-4761-aaa9-976b7f505117",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q3(e)3\")#  — save enriched stations table to user area\n",
    "\n",
    " \n",
    "cell_time = time.time()\n",
    "\n",
    "out_enriched = \"wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/enriched_stations.parquet\"\n",
    "\n",
    "print(\"[status] writing enriched stations table to user area...\")\n",
    "enriched.write.mode(\"overwrite\").parquet(out_enriched)\n",
    "print(f\"[status] wrote enriched -> {out_enriched}\")\n",
    "\n",
    "# Light confirmation\n",
    "print(\"[status] enriched rows:\", enriched.count())\n",
    "print(\"[status] enriched column count:\", len(enriched.columns))\n",
    "print(\"[status] enriched schema:\")\n",
    "enriched.printSchema()\n",
    "\n",
    "# timing\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\")\n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698cc7a-10da-410d-a39a-1ce12e4e249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q1(b)25\") \n",
    "\n",
    "has_enriched  = has_parquet(enriched_write_name)\n",
    "has_stations  = has_parquet(stations_write_name)\n",
    "has_inventory = has_parquet(inventory_write_name)\n",
    "has_states    = has_parquet(states_write_name)\n",
    "has_countries = has_parquet(countries_write_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f4619-ae08-4d5a-b6a5-369cc6af6f3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bprint(\"Q1(b)26\")\n",
    "#build parquet files conditionally \n",
    "cell_time = time.time()\n",
    "if(not has_enriched):\n",
    "    write_parquet(enriched,enriched_write_name)\n",
    "    print(\"_\"*70)\n",
    "    print(\"[written] \",enriched_write_name)\n",
    "    cell_time = time.time()\n",
    "    print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886c38aa-bfd3-454b-8fee-b22a77724e52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bprint(\"Q1(b)26a\")\n",
    "cell_time = time.time()\n",
    "if(not has_stations):\n",
    "    write_parquet(stations,stations_write_name)\n",
    "    print(\"_\"*70)\n",
    "    print(\"[written] \",stations_write_name)    \n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388fbb15-0b67-4a52-918a-552e7f5fdfbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bprint(\"Q1(b)27\") \n",
    "cell_time = time.time()\n",
    "\n",
    "if(not has_inventory):\n",
    "    write_parquet(inventory,inventory_write_name)\n",
    "    print(\"_\"*70)\n",
    "    print(\"[written] \",inventory_write_name)    \n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fea080-5ff1-42f2-a98b-6f1193e315a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bprint(\"Q1(b)28\")\n",
    "cell_time = time.time()\n",
    "if(not has_states):\n",
    "    write_parquet(states,states_write_name)\n",
    "    print(\"_\"*70)\n",
    "    print(\"[written] \",states_write_name)\n",
    "    \n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ebe258-ab74-4063-8b85-aa7788f5879b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bprint(\"Q1(b)29\")\n",
    "cell_time = time.time()\n",
    "if(not has_countries):\n",
    "    write_parquet(countries,countries_write_name)\n",
    "    print(\"_\"*70)\n",
    "    print(\"[written] \",countries_write_name)\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2f583-c003-4403-bc82-9f1b8d140d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"x 33\")\n",
    "print(\"[CHECK] read_stations schema:\"); read_stations.printSchema()\n",
    "print(\"[CHECK] stations schema:\"); stations.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4be94a-f0c7-4f23-bf82-6fcaee89fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q1(b) 30\") #  – station coverage: stations with all 5 core elements\n",
    "\n",
    "\n",
    "\n",
    "core_elements = [\"PRCP\", \"SNOW\", \"SNWD\", \"TMAX\", \"TMIN\"]\n",
    "all_core_time = time.time()\n",
    "stations_with_all_core = (inventory\n",
    "    .filter(F.col(\"ELEMENT\").isin(core_elements))\n",
    "    .groupBy(\"ID\")\n",
    "    .agg(F.countDistinct(\"ELEMENT\").alias(\"core_count\"))\n",
    "    .filter(F.col(\"core_count\") == len(core_elements))\n",
    ")\n",
    "\n",
    "count_all_core = stations_with_all_core.count()\n",
    "all_core_time  = time.time() - all_core_time\n",
    "print(\"[query] Stations with all 5 core elements:\", count_all_core)\n",
    "print(f\"[time] all_core_time (sec): {all_core_time:6.2f}\")\n",
    "print(f\"[time] all_core_time (min): {all_core_time/60:6.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef9cef-892f-4af1-8db6-48eea710964d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bprint(\"Q1(b) 31\") #  – station coverage: stations with only PRCP\n",
    "\n",
    "only_prcp_time = time.time()\n",
    "stations_with_only_prcp = (inventory\n",
    "    .groupBy(\"ID\")\n",
    "    .agg(F.collect_set(\"ELEMENT\").alias(\"elements\"))\n",
    "    .filter(F.size(\"elements\") == 1)                # only one element\n",
    "    .filter(F.array_contains(F.col(\"elements\"), \"PRCP\"))  # that element is PRCP\n",
    ")\n",
    "\n",
    "count_only_prcp = stations_with_only_prcp.count()\n",
    "only_prcp_time = time.time() - only_prcp_time\n",
    "print(\"[coverage] Stations with only PRCP:\", count_only_prcp)\n",
    "print(f\"[time] only_prcp_time (sec): {only_prcp_time:6.2f}\")\n",
    "print(f\"[time] only_prcp_time (min): {only_prcp_time/60:6.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0c944-c7bb-46d8-91e2-6534ff06ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"DIAGNOSTIC 7\")\n",
    "cell_time = time.time() \n",
    "probe_universe(daily, stations, inv_agg, tag=\"this C2 (GOOD) notebook NOT daily_for_overlap\")\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124bae2f-3bd3-46ae-bf5f-fe7424bdb465",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"DIAGNOSTIC 8b\") #— daily vs stations anti-joins (uses daily_for_overlap directly)\n",
    "\n",
    " \n",
    "cell_time = time.time() \n",
    "\n",
    "daily_ids   = canon_ids(daily_for_overlap)   # distinct, normalised ID from daily\n",
    "station_ids = canon_ids(stations)            # distinct, normalised ID from stations\n",
    "\n",
    "# Daily but NOT in stations\n",
    "Daily_NOT_Station_time = time.time()\n",
    "daily_not_in_stations = daily_ids.join(station_ids, on=\"ID\", how=\"left_anti\")\n",
    "count_daily_NOT_in_stations = daily_not_in_stations.count()\n",
    "Daily_NOT_Station_time = time.time() - Daily_NOT_Station_time\n",
    "print(f\"[status] daily_not_in_stations: {count_daily_NOT_in_stations:,}\")\n",
    "\n",
    "# Stations but NOT in daily\n",
    "Stations_NOT_Daily_time = time.time()\n",
    "stations_not_in_daily = station_ids.join(daily_ids, on=\"ID\", how=\"left_anti\")\n",
    "count_stations_not_in_daily = stations_not_in_daily.count()\n",
    "Stations_NOT_Daily_time = time.time() - Stations_NOT_Daily_time\n",
    "print(f\"[status] stations_not_in_daily: {count_stations_not_in_daily:,}\") \n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743feb55-1367-4ab2-8b14-f64fc085ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"DIAGNOSTIC 8\")\n",
    "\n",
    "cell_time = time.time() \n",
    "daily_ids      = canon_ids(daily_for_overlap)\n",
    "catalogue_ids  = canon_ids(stations)    # parsed ghcnd-stations.txt\n",
    "inventory_ids  = canon_ids(inv_agg)     # station universe for comparisons\n",
    "\n",
    "print(\"[COUNT] daily IDs     :\", daily_ids.count())\n",
    "print(\"[COUNT] inventory IDs :\", inventory_ids.count())\n",
    "print(\"[COUNT] catalogue IDs :\", catalogue_ids.count())\n",
    "\n",
    "# Use the inventory universe (this is what yields the familiar small number)\n",
    "stations_not_in_daily = inventory_ids.join(daily_ids, on=\"ID\", how=\"left_anti\")\n",
    "print(f\"[RESULT] Stations in stations (inventory) but not in daily: {stations_not_in_daily.count():3d}\")\n",
    "\n",
    "# Diagnostics to confirm catalogue vs inventory mismatch is tiny\n",
    "cat_minus_inv = catalogue_ids.join(inventory_ids, on=\"ID\", how=\"left_anti\").count()\n",
    "print(f\"[DIAG] catalogue − inventory: {cat_minus_inv}\")\n",
    "\n",
    " \n",
    "print(\"[SAMPLE] First 50 IDs not in daily:\")\n",
    "stations_not_in_daily.orderBy(\"ID\").show(50, truncate=False)\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216dd72b-6b41-4a3b-a564-b80bd56741f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"DIAGNOSTIC 10\")\n",
    "\n",
    "#daily_for_overlap = None\n",
    "cell_time = time.time()  \n",
    "\n",
    "# Try in-memory objects first (fastest)\n",
    "for name in [\"read_daily\", \"daily_all\", \"ghcnd_daily\", \"daily_raw\", \"daily_full\"]:\n",
    "    obj = globals().get(name)\n",
    "    if obj is not None:\n",
    "        try:\n",
    "            n = _ids(obj).count()\n",
    "            print(f\"[CHECK] {name} unique station IDs:\", n)\n",
    "            if n >= 120_000:\n",
    "                daily_for_overlap = obj\n",
    "                print(f\"[RESULT] Using in-memory {name} for overlaps.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not inspect {name}:\", e)\n",
    "\n",
    "# If still not found, try common path variables  \n",
    "if daily_for_overlap is None:\n",
    "    for var in [\"DAILY_READ_NAME\", \"DAILY_WRITE_NAME\", \"daily_read_name\", \"daily_write_name\", \"DAILY_NAME\"]:\n",
    "        path = globals().get(var)\n",
    "        if path:\n",
    "            try:\n",
    "                print(f\"[INFO] Trying {var} =\", path)\n",
    "                df = spark.read.parquet(str(path))\n",
    "                n = _ids(df).count()\n",
    "                print(f\"[CHECK] {var} unique station IDs:\", n)\n",
    "                if n >= 120_000:\n",
    "                    daily_for_overlap = df\n",
    "                    print(f\"[RESULT] Using {var} for overlaps.\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not read {var}:\", e)\n",
    "\n",
    "if daily_for_overlap is None:\n",
    "    raise SystemExit(\"[FATAL] Could not find the unfiltered daily dataset (~129k unique stations). \"\n",
    "                     \"Point to the full daily DF or its parquet path.\")\n",
    "\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13557e23-280f-4d62-8066-431289a17666",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"DIAGNOSTIC 9\")\n",
    "cell_time = time.time()  \n",
    "print(\"[INFO] Building canonical ID sets …\")\n",
    "catalogue_ids = canon_ids(stations)   # parsed ghcnd-stations.txt\n",
    "inventory_ids = canon_ids(inv_agg)    # inventory/enriched universe\n",
    "\n",
    "cat_minus_inv_ids = catalogue_ids.join(inventory_ids, on=\"ID\", how=\"left_anti\")\n",
    "print(\"[DIAG] catalogue − inventory count:\", cat_minus_inv_ids.count())  # expect ~39\n",
    "\n",
    "# Bring back station metadata for these IDs\n",
    "station_cols = [\"ID\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"STATE\", \"NAME\", \"GSN_FLAG\", \"HCNCRN_FLAG\", \"WMO_ID\"]\n",
    "suspect = (cat_minus_inv_ids\n",
    "           .join(stations.select(*station_cols), on=\"ID\", how=\"left\"))\n",
    "\n",
    "# Count NULLs across metadata to find rows with “a lot of NULL values”\n",
    "meta_cols = [c for c in station_cols if c != \"ID\"]\n",
    "null_sum_expr = sum([F.when(F.col(c).isNull() | (F.col(c) == \"\"), 1).otherwise(0) for c in meta_cols])\n",
    "suspect = suspect.withColumn(\"nulls_in_metadata\", null_sum_expr)\n",
    "\n",
    "print(\"[CHECK] Top rows ordered by NULL count (highest first):\")\n",
    "suspect.orderBy(F.col(\"nulls_in_metadata\").desc(), F.col(\"ID\")).show(50, truncate=False)\n",
    "\n",
    "# Persist for later deep dive\n",
    "diag_dir = \"diagnostics\"\n",
    "os.makedirs(diag_dir, exist_ok=True)\n",
    "parquet_path = os.path.join(diag_dir, \"catalogue_minus_inventory.parquet\")\n",
    "csv_path     = os.path.join(diag_dir, \"catalogue_minus_inventory.csv\")\n",
    "\n",
    "suspect.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "# Small (≈39 rows) → safe to use pandas for a convenience CSV\n",
    "suspect_pd = suspect.orderBy(\"ID\").toPandas()\n",
    "suspect_pd.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"[INFO] Saved Parquet: {parquet_path}\")\n",
    "print(f\"[INFO] Saved CSV    : {csv_path}\")\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686915f-09d0-4310-9658-4ce16f55e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"DIAGNOSTIC 11\")\n",
    "daily_ids = _canon_ids(daily_for_overlap)\n",
    "inv_ids   = _canon_ids(inv_agg)\n",
    "cat_ids   = _canon_ids(stations)\n",
    "\n",
    "print(\"[COUNT] daily IDs     :\", daily_ids.count())\n",
    "print(\"[COUNT] inventory IDs :\", inv_ids.count())\n",
    "print(\"[COUNT] catalogue IDs :\", cat_ids.count())\n",
    "\n",
    "inv_not_daily = inv_ids.join(daily_ids, on=\"ID\", how=\"left_anti\")\n",
    "cat_not_daily = cat_ids.join(daily_ids, on=\"ID\", how=\"left_anti\")\n",
    "\n",
    "print(\"[RESULT] Stations (inventory) but NOT in daily:\", inv_not_daily.count())   # expect 0\n",
    "print(\"[RESULT] Stations (catalogue)  but NOT in daily:\", cat_not_daily.count())  # expect 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f915d60-f802-4486-822b-192d0d54c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"DIAGNOSTIC CONFIRMATION 1\")\n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "# Ensure a daily DataFrame exists, then alias it as `daily` for these checks\n",
    "if 'daily_for_overlap' in globals():\n",
    "    daily = daily_for_overlap\n",
    "else:\n",
    "    daily_for_overlap = pick_unfiltered_daily()\n",
    "    daily = daily_for_overlap\n",
    "print(\"[status] using dataframe ->\", \"daily_for_overlap\")\n",
    "\n",
    "print(\"[CHECK] daily distinct ELEMENTs (expect many if unfiltered):\")\n",
    "daily.select(\"ELEMENT\").distinct().orderBy(\"ELEMENT\").show(20, truncate=False)\n",
    "\n",
    "print(\"\\n[CHECK] daily date range:\")\n",
    "daily.select(\n",
    "    F.min(\"DATE\").alias(\"min_date\"),\n",
    "    F.max(\"DATE\").alias(\"max_date\")\n",
    ").show()\n",
    "\n",
    "print(\"\\n[CHECK] sample daily rows:\")\n",
    "daily.show(5, truncate=False)\n",
    "\n",
    "# Sanity check: rough station ID cardinality\n",
    "print(\"\\n[CHECK] daily_for_overlap unique station IDs:\",\n",
    "      _canon_ids(daily_for_overlap).count())\n",
    "\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b812ab-ee26-4600-96aa-3bcbff9440bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"DIAGNOSTIC 99\")\n",
    "print()\n",
    "print(\"_\"*70)\n",
    "print(f\"[result] Daily size (bytes): {daily_size_Bytes:16,d}\")\n",
    "print(f\"[result] meta-data  (bytes): {meta_size_Bytes:16,d}\")\n",
    "\n",
    "print()\n",
    "print(f\"[result] Daily size (MB)   : {daily_size_Bytes / (1024**2):8.2f}\")\n",
    "print(f\"[result] meta-data  (MB)   : {meta_size_Bytes  / (1024**2):8.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"_\"*70)\n",
    "print(\"Row counts:\")\n",
    "print(f\"[result] stations : {stations.count():8,d}\")\n",
    "print(f\"[result] inventory: {inventory.count():8,d}\")\n",
    "print(f\"[result] countries: {countries.count():8,d}\")\n",
    "print(f\"[result] states   : {states.count():8,d}\")\n",
    "print(f\"[result] Total    : {(stations.count() + inventory.count() + countries.count() + states.count()):8,d}\")\n",
    "\n",
    "print()\n",
    "print(\"_\"*70)\n",
    "print(\"queries:\")\n",
    "print(f\"[result] Stations with 5 core elements: {count_all_core:12,d}\")\n",
    "print(f\"[result] Stations with  PRCP  only    : {count_only_prcp:12,d}\")\n",
    "print(f\"[time] all_core_time (sec) : {all_core_time:6.2f}\")\n",
    "print(f\"[time] only_prcp_time (sec): {only_prcp_time:6.2f}\")\n",
    "print(f\"[time] all_core_time (min) : {all_core_time/60:6.2f}\") \n",
    "print(f\"[time] only_prcp_time (min): {only_prcp_time/60:6.2f}\") \n",
    "\n",
    "print()\n",
    "print(\"_\"*70)\n",
    "print(f\"[result] Stations in daily but not in stations: {count_daily_NOT_in_stations:5,d}\")\n",
    "print(f\"[result] Stations in stations but not in daily: {count_stations_not_in_daily:5,d}\") \n",
    "print(f\"[time] Daily_NOT_Station time (sec) : {Daily_NOT_Station_time:6.2f}\")\n",
    "print(f\"[time] Daily_NOT_Station time (min) : {Daily_NOT_Station_time/60:6.2f}\")\n",
    "print(f\"[time] Stations_NOT_Daily time (sec): {Stations_NOT_Daily_time:6.2f}\")\n",
    "print(f\"[time] Stations_NOT_Daily time (min): {Stations_NOT_Daily_time/60:6.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"_\"*70)\n",
    "print(f\"[file] Enriched Stations file: {stations_write_name}\")\n",
    "print(f\"[file] Inventory         file: {inventory_write_name}\")\n",
    "print(f\"[file] Countries         file: {countries_write_name}\")\n",
    "print(f\"[file] States            file: {states_write_name}\")\n",
    "\n",
    "\n",
    "print(f\"[time] notebook_run_time (sec): {(time.time() - notebook_run_time)   :5.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5173cb4-8b64-4e64-980a-64bbd447d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process A X (12) \")\n",
    "notebook_run_time = time.time() - notebook_run_time\n",
    "#plot_daily_size.show()\n",
    "#pie_chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522f6d0-2cfa-48da-8c43-b2c1557ee7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process A X (11) \n",
    "# PROCESSING — FUNCTIONAL SUMMARY (show_df-only; no loops)\n",
    "# Reprints prior answers using only existing DataFrame objects.\n",
    "# show_df() already prints schema + pretty head; we add labels.\n",
    "# If an object isn't defined yet, the try/except keeps the cell running.\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== Q1 — High-level exploration (DataFrame reprints only) ===\")\n",
    "\n",
    "# Q1(b) Years in daily & size change over years\n",
    "try:\n",
    "    print(\"\\nQ1(b) year_sizes_df (per-year size metrics) — head\")\n",
    "    show_df(year_sizes_df.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  year_sizes_df not available:\", e)\n",
    "\n",
    "# (If you also created an alternative per-year table, you can keep one of these.)\n",
    "try:\n",
    "    print(\"\\nQ1(b) years_size_df — head\")\n",
    "    show_df(years_size_df.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  years_size_df not available:\", e)\n",
    "\n",
    "\n",
    "print(\"\\n=== Q2 — Loading & schemas (shown via show_df) ===\")\n",
    "\n",
    "# Q2(a–b) Daily schema & preview (choose whichever exists in your run)\n",
    "try:\n",
    "    print(\"\\nQ2(a–b) daily_df — head\")\n",
    "    show_df(daily_df.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  daily_df not available:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nQ2(a–b) daily — head\")\n",
    "    show_df(daily.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  daily not available:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nQ2(a–b) ghcnd_daily — head\")\n",
    "    show_df(ghcnd_daily.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  ghcnd_daily not available:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nQ2(a–b) read_daily — head\")\n",
    "    show_df(read_daily.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  read_daily not available:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nQ2(a–b) daily_all — head\")\n",
    "    show_df(daily_all.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  daily_all not available:\", e)\n",
    "\n",
    "# Q2(c) Fixed-width metadata loads — heads\n",
    "try:\n",
    "    print(\"\\nQ2(c) stations — head\")\n",
    "    show_df(stations.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  stations not available:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nQ2(c) states — head\")\n",
    "    show_df(states.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  states not available:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nQ2(c) countries — head\")\n",
    "    show_df(countries.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  countries not available:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nQ2(c) inventory — head\")\n",
    "    show_df(inventory.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  inventory not available:\", e)\n",
    "\n",
    "\n",
    "print(\"\\n=== Q3 — Enriched stations & coverage ===\")\n",
    "\n",
    "# Q3(a–c) Enriched stations (any canonical name you used)\n",
    "try:\n",
    "    print(\"\\nQ3(a–c) enriched_stations — head\")\n",
    "    show_df(enriched_stations.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  enriched_stations not available:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nQ3(a–c) stations_enriched — head\")\n",
    "    show_df(stations_enriched.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  stations_enriched not available:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nQ3(a–c) enriched_stn — head\")\n",
    "    show_df(enriched_stn.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  enriched_stn not available:\", e)\n",
    "\n",
    "# Q3(d) Activity & element coverage summaries (heads)\n",
    "try:\n",
    "    print(\"\\nQ3(d) station_activity — head\")\n",
    "    show_df(station_activity.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  station_activity not available:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nQ3(d) station_element_counts — head\")\n",
    "    show_df(station_element_counts.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  station_element_counts not available:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nQ3(d) core_element_counts — head\")\n",
    "    show_df(core_element_counts.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  core_element_counts not available:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nQ3(d) other_element_counts — head\")\n",
    "    show_df(other_element_counts.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  other_element_counts not available:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nQ3(d) core_coverage_summary — head\")\n",
    "    show_df(core_coverage_summary.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  core_coverage_summary not available:\", e)\n",
    "\n",
    "\n",
    "print(\"\\n=== Q4 — Stations present in stations but not in daily ===\")\n",
    "\n",
    "# Q4(a) Join subset preview (proof rows)\n",
    "try:\n",
    "    print(\"\\nQ4(a) daily_stations_left — head\")\n",
    "    show_df(daily_stations_left.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  daily_stations_left not available:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nQ4(a) join_sample — head\")\n",
    "    show_df(join_sample.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  join_sample not available:\", e)\n",
    "\n",
    "# Q4(b) Unmatched stations (anti-join) — head\n",
    "try:\n",
    "    print(\"\\nQ4(b) stations_not_in_daily — head\")\n",
    "    show_df(stations_not_in_daily.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  stations_not_in_daily not available:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nQ4(b) stations_missing_df — head\")\n",
    "    show_df(stations_missing_df.limit(10))\n",
    "except Exception as e:\n",
    "    print(\"  stations_missing_df not available:\", e)\n",
    "\n",
    "print(\"\\n— End of show_df-only Processing summary —\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2318c1-9ef3-4db9-a5cb-7d028114864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process A X (11) \")\n",
    "val = spark.range(1).select(F.date_format(F.current_timestamp(), 'yyyy.MM.dd HH:mm').alias('t')).first()['t']\n",
    "cell_time = time.time() - cell_time  \n",
    "print(f\"[time] current time           :  {val}\")\n",
    "print(f\"[time] Cell time (sec)        : {cell_time:6.2f}\") \n",
    "#stop_spark()\n",
    "print(f\"[time] Cell time (min)        : {cell_time/60:6.2f}\") \n",
    "print(f\"[time] notebook_run_time (min):  {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
