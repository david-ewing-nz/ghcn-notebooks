{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "global azure_account_name\n",
    "global azure_data_container_name\n",
    "global azure_user_container_name\n",
    "global azure_user_token\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "azure_user_token = r\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def start_spark(executor_memory='4g', executor_cores=4, dynamic_allocation=True,\n",
    "                max_executors=20, min_executors=1, initial_executors=2):\n",
    "    \"\"\"Start a Spark session with the specified configuration.\"\"\"\n",
    "    \n",
    "    # Build the configuration\n",
    "    config = pyspark.SparkConf()\n",
    "    config.set('spark.app.name', f'{username}-notebook')\n",
    "    config.set('spark.executor.memory', executor_memory)\n",
    "    config.set('spark.executor.cores', executor_cores)\n",
    "    config.set('spark.sql.adaptive.enabled', 'true')\n",
    "    config.set('spark.sql.adaptive.coalescePartitions.enabled', 'true')\n",
    "    config.set('spark.sql.execution.arrow.pyspark.enabled', 'true')\n",
    "    \n",
    "    # Azure Blob Storage configuration\n",
    "    config.set(f'fs.azure.sas.{azure_data_container_name}.{azure_account_name}.blob.core.windows.net', azure_user_token)\n",
    "    config.set(f'fs.azure.sas.{azure_user_container_name}.{azure_account_name}.blob.core.windows.net', azure_user_token)\n",
    "    \n",
    "    if dynamic_allocation:\n",
    "        config.set('spark.dynamicAllocation.enabled', 'true')\n",
    "        config.set('spark.dynamicAllocation.maxExecutors', str(max_executors))\n",
    "        config.set('spark.dynamicAllocation.minExecutors', str(min_executors))\n",
    "        config.set('spark.dynamicAllocation.initialExecutors', str(initial_executors))\n",
    "    \n",
    "    # Create the Spark session\n",
    "    spark = SparkSession.builder.config(conf=config).getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    print(f\"Spark session started for user: {username}\")\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "    print(f\"Spark UI: {sc.uiWebUrl}\")\n",
    "    \n",
    "    return spark, sc\n",
    "\n",
    "\n",
    "def stop_spark():\n",
    "    \"\"\"Stop the current Spark session.\"\"\"\n",
    "    try:\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "            print(\"Spark session stopped successfully\")\n",
    "        else:\n",
    "            print(\"No active Spark session found\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error stopping Spark session: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6223d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark, sc = start_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bcb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "notebook_run_time = time.time()\n",
    "\n",
    "def bprint(s):\n",
    "    print(f\"_{s:_^60}_\")\n",
    "\n",
    "def show_as_html(df, limit=20):\n",
    "    \"\"\"Display Spark DataFrame as HTML table\"\"\"\n",
    "    pandas_df = df.limit(limit).toPandas()\n",
    "    display(HTML(pandas_df.to_html()))\n",
    "\n",
    "def show_df(df, name=\"DataFrame\", limit=5):\n",
    "    \"\"\"Show DataFrame info and sample\"\"\"\n",
    "    print(f\"[diag] {name} schema:\")\n",
    "    df.printSchema()\n",
    "    print(f\"[diag] {name} sample:\")\n",
    "    df.show(limit, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e97f63",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba71e3a",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d5a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q1(a)1 - Load enriched stations\")\n",
    "\n",
    "enriched_write_name = f\"wasbs://campus-user@madsstorage002.blob.core.windows.net/{username}/enriched_stations.parquet\"\n",
    "enriched = spark.read.parquet(enriched_write_name).cache()\n",
    "\n",
    "show_as_html(enriched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e0d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q1(a)2 - Total and active stations\")\n",
    "\n",
    "total_stations = enriched.count()\n",
    "print(f\"Total stations: {total_stations:,}\")\n",
    "\n",
    "active_stations = enriched.filter(F.col(\"station_last\") >= 2025)\n",
    "active_count = active_stations.count()\n",
    "print(f\"Active stations in 2025: {active_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ce76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q1(a)3 - Network counts\")\n",
    "\n",
    "gsn = enriched.filter(enriched['gsn_flag'] == \"GSN\").count()\n",
    "print(f\"GSN stations: {gsn:,}\")\n",
    "\n",
    "hcn = enriched.filter(enriched['hcn_crn_flag'] == \"HCN\").count()\n",
    "print(f\"HCN stations: {hcn:,}\")\n",
    "\n",
    "crn = enriched.filter(enriched['hcn_crn_flag'] == \"CRN\").count()\n",
    "print(f\"CRN stations: {crn:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27262ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q1(a)4 - Network overlaps\")\n",
    "\n",
    "gsn_hcn = enriched.filter((enriched['gsn_flag'] == \"GSN\") & (enriched['hcn_crn_flag'] == \"HCN\")).count()\n",
    "print(f\"GSN ∩ HCN stations: {gsn_hcn:,}\")\n",
    "\n",
    "gsn_crn = enriched.filter((enriched['gsn_flag'] == \"GSN\") & (enriched['hcn_crn_flag'] == \"CRN\")).count()\n",
    "print(f\"GSN ∩ CRN stations: {gsn_crn:,}\")\n",
    "\n",
    "hcn_crn = enriched.filter((enriched['hcn_crn_flag'] == \"HCN\") & (enriched['hcn_crn_flag'] == \"CRN\")).count()\n",
    "print(f\"HCN ∩ CRN stations: {hcn_crn:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d43fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q1(a)5 - Network overlap visualization\")\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "counts = {\n",
    "    \"GSN\": gsn,\n",
    "    \"HCN\": hcn,\n",
    "    \"CRN\": crn,\n",
    "    \"GSN ∩ HCN\": gsn_hcn,\n",
    "    \"GSN ∩ CRN\": gsn_crn,\n",
    "    \"HCN ∩ CRN\": hcn_crn,\n",
    "    \"All three\": 0\n",
    "}\n",
    "\n",
    "labels = list(counts.keys())\n",
    "values = list(counts.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(labels, values, color='orange')\n",
    "plt.ylabel(\"Number of stations\")\n",
    "plt.title(\"Station membership and overlaps (GSN, HCN, CRN)\")\n",
    "plt.xticks(rotation=0, ha=\"center\")\n",
    "\n",
    "for b in bars:\n",
    "    height = b.get_height()\n",
    "    plt.text(b.get_x() + b.get_width()/2, height + max(values)*0.01,\n",
    "             f'{int(height):,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/network_overlaps.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ed8eee",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50636cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q1(b)1 - Southern Hemisphere stations\")\n",
    "\n",
    "southern_stations = enriched.filter(F.col(\"latitude\") < 0).count()\n",
    "print(f\"Southern Hemisphere stations: {southern_stations:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5075ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q1(b)2 - US territories\")\n",
    "\n",
    "us_territories = enriched.filter(\n",
    "    (F.col(\"country_name\").contains(\"United States\")) & \n",
    "    (F.col('country_code') != 'US')\n",
    ")\n",
    "\n",
    "us_territories_count = us_territories.count()\n",
    "print(f\"US territories stations: {us_territories_count:,}\")\n",
    "\n",
    "if us_territories_count > 0:\n",
    "    us_territories.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613fc49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q1(b)3 - Hemispheric analysis visualization\")\n",
    "\n",
    "north_hem = enriched.filter(F.col(\"latitude\") >= 0).count()\n",
    "south_hem = southern_stations\n",
    "century_global = enriched.filter((F.col(\"station_last\") - F.col(\"station_first\")) >= 100).count()\n",
    "nz_stations = enriched.filter(F.col(\"country_code\") == \"NZ\").count()\n",
    "\n",
    "labels1 = [\"Southern\\nHemisphere\", \"US territories\\n(south of equator)\", \"≥100 years (global)\"]\n",
    "us_territories_south = us_territories.filter(F.col(\"latitude\") < 0).count()\n",
    "values1 = [south_hem, us_territories_south, century_global]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(labels1, values1, color='skyblue')\n",
    "plt.ylabel(\"Number of stations\")\n",
    "plt.title(\"Coverage (southern focus)\")\n",
    "plt.xticks(rotation=0, ha=\"center\")\n",
    "\n",
    "for b in bars:\n",
    "    height = b.get_height()\n",
    "    plt.text(b.get_x() + b.get_width()/2, height + max(values1)*0.01,\n",
    "             f'{int(height):,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/southern_focus.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "labels2 = [\"Northern\\nHemisphere\", \"Southern\\nHemisphere\", \"New Zealand\\n(all)\"]\n",
    "values2 = [north_hem, south_hem, nz_stations]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(labels2, values2, color='lightcoral')\n",
    "plt.ylabel(\"Number of stations\")\n",
    "plt.title(\"Coverage by hemisphere with New Zealand\")\n",
    "plt.xticks(rotation=0, ha=\"center\")\n",
    "\n",
    "for b in bars:\n",
    "    height = b.get_height()\n",
    "    plt.text(b.get_x() + b.get_width()/2, height + max(values2)*0.01,\n",
    "             f'{int(height):,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/hemispheric_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c080da05",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q1(c)1 - Country station counts\")\n",
    "\n",
    "country_counts = enriched.groupBy(\"country_code\", \"country_name\").agg(\n",
    "    F.count(\"*\").alias(\"station_count\")\n",
    ").orderBy(F.desc(\"station_count\"))\n",
    "\n",
    "print(\"Top 10 countries by station count:\")\n",
    "show_as_html(country_counts, 10)\n",
    "\n",
    "country_counts_write = f\"wasbs://campus-user@madsstorage002.blob.core.windows.net/{username}/country_station_counts.parquet\"\n",
    "country_counts.write.mode(\"overwrite\").parquet(country_counts_write)\n",
    "print(f\"[info] Country counts saved to: {country_counts_write}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb8313",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q1(c)2 - Core elements coverage\")\n",
    "\n",
    "core_elements = ['PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN']\n",
    "stations_with_core = enriched.filter(F.size(F.col(\"elements\")) >= 1).count()\n",
    "stations_with_all5 = enriched.filter(F.size(F.col(\"elements\")) >= 5).count()\n",
    "\n",
    "print(f\"Stations with ≥1 core element: {stations_with_core:,}\")\n",
    "print(f\"Stations with all 5 core elements: {stations_with_all5:,}\")\n",
    "\n",
    "labels = ['≥1 core element', 'All 5 core elements']\n",
    "values = [stations_with_core, stations_with_all5]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(labels, values, color='lightgreen')\n",
    "plt.ylabel(\"Number of stations\")\n",
    "plt.title(\"Core element coverage\")\n",
    "plt.xticks(rotation=0, ha=\"center\")\n",
    "\n",
    "for b in bars:\n",
    "    height = b.get_height()\n",
    "    plt.text(b.get_x() + b.get_width()/2, height + max(values)*0.01,\n",
    "             f'{int(height):,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/core_elements_coverage.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb2895",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b207cf",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68228d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q2(a)1 - Haversine distance function\")\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the great circle distance between two points on the earth\"\"\"\n",
    "    if any(x is None for x in [lat1, lon1, lat2, lon2]):\n",
    "        return None\n",
    "        \n",
    "    R = 6371.0  # Earth radius in kilometers\n",
    "    \n",
    "    lat1_rad = radians(lat1)\n",
    "    lon1_rad = radians(lon1)\n",
    "    lat2_rad = radians(lat2)\n",
    "    lon2_rad = radians(lon2)\n",
    "    \n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    a = sin(dlat/2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "haversine_udf = udf(haversine_distance, DoubleType())\n",
    "print(\"[info] Haversine distance UDF registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc1fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q2(a)2 - Distance function test\")\n",
    "\n",
    "test_stations = enriched.limit(5)\n",
    "\n",
    "cross_df = test_stations.alias(\"a\").crossJoin(test_stations.alias(\"b\"))\n",
    "cross_df = cross_df.filter(F.col(\"a.id\") < F.col(\"b.id\"))\n",
    "\n",
    "result_df = cross_df.withColumn(\n",
    "    'distance_km', \n",
    "    haversine_udf(F.col(\"a.latitude\"), F.col(\"a.longitude\"),\n",
    "                  F.col(\"b.latitude\"), F.col(\"b.longitude\"))\n",
    ")\n",
    "\n",
    "result_df = result_df.select(\n",
    "    \"a.country_code\", \"a.station_name\", \"a.latitude\", \"a.longitude\",\n",
    "    \"b.country_code\", \"b.station_name\", \"b.latitude\", \"b.longitude\",\n",
    "    \"distance_km\"\n",
    ")\n",
    "\n",
    "print(\"Distance calculation test results:\")\n",
    "show_as_html(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d122e364",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed006cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q2(b)1 - New Zealand pairwise distances\")\n",
    "\n",
    "nz_stations_df = enriched.filter(F.col(\"country_code\") == \"NZ\")\n",
    "nz_count = nz_stations_df.count()\n",
    "print(f\"New Zealand stations: {nz_count}\")\n",
    "\n",
    "if nz_count > 0:\n",
    "    nz_cross = nz_stations_df.alias(\"a\").crossJoin(nz_stations_df.alias(\"b\"))\n",
    "    nz_cross = nz_cross.filter(F.col(\"a.id\") < F.col(\"b.id\"))\n",
    "    \n",
    "    nz_distances = nz_cross.withColumn(\n",
    "        'distance_km',\n",
    "        haversine_udf(F.col(\"a.latitude\"), F.col(\"a.longitude\"),\n",
    "                      F.col(\"b.latitude\"), F.col(\"b.longitude\"))\n",
    "    )\n",
    "    \n",
    "    nz_result = nz_distances.select(\n",
    "        F.col(\"a.id\").alias(\"station_a\"),\n",
    "        F.col(\"a.station_name\").alias(\"name_a\"),\n",
    "        F.col(\"b.id\").alias(\"station_b\"),\n",
    "        F.col(\"b.station_name\").alias(\"name_b\"),\n",
    "        \"distance_km\"\n",
    "    ).orderBy(\"distance_km\")\n",
    "    \n",
    "    pairs_count = nz_result.count()\n",
    "    print(f\"Total station pairs: {pairs_count}\")\n",
    "    \n",
    "    print(\"\\nClosest station pairs:\")\n",
    "    show_as_html(nz_result, 5)\n",
    "    \n",
    "    print(\"\\nFarthest station pairs:\")\n",
    "    show_as_html(nz_result.orderBy(F.desc(\"distance_km\")), 5)\n",
    "    \n",
    "    nz_distances_write = f\"wasbs://campus-user@madsstorage002.blob.core.windows.net/{username}/nz_station_distances.parquet\"\n",
    "    nz_result.write.mode(\"overwrite\").parquet(nz_distances_write)\n",
    "    print(f\"\\n[info] NZ distances saved to: {nz_distances_write}\")\n",
    "else:\n",
    "    print(\"[warning] No New Zealand stations found in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208766de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q2(b)2 - Precipitation analysis preparation\")\n",
    "\n",
    "daily_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"element\", StringType(), True),\n",
    "    StructField(\"value\", IntegerType(), True),\n",
    "    StructField(\"measurement_flag\", StringType(), True),\n",
    "    StructField(\"quality_flag\", StringType(), True),\n",
    "    StructField(\"source_flag\", StringType(), True),\n",
    "    StructField(\"observation_time\", StringType(), True),\n",
    "])\n",
    "\n",
    "daily_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .schema(daily_schema) \\\n",
    "    .load('wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/*.csv.gz')\n",
    "\n",
    "print(\"[info] Daily data loaded\")\n",
    "print(f\"[info] Daily data sample count: {daily_df.limit(1000).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7494eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q2(b)3 - Precipitation by country and year\")\n",
    "\n",
    "prcp_df = daily_df.filter(F.col(\"element\") == \"PRCP\")\n",
    "\n",
    "prcp_df = prcp_df.withColumn(\"country_code\", F.substring(\"id\", 1, 2))\n",
    "prcp_df = prcp_df.withColumn(\"year\", F.year(F.to_date(\"date\", \"yyyyMMdd\")))\n",
    "\n",
    "prcp_df = prcp_df.withColumn(\"value_mm\", F.col(\"value\") / 10.0)\n",
    "prcp_df = prcp_df.filter(F.col(\"value_mm\") >= 0)\n",
    "\n",
    "prcp_agg = prcp_df.groupBy(\"year\", \"country_code\").agg(\n",
    "    F.avg(\"value_mm\").alias(\"avg_daily_rainfall\")\n",
    ").orderBy(\"year\", \"country_code\")\n",
    "\n",
    "print(\"Sample of precipitation aggregation:\")\n",
    "show_as_html(prcp_agg, 10)\n",
    "\n",
    "prcp_agg_write = f\"wasbs://campus-user@madsstorage002.blob.core.windows.net/{username}/precipitation_by_country_year.parquet\"\n",
    "prcp_agg.write.mode(\"overwrite\").parquet(prcp_agg_write)\n",
    "print(f\"\\n[info] Precipitation data saved to: {prcp_agg_write}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q2(b)4 - Global rainfall 2024 choropleth map\")\n",
    "\n",
    "prcp_2024 = prcp_agg.filter(F.col(\"year\") == 2024)\n",
    "prcp_2024_pandas = prcp_2024.toPandas()\n",
    "\n",
    "print(f\"Countries with 2024 rainfall data: {len(prcp_2024_pandas)}\")\n",
    "\n",
    "if len(prcp_2024_pandas) > 0:\n",
    "    world_url = \"https://raw.githubusercontent.com/datasets/geo-countries/master/data/countries.geojson\"\n",
    "    world = gpd.read_file(world_url)\n",
    "    \n",
    "    prcp_2024_pandas['avg_daily_rainfall'] = pd.to_numeric(prcp_2024_pandas['avg_daily_rainfall'], errors='coerce')\n",
    "    prcp_2024_pandas = prcp_2024_pandas.dropna(subset=['avg_daily_rainfall'])\n",
    "    \n",
    "    world_merged = world.merge(prcp_2024_pandas, left_on='ISO_A2', right_on='country_code', how='left')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    world_merged.plot(\n",
    "        column='avg_daily_rainfall', \n",
    "        ax=ax, \n",
    "        legend=True, \n",
    "        cmap='Blues',\n",
    "        missing_kwds={'color': 'lightgrey'},\n",
    "        legend_kwds={'label': \"Average Daily Rainfall (mm)\", 'orientation': \"vertical\"}\n",
    "    )\n",
    "    \n",
    "    plt.title('Global Average Daily Rainfall (2024)', fontsize=16, pad=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig('figures/global_rainfall_2024.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nRainfall statistics for 2024:\")\n",
    "    print(f\"Countries with data: {len(prcp_2024_pandas):,}\")\n",
    "    print(f\"Average rainfall range: {prcp_2024_pandas['avg_daily_rainfall'].min():.2f} - {prcp_2024_pandas['avg_daily_rainfall'].max():.2f} mm\")\n",
    "    print(f\"Global mean: {prcp_2024_pandas['avg_daily_rainfall'].mean():.2f} mm\")\n",
    "    \n",
    "    print(\"\\nTop 10 wettest countries in 2024:\")\n",
    "    top_wet = prcp_2024_pandas.nlargest(10, 'avg_daily_rainfall')[['country_code', 'avg_daily_rainfall']]\n",
    "    for _, row in top_wet.iterrows():\n",
    "        print(f\"  {row['country_code']}: {row['avg_daily_rainfall']:.2f} mm\")\n",
    "        \n",
    "else:\n",
    "    print(\"[warning] No 2024 precipitation data available for choropleth map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ddea6",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a5b2e",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab692fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q3(a)1 - Daily row count\")\n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "total_daily_rows = daily_df.count()\n",
    "print(f\"Total rows in daily dataset: {total_daily_rows:,}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Count operation took: {cell_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200e1d2",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a75b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q3(b)1 - Core elements observation counts\")\n",
    "\n",
    "core_elements = ['PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN']\n",
    "\n",
    "daily_subset = daily_df.sample(0.001)\n",
    "\n",
    "core_element_counts = daily_subset.filter(\n",
    "    F.col(\"element\").isin(core_elements)\n",
    ").groupBy(\"element\").agg(\n",
    "    F.count(\"*\").alias(\"observation_count\")\n",
    ").orderBy(F.desc(\"observation_count\"))\n",
    "\n",
    "print(\"Core elements observation counts (from sample):\")\n",
    "show_as_html(core_element_counts)\n",
    "\n",
    "core_counts_pandas = core_element_counts.toPandas()\n",
    "if len(core_counts_pandas) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(core_counts_pandas['element'], core_counts_pandas['observation_count'], \n",
    "                   color=['blue', 'lightblue', 'cyan', 'red', 'orange'])\n",
    "    plt.ylabel(\"Observation count (sample)\")\n",
    "    plt.title(\"Core Elements Observation Counts\")\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    for b in bars:\n",
    "        height = b.get_height()\n",
    "        plt.text(b.get_x() + b.get_width()/2, height + max(core_counts_pandas['observation_count'])*0.01,\n",
    "                 f'{int(height):,}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures/core_elements_counts.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8a0d4",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Analysis Q3(c)1 - TMAX without TMIN analysis\")\n",
    "\n",
    "daily_sample = daily_df.sample(0.01)\n",
    "\n",
    "tmax_obs = daily_sample.filter(daily_sample.element == \"TMAX\").select(\"id\", \"date\", \"element\")\n",
    "tmin_obs = daily_sample.filter(daily_sample.element == \"TMIN\").select(\"id\", \"date\", \"element\")\n",
    "\n",
    "print(f\"TMAX observations in sample: {tmax_obs.count():,}\")\n",
    "print(f\"TMIN observations in sample: {tmin_obs.count():,}\")\n",
    "\n",
    "tmax_without_tmin = tmax_obs.join(tmin_obs, on=[\"id\", \"date\"], how=\"left_anti\")\n",
    "\n",
    "tmax_no_tmin_count = tmax_without_tmin.count()\n",
    "unique_stations = tmax_without_tmin.select(\"id\").distinct().count()\n",
    "\n",
    "print(f\"\\nTMAX observations without corresponding TMIN: {tmax_no_tmin_count:,}\")\n",
    "print(f\"Unique stations contributing: {unique_stations:,}\")\n",
    "\n",
    "if tmax_no_tmin_count > 0:\n",
    "    percentage = (tmax_no_tmin_count / tmax_obs.count()) * 100\n",
    "    print(f\"Percentage of TMAX without TMIN: {percentage:.2f}%\")\n",
    "    \n",
    "    print(\"\\nSample stations with TMAX but no TMIN:\")\n",
    "    station_sample = tmax_without_tmin.select(\"id\").distinct().limit(10)\n",
    "    show_as_html(station_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Additional Analysis 1 - NZ temperature time series\")\n",
    "\n",
    "if nz_count > 0:\n",
    "    nz_daily = daily_df.join(nz_stations_df.select(\"id\"), on=\"id\", how=\"inner\")\n",
    "    nz_temp = nz_daily.filter((nz_daily.element == \"TMIN\") | (nz_daily.element == \"TMAX\"))\n",
    "    nz_temp = nz_temp.select(\"id\", \"date\", \"element\", \"value\")\n",
    "    \n",
    "    nz_temp_sample = nz_temp.sample(0.1)\n",
    "    nz_temp_pandas = nz_temp_sample.toPandas()\n",
    "    \n",
    "    if len(nz_temp_pandas) > 0:\n",
    "        nz_temp_pandas['date'] = pd.to_datetime(nz_temp_pandas['date'], format='%Y%m%d')\n",
    "        nz_temp_pandas['temp_c'] = nz_temp_pandas['value'] / 10.0\n",
    "        \n",
    "        nz_pivot = nz_temp_pandas.pivot_table(\n",
    "            index=['id', 'date'], \n",
    "            columns='element', \n",
    "            values='temp_c'\n",
    "        ).reset_index()\n",
    "        \n",
    "        nz_pivot['year_month'] = nz_pivot['date'].dt.to_period('M')\n",
    "        monthly_avg = nz_pivot.groupby('year_month')[['TMIN', 'TMAX']].mean().reset_index()\n",
    "        monthly_avg['year_month'] = monthly_avg['year_month'].dt.to_timestamp()\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(monthly_avg['year_month'], monthly_avg['TMIN'], label='Avg TMIN', color='blue', alpha=0.7)\n",
    "        plt.plot(monthly_avg['year_month'], monthly_avg['TMAX'], label='Avg TMAX', color='red', alpha=0.7)\n",
    "        plt.title(\"Average TMIN and TMAX in New Zealand\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Temperature (°C)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"figures/nz_temperature_timeseries.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"[info] Processed {len(nz_temp_pandas):,} NZ temperature observations\")\n",
    "    else:\n",
    "        print(\"[warning] No NZ temperature data in sample\")\n",
    "else:\n",
    "    print(\"[warning] No New Zealand stations available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fd5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Final Summary - Analysis completion\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ANALYSIS COMPLETION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"✅ Q1 Analysis:\")\n",
    "print(f\"   • Total stations: {total_stations:,}\")\n",
    "print(f\"   • Active in 2025: {active_count:,}\")\n",
    "print(f\"   • Southern Hemisphere: {southern_stations:,}\")\n",
    "print(f\"   • Network analysis complete\")\n",
    "print(f\"   • Hemispheric visualizations complete\")\n",
    "print()\n",
    "print(\"✅ Q2 Analysis:\")\n",
    "print(f\"   • Haversine distance function implemented\")\n",
    "print(f\"   • NZ station analysis: {nz_count} stations\")\n",
    "print(f\"   • Precipitation analysis complete\")\n",
    "print(f\"   • Global rainfall 2024 choropleth map created\")\n",
    "print()\n",
    "print(\"✅ Q3 Analysis:\")\n",
    "print(f\"   • Daily dataset rows: {total_daily_rows:,}\")\n",
    "print(f\"   • Core elements analysis complete\")\n",
    "print(f\"   • TMAX/TMIN correspondence analysis complete\")\n",
    "print()\n",
    "print(\"📊 Visualizations Created:\")\n",
    "print(\"   • Network overlaps chart\")\n",
    "print(\"   • Hemispheric analysis charts\")\n",
    "print(\"   • Core elements coverage chart\")\n",
    "print(\"   • Global rainfall 2024 choropleth map\")\n",
    "print(\"   • Core elements observation counts\")\n",
    "print(\"   • NZ temperature time series\")\n",
    "print()\n",
    "\n",
    "total_runtime = time.time() - notebook_run_time\n",
    "print(f\"📈 Total Analysis Runtime: {total_runtime/60:.2f} minutes\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea54c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Stop Spark session\")\n",
    "\n",
    "stop_spark()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
