{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc067e2",
   "metadata": {},
   "source": [
    "# DATA420 A1 — Analysis + Visualisations (additions only)\n",
    "These cells can be appended to your existing **Analysis.ipynb** (and/or Visualizations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4519844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Spark and load enriched stations\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "print(\"Starting SparkSession for Analysis additions...\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "\n",
    "WASBS_DATA = f\"wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd\"\n",
    "WASBS_DAILY = f\"{WASBS_DATA}/daily\"\n",
    "WASBS_USER_BASE = f\"wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/dew59\"\n",
    "\n",
    "enriched_path = f\"{WASBS_USER_BASE}/enriched_stations.parquet/\"\n",
    "print(\"Loading enriched stations:\", enriched_path)\n",
    "enriched_stations = spark.read.parquet(enriched_path)\n",
    "print(\"Enriched stations count:\", enriched_stations.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb704998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1(a): Totals and network overlaps\n",
    "total_stations = enriched_stations.count()\n",
    "active_2025 = enriched_stations.filter(F.col(\"last_year\") >= 2025).count()\n",
    "\n",
    "gsn = enriched_stations.filter(F.col(\"gsn_flag\")==\"GSN\").count()\n",
    "hcn = enriched_stations.filter(F.col(\"hcn_crn_flag\")==\"HCN\").count()\n",
    "crn = enriched_stations.filter(F.col(\"hcn_crn_flag\")==\"CRN\").count()\n",
    "\n",
    "gsn_hcn = enriched_stations.filter((F.col(\"gsn_flag\")==\"GSN\") & (F.col(\"hcn_crn_flag\")==\"HCN\")).count()\n",
    "gsn_crn = enriched_stations.filter((F.col(\"gsn_flag\")==\"GSN\") & (F.col(\"hcn_crn_flag\")==\"CRN\")).count()\n",
    "hcn_crn = enriched_stations.filter((F.col(\"hcn_crn_flag\")==\"HCN\") & (F.col(\"hcn_crn_flag\")==\"CRN\")).count()\n",
    "\n",
    "print(\"Total stations:\", total_stations)\n",
    "print(\"Active in 2025:\", active_2025)\n",
    "print(\"GSN:\", gsn, \"HCN:\", hcn, \"CRN:\", crn)\n",
    "print(\"GSN∩HCN:\", gsn_hcn, \"GSN∩CRN:\", gsn_crn, \"HCN∩CRN:\", hcn_crn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed9dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1(b): Southern Hemisphere and U.S. territories outside 'US'\n",
    "south_hemi = enriched_stations.filter(F.col(\"latitude\") < 0).count()\n",
    "\n",
    "us_territory_like = enriched_stations.filter(\n",
    "    (F.col(\"country_name\").contains(\"United States\")) & (F.col(\"country_code\") != \"US\")\n",
    ").count()\n",
    "\n",
    "print(\"Southern Hemisphere stations:\", south_hemi)\n",
    "print(\"United States territories (excluding US):\", us_territory_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be3abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1(c): Counts per country and per state\n",
    "by_country = (enriched_stations.groupBy(\"country_code\",\"country_name\").agg(F.count(\"*\").alias(\"station_count\")))\n",
    "by_state = (enriched_stations.filter(F.col(\"state\").isNotNull())\n",
    "            .groupBy(\"state\",\"state_name\").agg(F.count(\"*\").alias(\"station_count\")))\n",
    "\n",
    "out_countries = f\"{WASBS_USER_BASE}/enriched_countries.parquet/\"\n",
    "out_states = f\"{WASBS_USER_BASE}/enriched_states.parquet/\"\n",
    "print(\"Writing:\", out_countries)\n",
    "by_country.write.mode(\"overwrite\").parquet(out_countries)\n",
    "print(\"Writing:\", out_states)\n",
    "by_state.write.mode(\"overwrite\").parquet(out_states)\n",
    "\n",
    "print(\"Sample countries:\")\n",
    "by_country.orderBy(F.desc(\"station_count\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143bf171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a): Haversine UDF and small CROSS JOIN demo\n",
    "import math\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    phi1, phi2 = math.radians(lat1), math.radians(lat2)\n",
    "    dphi = math.radians(lat2 - lat1)\n",
    "    dlambda = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dphi/2.0)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2.0)**2\n",
    "    c = 2*math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    return R*c\n",
    "\n",
    "haversine_udf = F.udf(haversine_km, T.DoubleType())\n",
    "\n",
    "demo = enriched_stations.select(\"id\",\"name\",\"latitude\",\"longitude\").limit(10)\n",
    "pairs = demo.crossJoin(demo.select(\n",
    "    F.col(\"id\").alias(\"id2\"),\n",
    "    F.col(\"name\").alias(\"name2\"),\n",
    "    F.col(\"latitude\").alias(\"lat2\"),\n",
    "    F.col(\"longitude\").alias(\"lon2\")\n",
    ")).filter(F.col(\"id\") < F.col(\"id2\"))\n",
    "\n",
    "distances_demo = pairs.withColumn(\"km\", haversine_udf(\"latitude\",\"longitude\",\"lat2\",\"lon2\"))\n",
    "print(\"Demo pairwise distances (top 10):\")\n",
    "distances_demo.orderBy(\"km\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e34972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(b): Pairwise distances across all NZ stations; save and show closest pair\n",
    "nz = enriched_stations.filter(F.col(\"country_code\")==\"NZ\").select(\"id\",\"name\",\"latitude\",\"longitude\")\n",
    "left = nz.alias(\"a\")\n",
    "right = nz.alias(\"b\")\n",
    "\n",
    "nz_pairs = (left.join(right, F.col(\"a.id\") < F.col(\"b.id\"))\n",
    "                 .select(F.col(\"a.id\").alias(\"id1\"),\n",
    "                         F.col(\"a.name\").alias(\"name1\"),\n",
    "                         F.col(\"a.latitude\").alias(\"lat1\"),\n",
    "                         F.col(\"a.longitude\").alias(\"lon1\"),\n",
    "                         F.col(\"b.id\").alias(\"id2\"),\n",
    "                         F.col(\"b.name\").alias(\"name2\"),\n",
    "                         F.col(\"b.latitude\").alias(\"lat2\"),\n",
    "                         F.col(\"b.longitude\").alias(\"lon2\")))\n",
    "\n",
    "nz_dist = nz_pairs.withColumn(\"km\", haversine_udf(\"lat1\",\"lon1\",\"lat2\",\"lon2\"))\n",
    "closest = nz_dist.orderBy(\"km\").limit(1)\n",
    "\n",
    "out_nz_pairs = f\"{WASBS_USER_BASE}/nz_station_pairwise_distances.parquet/\"\n",
    "print(\"Writing:\", out_nz_pairs)\n",
    "nz_dist.write.mode(\"overwrite\").parquet(out_nz_pairs)\n",
    "\n",
    "print(\"Closest pair in NZ:\")\n",
    "closest.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3(a): Core element observation counts\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "daily_schema = T.StructType([\n",
    "    T.StructField(\"ID\", T.StringType(), True),\n",
    "    T.StructField(\"DATE\", T.StringType(), True),\n",
    "    T.StructField(\"ELEMENT\", T.StringType(), True),\n",
    "    T.StructField(\"VALUE\", T.IntegerType(), True),\n",
    "    T.StructField(\"MFLAG\", T.StringType(), True),\n",
    "    T.StructField(\"QFLAG\", T.StringType(), True),\n",
    "    T.StructField(\"SFLAG\", T.StringType(), True),\n",
    "    T.StructField(\"OBSTIME\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "daily_all = spark.read.csv(f\"{WASBS_DAILY}/*.csv.gz\", schema=daily_schema, header=False, mode=\"PERMISSIVE\")\n",
    "core = [\"PRCP\",\"SNOW\",\"SNWD\",\"TMAX\",\"TMIN\"]\n",
    "core_counts = (daily_all.where(F.col(\"ELEMENT\").isin(core))\n",
    "               .groupBy(\"ELEMENT\").agg(F.count(\"*\").alias(\"obs_count\"))\n",
    "               .orderBy(F.desc(\"obs_count\")))\n",
    "\n",
    "print(\"Core element counts:\")\n",
    "core_counts.show(truncate=False)\n",
    "\n",
    "out_core = f\"{WASBS_USER_BASE}/core_element_counts.parquet/\"\n",
    "print(\"Writing:\", out_core)\n",
    "core_counts.write.mode(\"overwrite\").parquet(out_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4787cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3(b): TMAX observations without corresponding TMIN\n",
    "tmax = daily_all.filter(F.col(\"ELEMENT\")==\"TMAX\").select(F.col(\"ID\").alias(\"id\"), F.col(\"DATE\").alias(\"date\"))\n",
    "tmin = daily_all.filter(F.col(\"ELEMENT\")==\"TMIN\").select(F.col(\"ID\").alias(\"id\"), F.col(\"DATE\").alias(\"date\"))\n",
    "\n",
    "tmax_only = tmax.join(tmin, on=[\"id\",\"date\"], how=\"left_anti\")\n",
    "tmax_only_count = tmax_only.count()\n",
    "tmax_only_stations = tmax_only.select(\"id\").distinct().count()\n",
    "\n",
    "print(\"TMAX without TMIN observations:\", tmax_only_count)\n",
    "print(\"Distinct stations contributing:\", tmax_only_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation 1: TMIN and TMAX subplots for NZ stations + national average\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nz_ids = enriched_stations.filter(F.col(\"country_code\")==\"NZ\").select(\"id\").limit(12)\n",
    "nz_ids_list = [r[\"id\"] for r in nz_ids.collect()]\n",
    "\n",
    "nz_daily = (daily_all.filter(F.col(\"ID\").isin(nz_ids_list) & F.col(\"ELEMENT\").isin([\"TMIN\",\"TMAX\"]))\n",
    "            .select(\"ID\",\"DATE\",\"ELEMENT\",\"VALUE\"))\n",
    "\n",
    "pdf = nz_daily.toPandas()\n",
    "pdf[\"date\"] = pd.to_datetime(pdf[\"DATE\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "pdf = pdf.dropna(subset=[\"date\"])\n",
    "pdf[\"value_c\"] = pdf[\"VALUE\"] / 10.0\n",
    "\n",
    "stations = sorted(pdf[\"ID\"].unique())\n",
    "n = len(stations)\n",
    "cols = 3\n",
    "rows = int((n + cols - 1) / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows), squeeze=False)\n",
    "for i, sid in enumerate(stations):\n",
    "    ax = axes[i//cols][i%cols]\n",
    "    sub = pdf[pdf[\"ID\"]==sid]\n",
    "    for elt in [\"TMIN\",\"TMAX\"]:\n",
    "        ss = sub[sub[\"ELEMENT\"]==elt].sort_values(\"date\")\n",
    "        ax.plot(ss[\"date\"], ss[\"value_c\"], label=elt)\n",
    "    ax.set_title(sid)\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"°C\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "out_png1 = \"/mnt/data/dew59_nz_tmin_tmax_subplots.png\"\n",
    "plt.savefig(out_png1, dpi=160)\n",
    "print(\"Saved figure:\", out_png1)\n",
    "\n",
    "pdf[\"week\"] = pdf[\"date\"].dt.to_period(\"W\").apply(lambda r: r.start_time)\n",
    "avg_nat = (pdf.groupby([\"week\",\"ELEMENT\"])[\"value_c\"].mean().reset_index())\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for elt in [\"TMIN\",\"TMAX\"]:\n",
    "    s = avg_nat[avg_nat[\"ELEMENT\"]==elt].sort_values(\"week\")\n",
    "    plt.plot(s[\"week\"], s[\"value_c\"], label=elt)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"°C\")\n",
    "plt.title(\"NZ average weekly TMIN/TMAX\")\n",
    "\n",
    "out_png2 = \"/mnt/data/dew59_nz_tmin_tmax_country.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_png2, dpi=160)\n",
    "print(\"Saved figure:\", out_png2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1299cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation 2: Choropleth for 2024 average daily rainfall by country\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prcp_path = f\"{WASBS_USER_BASE}/q2a_prcp_year_country.parquet/\"\n",
    "print(\"Loading:\", prcp_path)\n",
    "prcp = spark.read.parquet(prcp_path)\n",
    "\n",
    "prcp_2024 = prcp.filter(F.col(\"year\")==2024).toPandas()\n",
    "countries = enriched_stations.select(\"country_code\",\"country_name\").distinct().toPandas()\n",
    "\n",
    "df = prcp_2024.merge(countries, on=\"country_code\", how=\"left\")\n",
    "\n",
    "import geopandas as gpd\n",
    "world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n",
    "\n",
    "df[\"country_name_l\"] = df[\"country_name\"].str.lower()\n",
    "world[\"name_l\"] = world[\"name\"].str.lower()\n",
    "\n",
    "gdf = world.merge(df, left_on=\"name_l\", right_on=\"country_name_l\", how=\"left\")\n",
    "\n",
    "ax = gdf.plot(column=\"avg_prcp_mm\", legend=True, figsize=(14,7), missing_kwds={\"color\":\"lightgrey\"})\n",
    "ax.set_title(\"Average daily rainfall (mm) by country, 2024\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "out_png3 = \"/mnt/data/dew59_2024_rainfall_choropleth.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_png3, dpi=160)\n",
    "print(\"Saved figure:\", out_png3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
