{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b63cb5",
   "metadata": {},
   "source": [
    "# DATA420 A1 — Processing (additions only)\n",
    "These cells can be appended to your existing **Processing.ipynb**. They avoid guards and focus on straightforward functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba930764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Spark session and common paths\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "print(\"Starting SparkSession for Processing additions...\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "\n",
    "WASBS_DATA = f\"wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd\"\n",
    "WASBS_DAILY = f\"{WASBS_DATA}/daily\"\n",
    "WASBS_COUNTRIES = f\"{WASBS_DATA}/countries.txt\"\n",
    "WASBS_STATES = f\"{WASBS_DATA}/states.txt\"\n",
    "WASBS_STATIONS = f\"{WASBS_DATA}/stations.txt\"\n",
    "WASBS_INVENTORY = f\"{WASBS_DATA}/inventory.txt\"\n",
    "\n",
    "WASBS_USER_BASE = f\"wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/dew59\"\n",
    "print(\"DATA:\", WASBS_DATA)\n",
    "print(\"USER:\", WASBS_USER_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf8c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Inspect storage and capture year sizes\n",
    "import subprocess, re, pandas as pd\n",
    "\n",
    "print(\"Listing data container...\")\n",
    "_ = subprocess.call([\"bash\",\"-lc\", f\"hdfs dfs -ls {WASBS_DATA}\"])\n",
    "\n",
    "print(\"Listing daily files...\")\n",
    "ls_output = subprocess.check_output([\"bash\",\"-lc\", f\"hdfs dfs -ls {WASBS_DAILY}\"], text=True)\n",
    "lines = [ln for ln in ls_output.splitlines() if ln.strip().endswith(\".csv.gz\")]\n",
    "\n",
    "rows = []\n",
    "for ln in lines:\n",
    "    parts = ln.split()\n",
    "    size_bytes = int(parts[4])\n",
    "    path = parts[-1]\n",
    "    year = int(re.search(r'/([0-9]{4})\\.csv\\.gz$', path).group(1))\n",
    "    rows.append((year, size_bytes, path))\n",
    "\n",
    "year_sizes_df = pd.DataFrame(rows, columns=[\"year\",\"size_bytes\",\"path\"]).sort_values(\"year\")\n",
    "year_sizes_df[\"size_MB\"] = year_sizes_df[\"size_bytes\"] / (1024*1024)\n",
    "print(\"Year-size head:\")\n",
    "print(year_sizes_df.head(10))\n",
    "\n",
    "print(\"Total daily size (MB):\", round(year_sizes_df[\"size_MB\"].sum(),2))\n",
    "\n",
    "year_sizes_sdf = spark.createDataFrame(year_sizes_df[[\"year\",\"size_bytes\",\"size_MB\"]])\n",
    "target_year_sizes = f\"{WASBS_USER_BASE}/years_size_metrics.parquet/\"\n",
    "print(\"Writing:\", target_year_sizes)\n",
    "year_sizes_sdf.write.mode(\"overwrite\").parquet(target_year_sizes)\n",
    "print(\"Done writing years_size_metrics.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a,b): Daily schema and most recent year subset\n",
    "daily_schema = T.StructType([\n",
    "    T.StructField(\"ID\", T.StringType(), True),\n",
    "    T.StructField(\"DATE\", T.StringType(), True),\n",
    "    T.StructField(\"ELEMENT\", T.StringType(), True),\n",
    "    T.StructField(\"VALUE\", T.IntegerType(), True),\n",
    "    T.StructField(\"MFLAG\", T.StringType(), True),\n",
    "    T.StructField(\"QFLAG\", T.StringType(), True),\n",
    "    T.StructField(\"SFLAG\", T.StringType(), True),\n",
    "    T.StructField(\"OBSTIME\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "most_recent_year = year_sizes_df[\"year\"].max()\n",
    "recent_path = f\"{WASBS_DAILY}/{most_recent_year}.csv.gz\"\n",
    "print(\"Most recent year:\", most_recent_year)\n",
    "print(\"Loading:\", recent_path)\n",
    "\n",
    "daily_recent = spark.read.csv(recent_path, schema=daily_schema, header=False, mode=\"PERMISSIVE\")\n",
    "print(\"Recent daily count:\", daily_recent.count())\n",
    "daily_recent.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867685d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(c): Load fixed-width metadata tables\n",
    "stn = spark.read.text(WASBS_STATIONS).withColumnRenamed(\"value\",\"raw\")\n",
    "stations_df = stn.select(\n",
    "    F.substring(\"raw\",1,11).alias(\"id\"),\n",
    "    F.trim(F.substring(\"raw\",13,8)).cast(\"double\").alias(\"latitude\"),\n",
    "    F.trim(F.substring(\"raw\",22,9)).cast(\"double\").alias(\"longitude\"),\n",
    "    F.trim(F.substring(\"raw\",32,6)).cast(\"double\").alias(\"elevation\"),\n",
    "    F.trim(F.substring(\"raw\",39,2)).alias(\"state\"),\n",
    "    F.trim(F.substring(\"raw\",42,30)).alias(\"name\"),\n",
    "    F.trim(F.substring(\"raw\",73,3)).alias(\"gsn_flag\"),\n",
    "    F.trim(F.substring(\"raw\",77,3)).alias(\"hcn_crn_flag\"),\n",
    "    F.trim(F.substring(\"raw\",81,5)).alias(\"wmo_id\")\n",
    ")\n",
    "\n",
    "cty = spark.read.text(WASBS_COUNTRIES).withColumnRenamed(\"value\",\"raw\")\n",
    "countries_df = cty.select(\n",
    "    F.substring(\"raw\",1,2).alias(\"code\"),\n",
    "    F.trim(F.substring(\"raw\",4,61)).alias(\"country_name\")\n",
    ")\n",
    "\n",
    "sta = spark.read.text(WASBS_STATES).withColumnRenamed(\"value\",\"raw\")\n",
    "states_df = sta.select(\n",
    "    F.substring(\"raw\",1,2).alias(\"state_code\"),\n",
    "    F.trim(F.substring(\"raw\",4,47)).alias(\"state_name\")\n",
    ")\n",
    "\n",
    "inv = spark.read.text(WASBS_INVENTORY).withColumnRenamed(\"value\",\"raw\")\n",
    "inventory_df = inv.select(\n",
    "    F.substring(\"raw\",1,11).alias(\"id\"),\n",
    "    F.trim(F.substring(\"raw\",13,8)).cast(\"double\").alias(\"lat\"),\n",
    "    F.trim(F.substring(\"raw\",22,9\")).cast(\"double\").alias(\"lon\"),\n",
    "    F.trim(F.substring(\"raw\",32,4)).alias(\"element\"),\n",
    "    F.trim(F.substring(\"raw\",37,4\")).cast(\"int\").alias(\"firstyear\"),\n",
    "    F.trim(F.substring(\"raw\",42,4\")).cast(\"int\").alias(\"lastyear\")\n",
    ")\n",
    "\n",
    "print(\"Stations:\", stations_df.count())\n",
    "print(\"Countries:\", countries_df.count())\n",
    "print(\"States:\", states_df.count())\n",
    "print(\"Inventory:\", inventory_df.count())\n",
    "\n",
    "stations_df.show(3, truncate=False)\n",
    "countries_df.show(3, truncate=False)\n",
    "states_df.show(3, truncate=False)\n",
    "inventory_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e1e8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3(a–e): Enriched stations and core flags\n",
    "stations_with_cc = stations_df.withColumn(\"country_code\", F.substring(\"id\",1,2))\n",
    "\n",
    "stations_cc = stations_with_cc.join(\n",
    "    countries_df.withColumnRenamed(\"code\",\"country_code\"),\n",
    "    on=\"country_code\",\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    states_df,\n",
    "    stations_with_cc[\"state\"] == states_df[\"state_code\"],\n",
    "    how=\"left\"\n",
    ").drop(states_df[\"state_code\"])\n",
    "\n",
    "inv_agg = (inventory_df\n",
    "           .groupBy(\"id\")\n",
    "           .agg(F.min(\"firstyear\").alias(\"first_year\"),\n",
    "                F.max(\"lastyear\").alias(\"last_year\"),\n",
    "                F.size(F.collect_set(\"element\")).alias(\"element_count\"),\n",
    "                F.collect_set(\"element\").alias(\"element_set\"))\n",
    ")\n",
    "\n",
    "enriched_stations = (stations_cc.join(inv_agg, on=\"id\", how=\"left\")\n",
    "                     .withColumn(\"has_prcp\", F.array_contains(\"element_set\",\"PRCP\"))\n",
    "                     .withColumn(\"has_snow\", F.array_contains(\"element_set\",\"SNOW\"))\n",
    "                     .withColumn(\"has_snwd\", F.array_contains(\"element_set\",\"SNWD\"))\n",
    "                     .withColumn(\"has_tmax\", F.array_contains(\"element_set\",\"TMAX\"))\n",
    "                     .withColumn(\"has_tmin\", F.array_contains(\"element_set\",\"TMIN\"))\n",
    "                     .withColumn(\"core_count\",\n",
    "                                 F.expr(\"int(has_prcp) + int(has_snow) + int(has_snwd) + int(has_tmax) + int(has_tmin)\"))\n",
    ")\n",
    "\n",
    "count_all_five = enriched_stations.filter(F.col(\"core_count\")==5).count()\n",
    "count_prcp_only = enriched_stations.filter(\n",
    "    (F.col(\"has_prcp\")==True) & (F.col(\"has_snow\")==False) & (F.col(\"has_snwd\")==False) & (F.col(\"has_tmax\")==False) & (F.col(\"has_tmin\")==False)\n",
    ").count()\n",
    "\n",
    "print(\"Stations with all five core elements:\", count_all_five)\n",
    "print(\"Stations with precipitation only:\", count_prcp_only)\n",
    "\n",
    "target_enriched = f\"{WASBS_USER_BASE}/enriched_stations.parquet/\"\n",
    "print(\"Writing enriched stations to:\", target_enriched)\n",
    "enriched_stations.write.mode(\"overwrite\").parquet(target_enriched)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466730a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4(a,b): Stations present in stations but not in daily\n",
    "daily_all = spark.read.csv(f\"{WASBS_DAILY}/*.csv.gz\", schema=daily_schema, header=False, mode=\"PERMISSIVE\")\n",
    "daily_station_ids = daily_all.select(\"ID\").distinct().withColumnRenamed(\"ID\",\"id\")\n",
    "\n",
    "stations_not_in_daily = enriched_stations.select(\"id\").join(daily_station_ids, on=\"id\", how=\"left_anti\")\n",
    "missing_count = stations_not_in_daily.count()\n",
    "print(\"Stations in stations but not in daily:\", missing_count)\n",
    "\n",
    "target_missing = f\"{WASBS_USER_BASE}/stations_missing_in_daily.parquet/\"\n",
    "print(\"Writing:\", target_missing)\n",
    "stations_not_in_daily.write.mode(\"overwrite\").parquet(target_missing)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbbf8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra outputs used later in analysis and visualisation\n",
    "q2a_station_date_element = (daily_all\n",
    "    .select(F.col(\"ID\").alias(\"id\"), \"DATE\", \"ELEMENT\")\n",
    ")\n",
    "out1 = f\"{WASBS_USER_BASE}/q2a_station_date_element.parquet/\"\n",
    "print(\"Writing:\", out1)\n",
    "q2a_station_date_element.write.mode(\"overwrite\").parquet(out1)\n",
    "\n",
    "daily_prcp = daily_all.filter(F.col(\"ELEMENT\")==\"PRCP\").withColumn(\"prcp_mm\", F.col(\"VALUE\")/10.0)\n",
    "daily_with_cc = daily_prcp.join(enriched_stations.select(\"id\",\"country_code\"), on=\"id\", how=\"left\")\n",
    "\n",
    "q2a_prcp_year_country = (daily_with_cc\n",
    "    .withColumn(\"year\", F.substring(\"DATE\",1,4).cast(\"int\"))\n",
    "    .groupBy(\"country_code\",\"year\")\n",
    "    .agg(F.mean(\"prcp_mm\").alias(\"avg_prcp_mm\"))\n",
    ")\n",
    "\n",
    "out2 = f\"{WASBS_USER_BASE}/q2a_prcp_year_country.parquet/\"\n",
    "print(\"Writing:\", out2)\n",
    "q2a_prcp_year_country.write.mode(\"overwrite\").parquet(out2)\n",
    "\n",
    "print(\"All Processing additions complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
