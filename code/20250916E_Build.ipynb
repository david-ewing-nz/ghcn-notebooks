{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db545270-fabc-4cf7-a32a-e4f4d6fef18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark \n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "global azure_account_name\n",
    "global azure_data_container_name\n",
    "global azure_user_container_name\n",
    "global azure_user_token\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "azure_user_token = r\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"\n",
    "\n",
    " \n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(f\"fs.azure.sas.{azure_user_container_name}.{azure_account_name}.blob.core.windows.net\",  azure_user_token)\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bbb004-5a22-4f65-b400-cb2bc3edf217",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e2f6d-f8f0-4150-8eec-e5bb2b0471e4",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06343f5-a638-4f7f-ade5-5f564a068bc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/09/17 22:55:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4041\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-82541ce020b84a55804a6444e21d5eef</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>32</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-0666ed995750c4e8</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1758106534286</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1758106534169</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16</td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=4, executor_cores=2, worker_memory=4, master_memory=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7556669f-ee60-4328-8b8f-29d26d48be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "\n",
    "from IPython.display     import display  # calls between environments\n",
    "from math                import acos, atan2, cos, radians, sin, sqrt\n",
    "from matplotlib.ticker   import FuncFormatter, MaxNLocator\n",
    "from pathlib             import Path\n",
    "from pyspark.sql         import DataFrame\n",
    "from pyspark.sql         import DataFrame as SparkDF\n",
    "from pyspark.sql         import functions as F, types as T\n",
    "from pyspark.sql.types   import *\n",
    "from pyspark.sql.utils   import AnalysisException\n",
    "from pyspark.sql.window  import Window\n",
    "from typing              import List, Optional, Tuple\n",
    "\n",
    "import itertools         as it\n",
    "import matplotlib.dates  as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import math, os, platform, re\n",
    "import subprocess, sys, time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cda5876-8608-4f13-8dba-fa5b7898d0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________HELPER / DIAGNOSTIC FUNCTIONS___________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"_\" * 35 + \"HELPER / DIAGNOSTIC FUNCTIONS\" + \"_\" * 35)\n",
    "\n",
    "notebook_run_time = time.time()\n",
    "\n",
    "def df_as_html(df, n: int = 5, right_align: bool = False, show_index: bool = False):\n",
    "    \"\"\"\n",
    "    HTML preview via pandas with no truncation. If right_align=True,\n",
    "    only numeric columns are right-justified; everything else is \n",
    "    explicitly left-aligned.\n",
    "    \"\"\"\n",
    "    \n",
    "    pdf = df.limit(n).toPandas()\n",
    "    print(\"[INFO] Converting Spark → pandas for HTML display (rows:\", len(pdf), \")\")\n",
    "    print(\"[INFO] right_align (numeric columns):\", right_align)\n",
    "\n",
    "    with pd.option_context(\n",
    "        \"display.max_colwidth\", None,   \n",
    "        \"display.max_columns\", None,    \n",
    "        \"display.width\", None            \n",
    "    ):\n",
    "        styler = pdf.style if show_index else pdf.style.hide(axis=\"index\")\n",
    "\n",
    "        #   table alignment: left for both headers and cells\n",
    "        styler = styler.set_table_styles(\n",
    "            [\n",
    "                {\"selector\": \"th\", \"props\": [(\"text-align\", \"left\")]},\n",
    "                {\"selector\": \"td\", \"props\": [(\"text-align\", \"left\")]},\n",
    "            ],\n",
    "            overwrite=True,  # make this the baseline\n",
    "        )\n",
    "         \n",
    "        if right_align:\n",
    "            numeric_cols = list(pdf.select_dtypes(include=[\"number\"]).columns)\n",
    "            print(\"[INFO] Right-aligning numeric columns:\", numeric_cols)\n",
    "            if numeric_cols:\n",
    "                styler = styler.set_properties(subset=numeric_cols,\n",
    "                                               **{\"text-align\": \"right\"})\n",
    "        display(styler)\n",
    "\n",
    "def _normalise_dir(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure trailing slash so we point to\n",
    "    the dataset directory (not a file)\n",
    "    \"\"\"\n",
    "    return s if s.endswith(\"/\") else s + \"/\"\n",
    "\n",
    "def ensure_dir(path: str) -> str:\n",
    "    \"\"\"\n",
    "    ensures that path is a path \n",
    "    and not representing a file;\n",
    "    add trailing slash if needed\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        raise ValueError(\"Path is None\")\n",
    "    path = _normalise_dir(path)\n",
    "#   print(\"ensure_dir -> \",path)\n",
    "    return path\n",
    "    \n",
    "def show_df(df: DataFrame, n: int = 10, name: str = \"\", right_align: bool = False):\n",
    "    \"\"\"\n",
    "    Print schema, \n",
    "    show an HTML sample,\n",
    "    and row count.\n",
    "    \"\"\"\n",
    "    bprint()\n",
    "    print(\"name : \",name)\n",
    "    df.printSchema()\n",
    "    print(\"[check] sample:\")\n",
    "    df_as_html(df, n=n, right_align=right_align)\n",
    "  \n",
    "def write_parquet(df: DataFrame, dir_as_path: str, df_name:str = \"\"):    \n",
    "    funct_time = time.time()\n",
    "    path = _normalise_dir(dir_as_path)\n",
    "    print(f\"[file] write_parquet  : {path}\")\n",
    "    try:      \n",
    "        show_df(df,df_name)\n",
    "    except Exception as e:\n",
    "        print(\"[cathch] sample failed:\", e)\n",
    "        os.system(f'hdfs dfs -rm -r -f \"{path}\"')   # idempotent cleanup\n",
    "    df.write.mode(\"overwrite\").format(\"parquet\").save(path)\n",
    "    os.system(f'hdfs dfs -ls -R \"{path}\"')\n",
    "    funct_time = time.time() - funct_time \n",
    "    print(f\"[time] write_parquet (min)   : {funct_time/60:5.2f}\")\n",
    "    print(f\"[time] write_parquet (sec)   : {funct_time:5.2f}\")\n",
    "  \n",
    "def has_parquet(dir_as_path: str) -> bool:\n",
    "    path   = _normalise_dir( dir_as_path)\n",
    "    marker = path + '_SUCCESS'\n",
    "    #print(\"\\n[check] dir_path:\", dir_path)\n",
    "    #print(\"\\n[check] path    :\", path)\n",
    "    print(\"\\n[check] marker  :\", marker)\n",
    "    rc = os.system(f'hdfs dfs -test -e \"{marker}\"')\n",
    "    print(\"[check] rc:\", rc, \"->\", (\"exists\" if rc == 0 else \"missing\"))\n",
    "    return (rc == 0)\n",
    "    \n",
    "def _to_spark(df_like, schema=None):\n",
    "    \"\"\"\n",
    "    Return a Spark DataFrame  .\n",
    "    \"\"\"\n",
    "    if isinstance(df_like, SparkDF):\n",
    "        return df_like\n",
    "    return spark.createDataFrame(df_like, schema=schema) if schema else spark.createDataFrame(df_like)\n",
    "\n",
    "def _success_exists(target_dir: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check for the Hadoop/Spark _SUCCESS marker;  \n",
    "    \"\"\"\n",
    "    jvm = spark._jvm\n",
    "    hconf = spark._jsc.hadoopConfiguration()\n",
    "    try:\n",
    "        uri = jvm.java.net.URI(target_dir)\n",
    "        fs = jvm.org.apache.hadoop.fs.FileSystem.get(uri, hconf)\n",
    "        success = jvm.org.apache.hadoop.fs.Path(target_dir + \"_SUCCESS\")\n",
    "        exists = fs.exists(success)\n",
    "        print(f\"[status] _SUCCESS check at: {target_dir}_SUCCESS -> {exists}\")\n",
    "        return bool(exists)\n",
    "    except Exception as e:\n",
    "        print(f\"[status] _SUCCESS check failed ({e}); attempting read-probe …\")\n",
    "        try:\n",
    "            spark.read.parquet(target_dir).limit(1).count()\n",
    "            print(f\"[dewstatus59] read-probe succeeded at: {target_dir}\")\n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"[status] read-probe failed ({e2}); treating as not existing.\")\n",
    "            return False\n",
    " \n",
    "\n",
    "def normalise_ids(df: DataFrame, col: str = \"ID\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    # Single source of truth for ID normalisation \n",
    "    Upper + trim + distinct on the given ID column.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] normalise_ids() on column: {col}\")\n",
    "    df.printSchema()\n",
    "    df.show(20)\n",
    "    return df.select(F.upper(F.trim(F.col(col))).alias(\"ID\")).distinct()\n",
    "    df.printSchema()\n",
    "    df.show(20)\n",
    "# hack \n",
    "_ids       = normalise_ids\n",
    "canon_ids  = normalise_ids\n",
    "_canon_ids = normalise_ids\n",
    "\n",
    "def probe_universe_optimized(daily_df, stations_df, inv_agg_df,\n",
    "                             tag=\"Optimized\"):\n",
    "    \"\"\"\n",
    "    Optimized version of probe_universe with memory-efficient operations\n",
    "    \"\"\"\n",
    "    print(f\"[{tag}] Starting optimized universe probe...\")\n",
    "\n",
    "    # OPTIMIZATION 1: Cache ID DataFrames immediately and force evaluation\n",
    "    print(f\"[{tag}] Caching ID DataFrames...\")\n",
    "    daily_ids = daily_df.select(\"ID\").distinct().cache()\n",
    "    station_ids = stations_df.select(\"ID\").distinct().cache()\n",
    "    inv_ids = inv_agg_df.select(\"ID\").distinct().cache()\n",
    "\n",
    "    # Force caching by triggering count operations\n",
    "    daily_count = daily_ids.count()\n",
    "    station_count = station_ids.count()\n",
    "    inv_count = inv_ids.count()\n",
    "\n",
    "    print(f\"[{tag}] Cached: daily={daily_count}, station={station_count}, \"\n",
    "          f\"inv={inv_count}\")\n",
    "\n",
    "    # OPTIMIZATION 2: Use broadcast joins for better memory efficiency\n",
    "    from pyspark.sql.functions import broadcast\n",
    "\n",
    "    # OPTIMIZATION 3: Compute differences with better memory management\n",
    "    print(f\"[{tag}] Computing set differences...\")\n",
    "\n",
    "    # Daily - Station\n",
    "    diff1 = daily_ids.join(broadcast(station_ids), \"ID\", \"left_anti\").cache()\n",
    "    diff1_count = diff1.count()\n",
    "    print(f\"[{tag}] Daily - Station: {diff1_count}\")\n",
    "\n",
    "    # Station - Daily\n",
    "    diff2 = station_ids.join(broadcast(daily_ids), \"ID\", \"left_anti\").cache()\n",
    "    diff2_count = diff2.count()\n",
    "    print(f\"[{tag}] Station - Daily: {diff2_count}\")\n",
    "\n",
    "    # Station - Inventory\n",
    "    diff3 = station_ids.join(broadcast(inv_ids), \"ID\", \"left_anti\").cache()\n",
    "    diff3_count = diff3.count()\n",
    "    print(f\"[{tag}] Station - Inventory: {diff3_count}\")\n",
    "\n",
    "    # Inventory - Daily\n",
    "    diff4 = inv_ids.join(broadcast(daily_ids), \"ID\", \"left_anti\").cache()\n",
    "    diff4_count = diff4.count()\n",
    "    print(f\"[{tag}] Inventory - Daily: {diff4_count}\")\n",
    "\n",
    "    # Clean up cached DataFrames\n",
    "    daily_ids.unpersist()\n",
    "    station_ids.unpersist()\n",
    "    inv_ids.unpersist()\n",
    "    diff1.unpersist()\n",
    "    diff2.unpersist()\n",
    "    diff3.unpersist()\n",
    "    diff4.unpersist()\n",
    "\n",
    " \n",
    "\n",
    "def probe_universe(daily_df, stations_df, inv_agg_df, tag=\"Default\"):\n",
    "    \"\"\"\n",
    "    Main probe_universe function that calls the optimized version\n",
    "    and formats output for diagnostic comparison\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {tag.upper()} DIAGNOSTICS ===\")\n",
    "    print(\"Starting universe probe analysis...\")\n",
    "\n",
    "    # Call the optimized version\n",
    "    probe_universe_optimized(daily_df, stations_df, inv_agg_df, tag)\n",
    "\n",
    "    # Additional diagnostic output for comparison\n",
    "    print(f\"\\n[{tag}] Additional diagnostics:\")\n",
    "\n",
    "    # Get counts using the helper function\n",
    "    daily_count = _count_unique_ids(daily_df)\n",
    "    station_count = _count_unique_ids(stations_df)\n",
    "    inv_count = _count_unique_ids(inv_agg_df)\n",
    "\n",
    "    print(f\"[COUNT] daily IDs         : {daily_count}\")\n",
    "    print(f\"[COUNT] station IDs (cat) : {station_count}\")\n",
    "    print(f\"[COUNT] inventory IDs     : {inv_count}\")\n",
    "\n",
    "    # Calculate differences\n",
    "    daily_ids = daily_df.select(\"ID\").distinct()\n",
    "    station_ids = stations_df.select(\"ID\").distinct()\n",
    "    inv_ids = inv_agg_df.select(\"ID\").distinct()\n",
    "\n",
    "    # Daily - Station\n",
    "    diff_daily_station = daily_ids.join(station_ids, \"ID\", \"left_anti\").count()\n",
    "    print(f\"[DIFF ] daily  – station   : {diff_daily_station}\")\n",
    "\n",
    "    # Station - Daily\n",
    "    diff_station_daily = station_ids.join(daily_ids, \"ID\", \"left_anti\").count()\n",
    "    print(f\"[DIFF ] station – daily    : {diff_station_daily}\")\n",
    "\n",
    "    # Station - Inventory\n",
    "    diff_station_inv = station_ids.join(inv_ids, \"ID\", \"left_anti\").count()\n",
    "    print(f\"[DIFF ] station – inv      : {diff_station_inv}\")\n",
    "\n",
    "    # Inventory - Daily\n",
    "    diff_inv_daily = inv_ids.join(daily_ids, \"ID\", \"left_anti\").count()\n",
    "    print(f\"[DIFF ] inv     – daily    : {diff_inv_daily}\")\n",
    "\n",
    "    # Inventory - Station\n",
    "    diff_inv_station = inv_ids.join(station_ids, \"ID\", \"left_anti\").count()\n",
    "    print(f\"[DIFF ] inv     – station  : {diff_inv_station}\")\n",
    "\n",
    "    print(f\"\\n[{tag}] Analysis complete!\")\n",
    "    return {\n",
    "        'daily_count': daily_count,\n",
    "        'station_count': station_count,\n",
    "        'inventory_count': inv_count,\n",
    "        'diffs': [diff_daily_station, diff_station_daily, diff_station_inv, diff_inv_daily]\n",
    "    }\n",
    "\n",
    "\n",
    "def _count_unique_ids(df: DataFrame) -> int:\n",
    "    return normalise_ids(df).count()\n",
    "\n",
    "def pick_unfiltered_daily(preferred_path: str = None) -> DataFrame:\n",
    "    \"\"\"Return an unfiltered daily DF (~129k unique station IDs).\"\"\"\n",
    "    cand_names = [\"daily\", \"read_daily\", \"daily_df\", \"daily_all\", \"ghcnd_daily\"]\n",
    "    print(\"[INFO] Candidate DataFrames:\", [n for n in cand_names if n in globals()])\n",
    "    for name in cand_names:\n",
    "        obj = globals().get(name)\n",
    "        if isinstance(obj, DataFrame):\n",
    "            try:\n",
    "                n = normalise_ids(obj).count()\n",
    "                print(f\"[CHECK] {name} unique station IDs:\", n)\n",
    "                if n >= 120_000:\n",
    "                    print(f\"[INFO] Using {name} as the unfiltered daily.\")\n",
    "                    return obj\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not inspect {name}:\", repr(e))\n",
    "    if preferred_path:\n",
    "        print(f\"[INFO] Trying preferred_path: {preferred_path}\")\n",
    "        df = spark.read.parquet(str(preferred_path))\n",
    "        n = normalise_ids(df).count()\n",
    "        print(\"[CHECK] preferred_path unique station IDs:\", n)\n",
    "        if n >= 120_000:\n",
    "            print(\"[INFO] Using preferred_path as the unfiltered daily.\")\n",
    "            return df\n",
    "    for var in [\"DAILY_READ_NAME\",\"DAILY_WRITE_NAME\",\"daily_read_name\",\"daily_write_name\",\"DAILY_NAME\"]:\n",
    "        if var in globals():\n",
    "            path = globals()[var]\n",
    "            try:\n",
    "                print(f\"[INFO] Trying {var} = {path}\")\n",
    "                df = spark.read.parquet(str(path))\n",
    "                n = normalise_ids(df).count()\n",
    "                print(f\"[CHECK] {var} unique station IDs:\", n)\n",
    "                if n >= 120_000:\n",
    "                    print(f\"[INFO] Using {var} as the unfiltered daily.\")\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not read {var}:\", repr(e))\n",
    "    raise SystemExit(\"[FATAL] Could not find an unfiltered daily dataset (expected ~129k unique station IDs).\")\n",
    "\n",
    "def bprint(text: str=\"\", l=50):\n",
    "    n = len(text)\n",
    "    n = abs(n - l)//2\n",
    "    \n",
    "    print(\"\\n\" + \"_\" * n + text + \"_\" * n)\n",
    "\n",
    "# Back-compat aliases hack to account for non-disciplined naming un-convention\n",
    "_ids       = normalise_ids\n",
    "canon_ids  = normalise_ids\n",
    "_canon_ids = normalise_ids\n",
    "\n",
    "#print(\"[TEST] Using _canon_ids:\", _canon_ids(stations).count())\n",
    "#print(\"[TEST] Using canon_ids :\", canon_ids(stations).count())\n",
    "#print(\"[TEST] Using _ids      :\", _ids(stations).count())\n",
    "\n",
    "\n",
    "# : pairwise city distances in km using Spark built-ins \n",
    "def pairwise_city_distances_spark(cities, radius_km=6371.0):\n",
    "    \"\"\"\n",
    "    cities: list[tuple[str, float, float]] -> [(name, lat_deg, lon_deg), ...]\n",
    "    returns: Spark DataFrame with columns:\n",
    "             city_a, city_b, haversine_km, slc_km, delta_km, delta_pct\n",
    "    \"\"\"\n",
    "  #  from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        raise RuntimeError(\"No active Spark session.\")\n",
    "\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"city\", T.StringType(), False),\n",
    "        T.StructField(\"lat\",  T.DoubleType(), False),\n",
    "        T.StructField(\"lon\",  T.DoubleType(), False),\n",
    "        ])\n",
    "    df = spark.createDataFrame(cities, schema)\n",
    "\n",
    "    a, b = df.alias(\"a\"), df.alias(\"b\")\n",
    "    pairs = (a.join(b, F.col(\"a.city\") < F.col(\"b.city\"))\n",
    "               .select(F.col(\"a.city\").alias(\"city_a\"),\n",
    "                       F.col(\"b.city\").alias(\"city_b\"),\n",
    "                       F.col(\"a.lat\").alias(\"lat1\"),\n",
    "                       F.col(\"a.lon\").alias(\"lon1\"),\n",
    "                       F.col(\"b.lat\").alias(\"lat2\"),\n",
    "                       F.col(\"b.lon\").alias(\"lon2\")))\n",
    "\n",
    "    R = F.lit(float(radius_km))\n",
    "    lat1 = F.radians(F.col(\"lat1\"));  lat2 = F.radians(F.col(\"lat2\"))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = F.radians(F.col(\"lon2\") - F.col(\"lon1\"))\n",
    "\n",
    "    a_term = F.sin(dlat/2)**2 + F.cos(lat1)*F.cos(lat2)*F.sin(dlon/2)**2\n",
    "    c_term = 2*F.atan2(F.sqrt(a_term), F.sqrt(1 - a_term))\n",
    "    hav_km = R * c_term\n",
    "\n",
    "    cos_val = F.sin(lat1)*F.sin(lat2) + F.cos(lat1)*F.cos(lat2)*F.cos(dlon)\n",
    "    cos_val = F.greatest(F.lit(-1.0), F.least(F.lit(1.0), cos_val))\n",
    "    slc_km = R * F.acos(cos_val)\n",
    "\n",
    "    delta_km  = F.abs(hav_km - slc_km)\n",
    "    delta_pct = F.when(hav_km == 0, F.lit(0.0)).otherwise(delta_km / hav_km * 100.0)\n",
    "\n",
    "    out_df = (pairs\n",
    "              .withColumn(\"haversine_km\", F.round(hav_km, 2))\n",
    "              .withColumn(\"slc_km\",       F.round(slc_km, 2))\n",
    "              .withColumn(\"delta_km\",     F.round(delta_km, 4))\n",
    "              .withColumn(\"delta_pct\",    F.round(delta_pct, 6))\n",
    "              .select(\"city_a\", \"city_b\", \"haversine_km\", \"slc_km\", \"delta_km\", \"delta_pct\")\n",
    "              .orderBy(\"haversine_km\"))\n",
    "    return out_df\n",
    "\n",
    "\n",
    "# --- Timing helpers for Spark & pure Python (no extra deps)\n",
    " \n",
    "\n",
    "def benchmark_python_distances(cities, radius_km=6371.0, repeats=50000):\n",
    "    \"\"\"\n",
    "    cities: [(name, lat_deg, lon_deg), ...]  (3 cities => 3 pairs)\n",
    "    repeats: loop count to make timings stable\n",
    "    returns: dict with seconds for haversine/slc\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for i in range(len(cities)):\n",
    "        for j in range(i+1, len(cities)):\n",
    "            (_, lat1, lon1), (_, lat2, lon2) = cities[i], cities[j]\n",
    "            pairs.append((lat1, lon1, lat2, lon2))\n",
    "\n",
    "    # haversine\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        for lat1, lon1, lat2, lon2 in pairs:\n",
    "            φ1, λ1, φ2, λ2 = map(radians, (lat1, lon1, lat2, lon2))\n",
    "            dφ, dλ = (φ2 - φ1), (λ2 - λ1)\n",
    "            a = sin(dφ/2)**2 + cos(φ1)*cos(φ2)*sin(dλ/2)**2\n",
    "            c = 2*atan2(sqrt(a), sqrt(1 - a))\n",
    "            _ = radius_km * c\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # spherical law of cosines (SLC)\n",
    "    t2 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        for lat1, lon1, lat2, lon2 in pairs:\n",
    "            φ1, λ1, φ2, λ2 = map(radians, (lat1, lon1, lat2, lon2))\n",
    "            cosv = sin(φ1)*sin(φ2) + cos(φ1)*cos(φ2)*cos(λ2 - λ1)\n",
    "            cosv = max(-1.0, min(1.0, cosv))\n",
    "            _ = radius_km * acos(cosv)\n",
    "    t3 = perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"python_haversine_sec\": t1 - t0,\n",
    "        \"python_slc_sec\":       t3 - t2,\n",
    "        \"repeats\": repeats,\n",
    "        \"pairs\": len(pairs),\n",
    "    }\n",
    "\n",
    "\n",
    "def _parse_ls_bytes(line): \n",
    "    parts = line.split()\n",
    "    if len(parts) < 8:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[4])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def _parse_du_bytes(line):\n",
    "    parts = line.split()\n",
    "    if len(parts) < 2:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[0])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def du_bytes(path):\n",
    "    lines = get_ipython().getoutput(f'hdfs dfs -du \"{path}\"')\n",
    "    total = 0\n",
    "    for ln in lines:\n",
    "        parts = ln.split()\n",
    "        if len(parts) >= 2:\n",
    "            try:\n",
    "                total += int(parts[0])\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return total\n",
    "    \n",
    "def benchmark_spark_distances(cities, radius_km=6368.6, repeats=3):\n",
    "    \"\"\"\n",
    "    Uses Spark built-ins only. Measures full execution\n",
    "    time by forcing an action.\n",
    "    \n",
    "    returns: dict with seconds for haversine/slc and\n",
    "    row counts used.\n",
    "    \n",
    "    For the radius:\n",
    "    \n",
    "    The Earth is slightly flattened, so the geocentric \n",
    "    radius depends on latitude.  For context: \n",
    "    \n",
    "    * equatorial radius = 6,378.137 km; \n",
    "    * polar radius      = 6,356.752 km \n",
    "    \n",
    "    Across New Zealand’s latitudes (≈36–47°S), using the\n",
    "    WGS-84 ellipsoid, you get roughly:\n",
    "\n",
    "    Auckland (37°S):       ~6,370.4 km\n",
    "    Christchurch (43.5°S): ~6,368.0 km\n",
    "    Dunedin (45.9°S):      ~6,367.2 km\n",
    "    __________________________________\n",
    "    Wellington (41°S):     ~6,369.0 km\n",
    "    mean                  ≈ 6,368.6 km\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        from pyspark.sql import SparkSession, functions as F, types as T\n",
    "    except Exception:\n",
    "        return None  # no Spark therefore save cannot run in vs code\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        return None\n",
    "\n",
    "    # build pairs once and cache\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"city\", T.StringType(), False),\n",
    "        T.StructField(\"lat\",  T.DoubleType(), False),\n",
    "        T.StructField(\"lon\",  T.DoubleType(), False),\n",
    "    ])\n",
    "    df = spark.createDataFrame(cities, schema)\n",
    "    a, b = df.alias(\"a\"), df.alias(\"b\")\n",
    "    pairs = (a.join(b, F.col(\"a.city\") < F.col(\"b.city\"))\n",
    "               .select(F.col(\"a.lat\").alias(\"lat1\"),\n",
    "                       F.col(\"a.lon\").alias(\"lon1\"),\n",
    "                       F.col(\"b.lat\").alias(\"lat2\"),\n",
    "                       F.col(\"b.lon\").alias(\"lon2\"))\n",
    "               .cache())\n",
    "    _ = pairs.count()\n",
    "\n",
    "    R = F.lit(float(radius_km))\n",
    "    lat1 = F.radians(F.col(\"lat1\")); lat2 = F.radians(F.col(\"lat2\"))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = F.radians(F.col(\"lon2\") - F.col(\"lon1\"))\n",
    "\n",
    "    # Haversine expr\n",
    "    a_term = F.sin(dlat/2)**2 + F.cos(lat1)*F.cos(lat2)*F.sin(dlon/2)**2\n",
    "    c_term = 2*F.atan2(F.sqrt(a_term), F.sqrt(1 - a_term))\n",
    "    hav    = R * c_term\n",
    "\n",
    "    # SLC expr\n",
    "    cosv = F.sin(lat1)*F.sin(lat2) + F.cos(lat1)*F.cos(lat2)*F.cos(dlon)\n",
    "    cosv = F.greatest(F.lit(-1.0), F.least(F.lit(1.0), cosv))\n",
    "    slc = R * F.acos(cosv)\n",
    "\n",
    "    # time Haversine\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(hav.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # time SLC\n",
    "    t2 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(slc.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t3 = perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"spark_pairs\": pairs.count(),\n",
    "        \"spark_repeats\": repeats,\n",
    "        \"spark_haversine_sec\": t1 - t0,\n",
    "        \"spark_slc_sec\":       t3 - t2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf14ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "124c14ad-23d1-4adb-9919-9829c3eff9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "____________SECTION 2: DATA INGESTION____________\n",
      "Reading all years: wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/*.csv.gz\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__________________________________________________\n",
      "name :  daily (full, wildcard)\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- ELEMENT: string (nullable = true)\n",
      " |-- VALUE: integer (nullable = true)\n",
      " |-- MFLAG: string (nullable = true)\n",
      " |-- QFLAG: string (nullable = true)\n",
      " |-- SFLAG: string (nullable = true)\n",
      " |-- OBSTIME: string (nullable = true)\n",
      "\n",
      "[check] sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Converting Spark → pandas for HTML display (rows: 10 )\n",
      "[INFO] right_align (numeric columns): False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d3225 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_d3225 td {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d3225\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_d3225_level0_col0\" class=\"col_heading level0 col0\" >ID</th>\n",
       "      <th id=\"T_d3225_level0_col1\" class=\"col_heading level0 col1\" >DATE</th>\n",
       "      <th id=\"T_d3225_level0_col2\" class=\"col_heading level0 col2\" >ELEMENT</th>\n",
       "      <th id=\"T_d3225_level0_col3\" class=\"col_heading level0 col3\" >VALUE</th>\n",
       "      <th id=\"T_d3225_level0_col4\" class=\"col_heading level0 col4\" >MFLAG</th>\n",
       "      <th id=\"T_d3225_level0_col5\" class=\"col_heading level0 col5\" >QFLAG</th>\n",
       "      <th id=\"T_d3225_level0_col6\" class=\"col_heading level0 col6\" >SFLAG</th>\n",
       "      <th id=\"T_d3225_level0_col7\" class=\"col_heading level0 col7\" >OBSTIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_d3225_row0_col0\" class=\"data row0 col0\" >ASN00030019</td>\n",
       "      <td id=\"T_d3225_row0_col1\" class=\"data row0 col1\" >2010-01-01</td>\n",
       "      <td id=\"T_d3225_row0_col2\" class=\"data row0 col2\" >PRCP</td>\n",
       "      <td id=\"T_d3225_row0_col3\" class=\"data row0 col3\" >24</td>\n",
       "      <td id=\"T_d3225_row0_col4\" class=\"data row0 col4\" >None</td>\n",
       "      <td id=\"T_d3225_row0_col5\" class=\"data row0 col5\" >None</td>\n",
       "      <td id=\"T_d3225_row0_col6\" class=\"data row0 col6\" >a</td>\n",
       "      <td id=\"T_d3225_row0_col7\" class=\"data row0 col7\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d3225_row1_col0\" class=\"data row1 col0\" >ASN00030021</td>\n",
       "      <td id=\"T_d3225_row1_col1\" class=\"data row1 col1\" >2010-01-01</td>\n",
       "      <td id=\"T_d3225_row1_col2\" class=\"data row1 col2\" >PRCP</td>\n",
       "      <td id=\"T_d3225_row1_col3\" class=\"data row1 col3\" >200</td>\n",
       "      <td id=\"T_d3225_row1_col4\" class=\"data row1 col4\" >None</td>\n",
       "      <td id=\"T_d3225_row1_col5\" class=\"data row1 col5\" >None</td>\n",
       "      <td id=\"T_d3225_row1_col6\" class=\"data row1 col6\" >a</td>\n",
       "      <td id=\"T_d3225_row1_col7\" class=\"data row1 col7\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d3225_row2_col0\" class=\"data row2 col0\" >ASN00030022</td>\n",
       "      <td id=\"T_d3225_row2_col1\" class=\"data row2 col1\" >2010-01-01</td>\n",
       "      <td id=\"T_d3225_row2_col2\" class=\"data row2 col2\" >TMAX</td>\n",
       "      <td id=\"T_d3225_row2_col3\" class=\"data row2 col3\" >294</td>\n",
       "      <td id=\"T_d3225_row2_col4\" class=\"data row2 col4\" >None</td>\n",
       "      <td id=\"T_d3225_row2_col5\" class=\"data row2 col5\" >None</td>\n",
       "      <td id=\"T_d3225_row2_col6\" class=\"data row2 col6\" >a</td>\n",
       "      <td id=\"T_d3225_row2_col7\" class=\"data row2 col7\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d3225_row3_col0\" class=\"data row3 col0\" >ASN00030022</td>\n",
       "      <td id=\"T_d3225_row3_col1\" class=\"data row3 col1\" >2010-01-01</td>\n",
       "      <td id=\"T_d3225_row3_col2\" class=\"data row3 col2\" >TMIN</td>\n",
       "      <td id=\"T_d3225_row3_col3\" class=\"data row3 col3\" >215</td>\n",
       "      <td id=\"T_d3225_row3_col4\" class=\"data row3 col4\" >None</td>\n",
       "      <td id=\"T_d3225_row3_col5\" class=\"data row3 col5\" >None</td>\n",
       "      <td id=\"T_d3225_row3_col6\" class=\"data row3 col6\" >a</td>\n",
       "      <td id=\"T_d3225_row3_col7\" class=\"data row3 col7\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d3225_row4_col0\" class=\"data row4 col0\" >ASN00030022</td>\n",
       "      <td id=\"T_d3225_row4_col1\" class=\"data row4 col1\" >2010-01-01</td>\n",
       "      <td id=\"T_d3225_row4_col2\" class=\"data row4 col2\" >PRCP</td>\n",
       "      <td id=\"T_d3225_row4_col3\" class=\"data row4 col3\" >408</td>\n",
       "      <td id=\"T_d3225_row4_col4\" class=\"data row4 col4\" >None</td>\n",
       "      <td id=\"T_d3225_row4_col5\" class=\"data row4 col5\" >None</td>\n",
       "      <td id=\"T_d3225_row4_col6\" class=\"data row4 col6\" >a</td>\n",
       "      <td id=\"T_d3225_row4_col7\" class=\"data row4 col7\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d3225_row5_col0\" class=\"data row5 col0\" >ASN00029121</td>\n",
       "      <td id=\"T_d3225_row5_col1\" class=\"data row5 col1\" >2010-01-01</td>\n",
       "      <td id=\"T_d3225_row5_col2\" class=\"data row5 col2\" >PRCP</td>\n",
       "      <td id=\"T_d3225_row5_col3\" class=\"data row5 col3\" >820</td>\n",
       "      <td id=\"T_d3225_row5_col4\" class=\"data row5 col4\" >None</td>\n",
       "      <td id=\"T_d3225_row5_col5\" class=\"data row5 col5\" >None</td>\n",
       "      <td id=\"T_d3225_row5_col6\" class=\"data row5 col6\" >a</td>\n",
       "      <td id=\"T_d3225_row5_col7\" class=\"data row5 col7\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d3225_row6_col0\" class=\"data row6 col0\" >ASN00029126</td>\n",
       "      <td id=\"T_d3225_row6_col1\" class=\"data row6 col1\" >2010-01-01</td>\n",
       "      <td id=\"T_d3225_row6_col2\" class=\"data row6 col2\" >TMAX</td>\n",
       "      <td id=\"T_d3225_row6_col3\" class=\"data row6 col3\" >371</td>\n",
       "      <td id=\"T_d3225_row6_col4\" class=\"data row6 col4\" >None</td>\n",
       "      <td id=\"T_d3225_row6_col5\" class=\"data row6 col5\" >None</td>\n",
       "      <td id=\"T_d3225_row6_col6\" class=\"data row6 col6\" >S</td>\n",
       "      <td id=\"T_d3225_row6_col7\" class=\"data row6 col7\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d3225_row7_col0\" class=\"data row7 col0\" >ASN00029126</td>\n",
       "      <td id=\"T_d3225_row7_col1\" class=\"data row7 col1\" >2010-01-01</td>\n",
       "      <td id=\"T_d3225_row7_col2\" class=\"data row7 col2\" >TMIN</td>\n",
       "      <td id=\"T_d3225_row7_col3\" class=\"data row7 col3\" >225</td>\n",
       "      <td id=\"T_d3225_row7_col4\" class=\"data row7 col4\" >None</td>\n",
       "      <td id=\"T_d3225_row7_col5\" class=\"data row7 col5\" >None</td>\n",
       "      <td id=\"T_d3225_row7_col6\" class=\"data row7 col6\" >S</td>\n",
       "      <td id=\"T_d3225_row7_col7\" class=\"data row7 col7\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d3225_row8_col0\" class=\"data row8 col0\" >ASN00029126</td>\n",
       "      <td id=\"T_d3225_row8_col1\" class=\"data row8 col1\" >2010-01-01</td>\n",
       "      <td id=\"T_d3225_row8_col2\" class=\"data row8 col2\" >PRCP</td>\n",
       "      <td id=\"T_d3225_row8_col3\" class=\"data row8 col3\" >0</td>\n",
       "      <td id=\"T_d3225_row8_col4\" class=\"data row8 col4\" >None</td>\n",
       "      <td id=\"T_d3225_row8_col5\" class=\"data row8 col5\" >None</td>\n",
       "      <td id=\"T_d3225_row8_col6\" class=\"data row8 col6\" >a</td>\n",
       "      <td id=\"T_d3225_row8_col7\" class=\"data row8 col7\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d3225_row9_col0\" class=\"data row9 col0\" >ASN00029126</td>\n",
       "      <td id=\"T_d3225_row9_col1\" class=\"data row9 col1\" >2010-01-01</td>\n",
       "      <td id=\"T_d3225_row9_col2\" class=\"data row9 col2\" >TAVG</td>\n",
       "      <td id=\"T_d3225_row9_col3\" class=\"data row9 col3\" >298</td>\n",
       "      <td id=\"T_d3225_row9_col4\" class=\"data row9 col4\" >H</td>\n",
       "      <td id=\"T_d3225_row9_col5\" class=\"data row9 col5\" >None</td>\n",
       "      <td id=\"T_d3225_row9_col6\" class=\"data row9 col6\" >S</td>\n",
       "      <td id=\"T_d3225_row9_col7\" class=\"data row9 col7\" >None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffff73c029d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[time] cell_time (sec): 3891.62\n",
      "[time] cell_time (min):  64.86\n",
      "[time] notebook_run_time (min): 65.09\n"
     ]
    }
   ],
   "source": [
    "bprint(\"SECTION 2: DATA INGESTION\")\n",
    "# supports: Q1(b). Q1(b) (verbatim): \"How many years are contained in daily, and how does the size of the data change?\"\n",
    "\n",
    "cell_time = time.time()  \n",
    "# — build FULL `daily` from all years (wildcard)\n",
    "# Ensure the schema exists (uses your column names, incl. OBSTIME)\n",
    "if \"daily_schema\" not in globals():\n",
    "    daily_schema = T.StructType([\n",
    "        T.StructField(\"ID\",       T.StringType(), True),\n",
    "        T.StructField(\"DATE\",     T.StringType(), True),  # parsed to DateType below\n",
    "        T.StructField(\"ELEMENT\",  T.StringType(), True),\n",
    "        T.StructField(\"VALUE\",    T.IntegerType(), True),\n",
    "        T.StructField(\"MFLAG\",    T.StringType(), True),\n",
    "        T.StructField(\"QFLAG\",    T.StringType(), True),\n",
    "        T.StructField(\"SFLAG\",    T.StringType(), True),\n",
    "        T.StructField(\"OBSTIME\",  T.StringType(), True),\n",
    "    ])\n",
    "\n",
    "print(\"Reading all years:\", f\"{daily_root}*.csv.gz\")\n",
    "print()\n",
    "\n",
    "_df = spark.read.csv(\n",
    "    f\"{daily_root}*.csv.gz\",\n",
    "    schema=daily_schema,\n",
    "    header=False,            # flip to True if your files have a header row\n",
    "    mode=\"PERMISSIVE\"\n",
    ")\n",
    "\n",
    "# Some dumps use STATION instead of ID\n",
    "if \"STATION\" in _df.columns and \"ID\" not in _df.columns:\n",
    "    _df = _df.withColumnRenamed(\"STATION\", \"ID\")\n",
    "\n",
    "daily_for_overlap = (\n",
    "    _df.withColumn(\n",
    "        \"DATE\",\n",
    "        F.coalesce(F.to_date(\"DATE\", \"yyyy-MM-dd\"),\n",
    "                   F.to_date(\"DATE\", \"yyyyMMdd\"))\n",
    "    )\n",
    "    .withColumn(\"ID\", F.upper(F.trim(F.col(\"ID\"))))\n",
    "    .select(\"ID\", \"DATE\", \"ELEMENT\", \"VALUE\", \"MFLAG\", \"QFLAG\", \"SFLAG\", \"OBSTIME\")\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "# Touch to materialise cache\n",
    "_ = daily_for_overlap.limit(1).count()\n",
    "\n",
    "show_df(daily_for_overlap.limit(10), name=\"daily (full, wildcard)\")\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66d8cade-6a39-48f2-a050-dffecad17f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "______________Process Answer: 1(b)6______________\n",
      "['3659      3659      wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-countries.txt', '35272064  35272064  wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-inventory.txt', '1086      1086      wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-states.txt', '11150502  11150502  wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-stations.txt']\n",
      "\n",
      "Metadata file count: 4\n",
      "Sample parsed rows: [('wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-countries.txt', 3659), ('wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-inventory.txt', 35272064), ('wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-states.txt', 1086), ('wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-stations.txt', 11150502)]\n",
      "\n",
      "[spark] other_files_df schema:\n",
      "root\n",
      " |-- path: string (nullable = false)\n",
      " |-- uncompressed_bytes: long (nullable = false)\n",
      "\n",
      "[spark] sample:\n",
      "+----------------------------------------------------------------------------------+------------------+\n",
      "|path                                                                              |uncompressed_bytes|\n",
      "+----------------------------------------------------------------------------------+------------------+\n",
      "|wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-countries.txt|3659              |\n",
      "|wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-inventory.txt|35272064          |\n",
      "|wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-states.txt   |1086              |\n",
      "|wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-stations.txt |11150502          |\n",
      "+----------------------------------------------------------------------------------+------------------+\n",
      "\n",
      "[time] cell_time (sec):   2.34\n",
      "[time] cell_time (min):   0.04\n",
      "[time] notebook_run_time (min): 65.13\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Process Answer: 1(b)6\")\n",
    "# supports: Q1(b) (robust HDFS parsing variant, \"NEW -ER\", for year/size extraction). Q1(b) (verbatim): \"How many years are contained in daily, and how does the size of the data change?\"\n",
    "\n",
    "\n",
    "cell_time = time.time() \n",
    "rows        = []\n",
    "# NOTE:  -du with a files-only --- size + path are stable (behaves like the GOOD run)\n",
    "lines       = get_ipython().getoutput(f'hdfs dfs -du \"{data_root}/ghcnd-*.txt\"')\n",
    "print(lines)\n",
    "for line in lines:                 # <-- was lines[15:] (skipped everything)\n",
    "    #print()\n",
    "    parts = line.split()\n",
    "    #print(line)\n",
    "    #print(parts)\n",
    "    #print(len(parts))\n",
    "    #print(parts[0])\n",
    "\n",
    "    if len(parts) >= 2:\n",
    "        size = int(parts[0])                 # bytes from `hdfs dfs -du`\n",
    "        path = parts[-1].strip()             # full path\n",
    "        #print(\"size:\",size)\n",
    "        # if not path.startswith(daily_root):   # files-only glob excludes /daily already\n",
    "        rows.append((path, size))             # not compressed\n",
    "\n",
    "print(\"\\nMetadata file count:\", len(rows))\n",
    "print(\"Sample parsed rows:\", rows[:5])\n",
    "# Spark schema\n",
    "schema = StructType([\n",
    "    StructField(\"path\", StringType(), False),\n",
    "    StructField(\"uncompressed_bytes\", LongType(), False),\n",
    "])\n",
    "\n",
    "metadata_files_df = spark.createDataFrame(rows, schema)\n",
    "\n",
    "print(\"\\n[spark] other_files_df schema:\")\n",
    "metadata_files_df.printSchema()\n",
    "print(\"[spark] sample:\")\n",
    "metadata_files_df.show( truncate=False)\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6dba4c5-b87a-4f37-9939-a2c89d5417a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____________________Q1(b)25_____________________\n",
      "\n",
      "[check] marker  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/enriched_write_name.parquet/_SUCCESS\n",
      "[check] rc: 0 -> exists\n",
      "\n",
      "[check] marker  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations.parquet/_SUCCESS\n",
      "[check] rc: 0 -> exists\n",
      "\n",
      "[check] marker  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet/_SUCCESS\n",
      "[check] rc: 0 -> exists\n",
      "\n",
      "[check] marker  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/states.parquet/_SUCCESS\n",
      "[check] rc: 0 -> exists\n",
      "\n",
      "[check] marker  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/countries.parquet/_SUCCESS\n",
      "[check] rc: 0 -> exists\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Q1(b)25\") \n",
    "\n",
    "has_enriched  = has_parquet(enriched_write_name)\n",
    "has_stations  = has_parquet(stations_write_name)\n",
    "has_inventory = has_parquet(inventory_write_name)\n",
    "has_states    = has_parquet(states_write_name)\n",
    "has_countries = has_parquet(countries_write_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b35a6fb6-ac8c-46c5-a30c-cee9f136b2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__________Process Answer: 1(c)2 — dataset sizes (HDFS) + est. uncompressed daily__________\n",
      "daily_root          : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/\n",
      "inventory_read_name : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-inventory.txt\n",
      "stations_read_name  : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-stations.txt\n",
      "countries_read_name : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-countries.txt\n",
      "states_read_name    : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-states.txt\n",
      "[status] gzip_expansion_factor -> 3.3\n",
      "[status] sizes (bytes) -> {'daily (folder)': 13993455698, 'ghcnd-inventory.txt': 35272064, 'ghcnd-stations.txt': 11150502, 'ghcnd-countries.txt': 3659, 'ghcnd-states.txt': 1086}\n",
      "[status] total (bytes)  -> 14039883009\n",
      "+-------------------+--------+\n",
      "|dataset            |size_mb |\n",
      "+-------------------+--------+\n",
      "|daily (folder)     |13345.2 |\n",
      "|ghcnd-inventory.txt|33.64   |\n",
      "|ghcnd-stations.txt |10.63   |\n",
      "|ghcnd-countries.txt|0.0     |\n",
      "|ghcnd-states.txt   |0.0     |\n",
      "|TOTAL              |13389.48|\n",
      "+-------------------+--------+\n",
      "\n",
      "[status] estimated uncompressed daily (MB): 44,039.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/18 00:01:23 WARN AzureFileSystemThreadPoolExecutor: Disabling threads for Delete operation as thread count 0 is <= 1\n",
      "25/09/18 00:01:25 WARN AzureFileSystemThreadPoolExecutor: Disabling threads for Delete operation as thread count 0 is <= 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[time] cell_time (sec):  13.38\n",
      "[time] cell_time (min):   0.22\n",
      "[time] notebook_run_time (min): 65.51\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Process Answer: 1(c)2 — dataset sizes (HDFS) + est. uncompressed daily\")\n",
    "# supports: Q1(c). Q1(c) (verbatim): \"What is the total size of all of the data, and how much of that is daily?\"\n",
    "\n",
    "cell_time = time.time()\n",
    "# \n",
    "print(f\"daily_root          : {daily_root}\")\n",
    "print(f\"inventory_read_name : {inventory_read_name}\")\n",
    "print(f\"stations_read_name  : {stations_read_name}\")\n",
    "print(f\"countries_read_name : {countries_read_name}\")\n",
    "print(f\"states_read_name    : {states_read_name}\")\n",
    "sizes = {\n",
    "    \"daily (folder)\":      du_bytes(daily_root),\n",
    "    \"ghcnd-inventory.txt\": du_bytes(inventory_read_name),\n",
    "    \"ghcnd-stations.txt\":  du_bytes(stations_read_name),\n",
    "    \"ghcnd-countries.txt\": du_bytes(countries_read_name),\n",
    "    \"ghcnd-states.txt\":    du_bytes(states_read_name),\n",
    "}\n",
    "total_bytes = sum(sizes.values())\n",
    "\n",
    "# Simple gzip expansion estimate  \n",
    "gzip_expansion_factor = 3.3\n",
    "est_uncomp_daily = int(sizes[\"daily (folder)\"] * gzip_expansion_factor)\n",
    "\n",
    "print(\"[status] gzip_expansion_factor ->\", gzip_expansion_factor)\n",
    "print(\"[status] sizes (bytes) ->\", sizes)\n",
    "print(\"[status] total (bytes)  ->\", total_bytes)\n",
    "\n",
    "# Present as a small Spark table (sizes in MB for readability)\n",
    "to_mb = 1024**2\n",
    "rows = []\n",
    "for k, v in sizes.items():\n",
    "    rows.append((k, round(v/to_mb, 2)))\n",
    "rows.append((\"TOTAL\", round(total_bytes/to_mb, 2)))\n",
    "\n",
    "sizes_df = spark.createDataFrame(rows, [\"dataset\", \"size_mb\"])\n",
    "sizes_df.show(truncate=False)\n",
    "\n",
    "print(f\"[status] estimated uncompressed daily (MB): {est_uncomp_daily/to_mb:,.2f}\")\n",
    "\n",
    " \n",
    "user_out = f\"wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/processing\"\n",
    "spark.createDataFrame(\n",
    "    [(k, v, round(v/to_mb,2)) for k, v in sizes.items()] + [(\"TOTAL\", total_bytes, round(total_bytes/to_mb, 2))]\n",
    "    , [\"dataset\",\"size_bytes\",\"size_mb\"]\n",
    ").coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(f\"{user_out}/dew59_sizes_mb_csv\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0777255a-e6d1-46bc-8413-83de590650e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____________Process Answer: Q2(c)71_____________\n",
      "stations\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- GSN_FLAG: string (nullable = true)\n",
      " |-- HCNCRN_FLAG: string (nullable = true)\n",
      " |-- WMO_ID: string (nullable = true)\n",
      "\n",
      "+-----------+--------+---------+---------+-----+---------------------+--------+-----------+------+\n",
      "|ID         |LATITUDE|LONGITUDE|ELEVATION|STATE|NAME                 |GSN_FLAG|HCNCRN_FLAG|WMO_ID|\n",
      "+-----------+--------+---------+---------+-----+---------------------+--------+-----------+------+\n",
      "|ACW00011604|17.1167 |-61.7833 |10.1     |     |ST JOHNS COOLIDGE FLD|        |           |      |\n",
      "|ACW00011647|17.1333 |-61.7833 |19.2     |     |ST JOHNS             |        |           |      |\n",
      "|AE000041196|25.333  |55.517   |34.0     |     |SHARJAH INTER. AIRP  |GSN     |           |41196 |\n",
      "|AEM00041194|25.255  |55.364   |10.4     |     |DUBAI INTL           |        |           |41194 |\n",
      "|AEM00041217|24.433  |54.651   |26.8     |     |ABU DHABI INTL       |        |           |41217 |\n",
      "|AEM00041218|24.262  |55.609   |264.9    |     |AL AIN INTL          |        |           |41218 |\n",
      "|AF000040930|35.317  |69.017   |3366.0   |     |NORTH-SALANG         |GSN     |           |40930 |\n",
      "|AFM00040938|34.21   |62.228   |977.2    |     |HERAT                |        |           |40938 |\n",
      "|AFM00040948|34.566  |69.212   |1791.3   |     |KABUL INTL           |        |           |40948 |\n",
      "|AFM00040990|31.5    |65.85    |1010.0   |     |KANDAHAR AIRPORT     |        |           |40990 |\n",
      "+-----------+--------+---------+---------+-----+---------------------+--------+-----------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "[time] notebook_run_time (min): 65.52\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Process Answer: Q2(c)71\")\n",
    "# supports: Q2(c) — [awaiting verbatim text] load fixed-width metadata into Spark and extract columns using substring by character ranges.\n",
    "# does: reads the fixed-width STATIONS text via spark.read.text and extracts ID, LATITUDE, LONGITUDE, ELEVATION, STATE, NAME, GSN_FLAG, HCNCRN_FLAG, WMO_ID with F.substring; prints schema/sample.\n",
    "\n",
    "read_stations = spark.read.text(stations_read_name)\n",
    "\n",
    "stations = (\n",
    "    read_stations.select(\n",
    "        F.trim(F.substring(\"value\",  1, 11)).alias(\"ID\"),                 # 1–11\n",
    "        F.trim(F.substring(\"value\", 13,  8)).cast(\"double\").alias(\"LATITUDE\"),   # 13–20\n",
    "        F.trim(F.substring(\"value\", 22,  9)).cast(\"double\").alias(\"LONGITUDE\"),  # 22–30\n",
    "        F.trim(F.substring(\"value\", 32,  6)).cast(\"double\").alias(\"ELEVATION\"),  # 32–37\n",
    "        F.trim(F.substring(\"value\", 39,  2)).alias(\"STATE\"),                     # 39–40\n",
    "        F.trim(F.substring(\"value\", 42, 30)).alias(\"NAME\"),                      # 42–71\n",
    "        F.trim(F.substring(\"value\", 73,  3)).alias(\"GSN_FLAG\"),                  # 73–75\n",
    "        F.trim(F.substring(\"value\", 77,  3)).alias(\"HCNCRN_FLAG\"),               # 77–79\n",
    "        F.trim(F.substring(\"value\", 81,  5)).alias(\"WMO_ID\")                     # 81–85\n",
    "    )\n",
    ")\n",
    "print(\"stations\")\n",
    "stations.printSchema()\n",
    "stations.show(10, truncate=False)\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb52e415-e5fc-434f-a73b-70d3513db1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "____________SECTION 3: DATA PROCESSING____________\n",
      "+----+--------------------+\n",
      "|CODE|        COUNTRY_NAME|\n",
      "+----+--------------------+\n",
      "|  AC| Antigua and Barbuda|\n",
      "|  AE|United Arab Emirates|\n",
      "|  AF|         Afghanistan|\n",
      "|  AG|             Algeria|\n",
      "|  AJ|          Azerbaijan|\n",
      "|  AL|             Albania|\n",
      "|  AM|             Armenia|\n",
      "|  AO|              Angola|\n",
      "|  AQ|American Samoa [U...|\n",
      "|  AR|           Argentina|\n",
      "|  AS|           Australia|\n",
      "|  AU|             Austria|\n",
      "|  AY|          Antarctica|\n",
      "|  BA|             Bahrain|\n",
      "|  BB|            Barbados|\n",
      "|  BC|            Botswana|\n",
      "|  BD|Bermuda [United K...|\n",
      "|  BE|             Belgium|\n",
      "|  BF|        Bahamas, The|\n",
      "|  BG|          Bangladesh|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+--------+---------+---------+-----+--------------------+--------+-----------+------+------------+\n",
      "|         ID|LATITUDE|LONGITUDE|ELEVATION|STATE|                NAME|GSN_FLAG|HCNCRN_FLAG|WMO_ID|COUNTRY_CODE|\n",
      "+-----------+--------+---------+---------+-----+--------------------+--------+-----------+------+------------+\n",
      "|ACW00011604| 17.1167| -61.7833|     10.1|     |ST JOHNS COOLIDGE...|        |           |      |          AC|\n",
      "|ACW00011647| 17.1333| -61.7833|     19.2|     |            ST JOHNS|        |           |      |          AC|\n",
      "|AE000041196|  25.333|   55.517|     34.0|     | SHARJAH INTER. AIRP|     GSN|           | 41196|          AE|\n",
      "|AEM00041194|  25.255|   55.364|     10.4|     |          DUBAI INTL|        |           | 41194|          AE|\n",
      "|AEM00041217|  24.433|   54.651|     26.8|     |      ABU DHABI INTL|        |           | 41217|          AE|\n",
      "|AEM00041218|  24.262|   55.609|    264.9|     |         AL AIN INTL|        |           | 41218|          AE|\n",
      "|AF000040930|  35.317|   69.017|   3366.0|     |        NORTH-SALANG|     GSN|           | 40930|          AF|\n",
      "|AFM00040938|   34.21|   62.228|    977.2|     |               HERAT|        |           | 40938|          AF|\n",
      "|AFM00040948|  34.566|   69.212|   1791.3|     |          KABUL INTL|        |           | 40948|          AF|\n",
      "|AFM00040990|    31.5|    65.85|   1010.0|     |    KANDAHAR AIRPORT|        |           | 40990|          AF|\n",
      "|AG000060390| 36.7167|     3.25|     24.0|     |  ALGER-DAR EL BEIDA|     GSN|           | 60390|          AG|\n",
      "|AG000060590| 30.5667|   2.8667|    397.0|     |            EL-GOLEA|     GSN|           | 60590|          AG|\n",
      "|AG000060611|   28.05|   9.6331|    561.0|     |           IN-AMENAS|     GSN|           | 60611|          AG|\n",
      "|AG000060680|    22.8|   5.4331|   1362.0|     |         TAMANRASSET|     GSN|           | 60680|          AG|\n",
      "|AGE00135039| 35.7297|     0.65|     50.0|     |ORAN-HOPITAL MILI...|        |           |      |          AG|\n",
      "|AGE00147704|   36.97|     7.79|    161.0|     | ANNABA-CAP DE GARDE|        |           |      |          AG|\n",
      "|AGE00147705|   36.78|     3.07|     59.0|     |ALGIERS-VILLE/UNI...|        |           |      |          AG|\n",
      "|AGE00147706|    36.8|     3.03|    344.0|     |   ALGIERS-BOUZAREAH|        |           |      |          AG|\n",
      "|AGE00147707|    36.8|     3.04|     38.0|     |  ALGIERS-CAP CAXINE|        |           |      |          AG|\n",
      "|AGE00147708|   36.72|     4.05|    222.0|     |          TIZI OUZOU|        |           | 60395|          AG|\n",
      "+-----------+--------+---------+---------+-----+--------------------+--------+-----------+------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+--------+---------+---------+-----+--------------------+--------+-----------+------+------------+--------------------+\n",
      "|         ID|LATITUDE|LONGITUDE|ELEVATION|STATE|                NAME|GSN_FLAG|HCNCRN_FLAG|WMO_ID|COUNTRY_CODE|        COUNTRY_NAME|\n",
      "+-----------+--------+---------+---------+-----+--------------------+--------+-----------+------+------------+--------------------+\n",
      "|ACW00011604| 17.1167| -61.7833|     10.1|     |ST JOHNS COOLIDGE...|        |           |      |          AC| Antigua and Barbuda|\n",
      "|ACW00011647| 17.1333| -61.7833|     19.2|     |            ST JOHNS|        |           |      |          AC| Antigua and Barbuda|\n",
      "|AE000041196|  25.333|   55.517|     34.0|     | SHARJAH INTER. AIRP|     GSN|           | 41196|          AE|United Arab Emirates|\n",
      "|AEM00041194|  25.255|   55.364|     10.4|     |          DUBAI INTL|        |           | 41194|          AE|United Arab Emirates|\n",
      "|AEM00041217|  24.433|   54.651|     26.8|     |      ABU DHABI INTL|        |           | 41217|          AE|United Arab Emirates|\n",
      "|AEM00041218|  24.262|   55.609|    264.9|     |         AL AIN INTL|        |           | 41218|          AE|United Arab Emirates|\n",
      "|AF000040930|  35.317|   69.017|   3366.0|     |        NORTH-SALANG|     GSN|           | 40930|          AF|         Afghanistan|\n",
      "|AFM00040938|   34.21|   62.228|    977.2|     |               HERAT|        |           | 40938|          AF|         Afghanistan|\n",
      "|AFM00040948|  34.566|   69.212|   1791.3|     |          KABUL INTL|        |           | 40948|          AF|         Afghanistan|\n",
      "|AFM00040990|    31.5|    65.85|   1010.0|     |    KANDAHAR AIRPORT|        |           | 40990|          AF|         Afghanistan|\n",
      "|AG000060390| 36.7167|     3.25|     24.0|     |  ALGER-DAR EL BEIDA|     GSN|           | 60390|          AG|             Algeria|\n",
      "|AG000060590| 30.5667|   2.8667|    397.0|     |            EL-GOLEA|     GSN|           | 60590|          AG|             Algeria|\n",
      "|AG000060611|   28.05|   9.6331|    561.0|     |           IN-AMENAS|     GSN|           | 60611|          AG|             Algeria|\n",
      "|AG000060680|    22.8|   5.4331|   1362.0|     |         TAMANRASSET|     GSN|           | 60680|          AG|             Algeria|\n",
      "|AGE00135039| 35.7297|     0.65|     50.0|     |ORAN-HOPITAL MILI...|        |           |      |          AG|             Algeria|\n",
      "|AGE00147704|   36.97|     7.79|    161.0|     | ANNABA-CAP DE GARDE|        |           |      |          AG|             Algeria|\n",
      "|AGE00147705|   36.78|     3.07|     59.0|     |ALGIERS-VILLE/UNI...|        |           |      |          AG|             Algeria|\n",
      "|AGE00147706|    36.8|     3.03|    344.0|     |   ALGIERS-BOUZAREAH|        |           |      |          AG|             Algeria|\n",
      "|AGE00147707|    36.8|     3.04|     38.0|     |  ALGIERS-CAP CAXINE|        |           |      |          AG|             Algeria|\n",
      "|AGE00147708|   36.72|     4.05|    222.0|     |          TIZI OUZOU|        |           | 60395|          AG|             Algeria|\n",
      "+-----------+--------+---------+---------+-----+--------------------+--------+-----------+------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+------------------------+------------+--------------------+\n",
      "|ID         |NAME                    |COUNTRY_CODE|COUNTRY_NAME        |\n",
      "+-----------+------------------------+------------+--------------------+\n",
      "|ACW00011604|ST JOHNS COOLIDGE FLD   |AC          |Antigua and Barbuda |\n",
      "|ACW00011647|ST JOHNS                |AC          |Antigua and Barbuda |\n",
      "|AE000041196|SHARJAH INTER. AIRP     |AE          |United Arab Emirates|\n",
      "|AEM00041194|DUBAI INTL              |AE          |United Arab Emirates|\n",
      "|AEM00041217|ABU DHABI INTL          |AE          |United Arab Emirates|\n",
      "|AEM00041218|AL AIN INTL             |AE          |United Arab Emirates|\n",
      "|AF000040930|NORTH-SALANG            |AF          |Afghanistan         |\n",
      "|AFM00040938|HERAT                   |AF          |Afghanistan         |\n",
      "|AFM00040948|KABUL INTL              |AF          |Afghanistan         |\n",
      "|AFM00040990|KANDAHAR AIRPORT        |AF          |Afghanistan         |\n",
      "|AG000060390|ALGER-DAR EL BEIDA      |AG          |Algeria             |\n",
      "|AG000060590|EL-GOLEA                |AG          |Algeria             |\n",
      "|AG000060611|IN-AMENAS               |AG          |Algeria             |\n",
      "|AG000060680|TAMANRASSET             |AG          |Algeria             |\n",
      "|AGE00135039|ORAN-HOPITAL MILITAIRE  |AG          |Algeria             |\n",
      "|AGE00147704|ANNABA-CAP DE GARDE     |AG          |Algeria             |\n",
      "|AGE00147705|ALGIERS-VILLE/UNIVERSITE|AG          |Algeria             |\n",
      "|AGE00147706|ALGIERS-BOUZAREAH       |AG          |Algeria             |\n",
      "|AGE00147707|ALGIERS-CAP CAXINE      |AG          |Algeria             |\n",
      "|AGE00147708|TIZI OUZOU              |AG          |Algeria             |\n",
      "+-----------+------------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[time]   Cell time (sec)   : 1758110474.19\n",
      "[time]   Cell time (min)   : 29301841.24\n",
      "[time] notebook_run_time (min): 65.54\n"
     ]
    }
   ],
   "source": [
    "bprint(\"SECTION 3: DATA PROCESSING\")\n",
    "# supports: Q3(a–c) — \"Derive country_code and join stations with countries/states.\"\n",
    "# does: parses COUNTRIES fixed-width into [CODE, COUNTRY_NAME]; derives COUNTRY_CODE from station IDs; left-joins stations↔countries to create stn_countries and previews results for verification.\n",
    "\n",
    "# countries\n",
    "read_countries = spark.read.text(countries_read_name)\n",
    "countries = (\n",
    "    read_countries.select(\n",
    "        F.substring(\"value\", 1, 2).alias(\"CODE\"),                # 1–2\n",
    "        F.trim(F.substring(\"value\", 4, 61)).alias(\"COUNTRY_NAME\")# 4–64\n",
    "    )\n",
    ")\n",
    "countries.show()\n",
    "# derive country code \n",
    "stations_cc = stations.withColumn(\"COUNTRY_CODE\", F.substring(\"ID\", 1, 2))\n",
    "# join country code \n",
    "stn_countries = (\n",
    "    stations_cc\n",
    "    .join(countries, stations_cc.COUNTRY_CODE == countries.CODE, \"left\")\n",
    "    .drop(countries.CODE)   # keep COUNTRY_CODE from stations, drop duplicate\n",
    ")\n",
    "stations_cc.show()\n",
    "stn_countries.show()\n",
    "stn_countries.select(\"ID\",\"NAME\",\"COUNTRY_CODE\",\"COUNTRY_NAME\").show(20, False)\n",
    "\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "759ccb34-1c8a-4478-b0ea-14154463ee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "____________Process Answer: Q3(a–c)62____________\n",
      "root\n",
      " |-- CODE: string (nullable = true)\n",
      " |-- STATE_NAME: string (nullable = true)\n",
      "\n",
      "+----+--------------------+\n",
      "|CODE|STATE_NAME          |\n",
      "+----+--------------------+\n",
      "|AB  |ALBERTA             |\n",
      "|AK  |ALASKA              |\n",
      "|AL  |ALABAMA             |\n",
      "|AR  |ARKANSAS            |\n",
      "|AS  |AMERICAN SAMOA      |\n",
      "|AZ  |ARIZONA             |\n",
      "|BC  |BRITISH COLUMBIA    |\n",
      "|CA  |CALIFORNIA          |\n",
      "|CO  |COLORADO            |\n",
      "|CT  |CONNECTICUT         |\n",
      "|DC  |DISTRICT OF COLUMBIA|\n",
      "|DE  |DELAWARE            |\n",
      "|FL  |FLORIDA             |\n",
      "|FM  |MICRONESIA          |\n",
      "|GA  |GEORGIA             |\n",
      "|GU  |GUAM                |\n",
      "|HI  |HAWAII              |\n",
      "|IA  |IOWA                |\n",
      "|ID  |IDAHO               |\n",
      "|IL  |ILLINOIS            |\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[time]   Cell time (sec)   :  0.16\n",
      "[time]   Cell time (min)   :  0.00\n",
      "[time] notebook_run_time (min): 65.54\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Process Answer: Q3(a–c)62\")\n",
    "# supports: Q3(a–c) — \"Derive country_code and join stations with countries/states.\"\n",
    "# does: reads STATES fixed-width from states_read_name, extracts CODE and STATE_NAME to build the states DataFrame, then prints schema and a sample; preparation for the stations↔states join.\n",
    "\n",
    "# states\n",
    "cell_time = time.time()  \n",
    "read_states = spark.read.text(states_read_name)\n",
    "\n",
    "states = (\n",
    "    read_states.select(\n",
    "        F.substring(\"value\", 1, 2).alias(\"CODE\"),                 # 1–2\n",
    "        F.trim(F.substring(\"value\", 4, 47)).alias(\"STATE_NAME\")   # 4–50  (length = 47)\n",
    "    )\n",
    ")\n",
    "\n",
    "states.printSchema()\n",
    "states.show(20, truncate=False)\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e313a575-ad27-4a3b-bc91-9d2cf0c58126",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51a0e93a-329a-43be-b71c-ad795a38ab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____________Process Answer: Q3(d)60_____________\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEMENT: string (nullable = true)\n",
      " |-- FIRSTYEAR: integer (nullable = true)\n",
      " |-- LASTYEAR: integer (nullable = true)\n",
      "\n",
      "+-----------+--------+---------+-------+---------+--------+\n",
      "|ID         |LATITUDE|LONGITUDE|ELEMENT|FIRSTYEAR|LASTYEAR|\n",
      "+-----------+--------+---------+-------+---------+--------+\n",
      "|ACW00011604|17.1167 |-61.7833 |TMAX   |1949     |1949    |\n",
      "|ACW00011604|17.1167 |-61.7833 |TMIN   |1949     |1949    |\n",
      "|ACW00011604|17.1167 |-61.7833 |PRCP   |1949     |1949    |\n",
      "|ACW00011604|17.1167 |-61.7833 |SNOW   |1949     |1949    |\n",
      "|ACW00011604|17.1167 |-61.7833 |SNWD   |1949     |1949    |\n",
      "|ACW00011604|17.1167 |-61.7833 |PGTM   |1949     |1949    |\n",
      "|ACW00011604|17.1167 |-61.7833 |WDFG   |1949     |1949    |\n",
      "|ACW00011604|17.1167 |-61.7833 |WSFG   |1949     |1949    |\n",
      "|ACW00011604|17.1167 |-61.7833 |WT03   |1949     |1949    |\n",
      "|ACW00011604|17.1167 |-61.7833 |WT08   |1949     |1949    |\n",
      "|ACW00011604|17.1167 |-61.7833 |WT16   |1949     |1949    |\n",
      "|ACW00011647|17.1333 |-61.7833 |TMAX   |1961     |1961    |\n",
      "|ACW00011647|17.1333 |-61.7833 |TMIN   |1961     |1961    |\n",
      "|ACW00011647|17.1333 |-61.7833 |PRCP   |1957     |1970    |\n",
      "|ACW00011647|17.1333 |-61.7833 |SNOW   |1957     |1970    |\n",
      "|ACW00011647|17.1333 |-61.7833 |SNWD   |1957     |1970    |\n",
      "|ACW00011647|17.1333 |-61.7833 |WT03   |1961     |1961    |\n",
      "|ACW00011647|17.1333 |-61.7833 |WT16   |1961     |1966    |\n",
      "|AE000041196|25.333  |55.517   |TMAX   |1944     |2025    |\n",
      "|AE000041196|25.333  |55.517   |TMIN   |1944     |2025    |\n",
      "+-----------+--------+---------+-------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[time]   Cell time (sec)   :  0.31\n",
      "[time]   Cell time (min)   :  0.01\n",
      "[time] notebook_run_time (min): 65.55\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Process Answer: Q3(d)60\")  # indirectly\n",
    "# supports: Q3(d) — aggregate inventory per station (FIRSTYEAR/LASTYEAR, element counts).\n",
    "# does: parses the fixed-width inventory file (inventory_read_name) into columns [ID, LATITUDE, LONGITUDE, ELEMENT, FIRSTYEAR, LASTYEAR], then prints schema and a sample to verify ingestion before aggregation.\n",
    "\n",
    "\n",
    "cell_time = time.time()   \n",
    "read_inventory = spark.read.text(inventory_read_name)\n",
    "\n",
    "inventory = (\n",
    "    read_inventory.select(\n",
    "        F.substring(\"value\",  1, 11).alias(\"ID\"),                  # 1–11\n",
    "        F.trim(F.substring(\"value\", 13,  8)).cast(\"double\").alias(\"LATITUDE\"),   # 13–20\n",
    "        F.trim(F.substring(\"value\", 22,  9)).cast(\"double\").alias(\"LONGITUDE\"),  # 22–30\n",
    "        F.substring(\"value\", 32,  4).alias(\"ELEMENT\"),             # 32–35\n",
    "        F.substring(\"value\", 37,  4).cast(\"int\").alias(\"FIRSTYEAR\"),# 37–40\n",
    "        F.substring(\"value\", 42,  4).cast(\"int\").alias(\"LASTYEAR\")  # 42–45\n",
    "    )\n",
    ")\n",
    "\n",
    "inventory.printSchema()\n",
    "inventory.show(20, truncate=False)\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3671b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "____________Process Answer: Q3(a–c)49 ____________\n",
      "[time] cell_time (sec):   0.00\n",
      "[time] cell_time (min):   0.00\n",
      "[time] notebook_run_time (min): 65.55\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Process Answer: Q3(a–c)49 \") # indirectly\n",
    "# supports: Q3(a–c) — derive country_code and join stations with countries/states.\n",
    "# does: persists the countries table to Parquet (if missing) at countries_write_name, so it’s available for the upcoming stations↔countries/states joins; prints the write path and timing.\n",
    "\n",
    "cell_time = time.time()\n",
    "if(not has_countries):\n",
    "    write_parquet(countries,countries_write_name)\n",
    "    bprint()\n",
    "    print(\"[written] \",countries_write_name)\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f32e9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "____________Process Answer: Q3(a–c)50____________\n",
      "[time] cell_time (sec):   0.00\n",
      "[time] cell_time (min):   0.00\n",
      "[time] notebook_run_time (min): 65.55\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Process Answer: Q3(a–c)50\")\n",
    "# supports: Q3(a–c) — derive country_code and join stations with countries/states.\n",
    "# does: writes the STATES table to Parquet at states_write_name when missing (has_states is False) so it’s ready for the upcoming stations↔states joins; prints the write path and timing.\n",
    "\n",
    "cell_time = time.time()\n",
    "if(not has_states):\n",
    "    write_parquet(states,states_write_name)\n",
    "    bprint()\n",
    "    print(\"[written] \",states_write_name)\n",
    "    \n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fc66bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____________Process Answer: Q3(d)51_____________\n",
      "[time] cell_time (sec):   0.00\n",
      "[time] cell_time (min):   0.00\n",
      "[time] notebook_run_time (min): 65.55\n"
     ]
    }
   ],
   "source": [
    " bprint(\"Process Answer: Q3(d)51\")\n",
    "# supports: Q3(d) — aggregate inventory per station (first/last year, element counts) as input to enrichment.\n",
    "# does: writes the INVENTORY table to Parquet at inventory_write_name when missing (has_inventory is False), ensuring inventory is available for the Q3(d) aggregations and later joins.\n",
    "\n",
    "cell_time = time.time() \n",
    "\n",
    "if(not has_inventory):\n",
    "    write_parquet(inventory,inventory_write_name)\n",
    "    bprint()\n",
    "    print(\"[written] \",inventory_write_name)    \n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b722f04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "____________Process Answer: Q3(a–c)52____________\n",
      "[time] cell_time (sec):   0.00\n",
      "[time] cell_time (min):   0.00\n",
      "[time] notebook_run_time (min): 65.55\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Process Answer: Q3(a–c)52\")\n",
    "# supports: Q3(a–c) — \"Derive country_code and join stations with countries/states.\"\n",
    "# does: writes the STATIONS table to Parquet at stations_write_name when missing (has_stations is False), ensuring the base stations dataset is persisted for the upcoming joins.\n",
    "\n",
    "cell_time = time.time()\n",
    "if(not has_stations):\n",
    "    write_parquet(stations,stations_write_name)\n",
    "    bprint()\n",
    "    print(\"[written] \",stations_write_name)    \n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "344e1aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________SECTION 4: INVENTORY AGGREGATION_________\n",
      "\n",
      "\n",
      "__________________________________________________\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- FIRSTYEAR: integer (nullable = true)\n",
      " |-- LASTYEAR: integer (nullable = true)\n",
      " |-- ELEMENT_COUNT: long (nullable = false)\n",
      " |-- CORE_ELEMENT_COUNT: long (nullable = false)\n",
      " |-- OTHER_ELEMENT_COUNT: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------+-------------+------------------+-------------------+\n",
      "|ID         |FIRSTYEAR|LASTYEAR|ELEMENT_COUNT|CORE_ELEMENT_COUNT|OTHER_ELEMENT_COUNT|\n",
      "+-----------+---------+--------+-------------+------------------+-------------------+\n",
      "|USW00013880|1937     |2025    |70           |5                 |65                 |\n",
      "|USW00014607|1939     |2025    |70           |5                 |65                 |\n",
      "|USW00023066|1900     |2025    |67           |5                 |62                 |\n",
      "|USW00013958|1938     |2025    |66           |5                 |61                 |\n",
      "|USW00024121|1888     |2025    |65           |5                 |60                 |\n",
      "|USW00093058|1942     |2025    |65           |5                 |60                 |\n",
      "|USW00093817|1948     |2025    |65           |5                 |60                 |\n",
      "|USW00014944|1932     |2025    |64           |5                 |59                 |\n",
      "|USW00024127|1941     |2025    |63           |5                 |58                 |\n",
      "|USW00024156|1939     |2025    |63           |5                 |58                 |\n",
      "|USW00024157|1889     |2025    |63           |5                 |58                 |\n",
      "|USW00013722|1944     |2025    |61           |5                 |56                 |\n",
      "|USW00014914|1891     |2025    |61           |5                 |56                 |\n",
      "|USW00025309|1936     |2025    |61           |5                 |56                 |\n",
      "|USW00026510|1939     |2025    |61           |5                 |56                 |\n",
      "|USW00093822|1901     |2025    |61           |5                 |56                 |\n",
      "|USW00094849|1916     |2025    |61           |5                 |56                 |\n",
      "|USW00094908|1951     |2025    |61           |5                 |56                 |\n",
      "|USW00003813|1941     |2025    |60           |5                 |55                 |\n",
      "|USW00003822|1948     |2025    |60           |5                 |55                 |\n",
      "+-----------+---------+--------+-------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[result] Aggregated inventory rows :      129,618\n",
      "[time]   Cell time (sec)   :  3.66\n",
      "[time]   Cell time (min)   :  0.06\n",
      "[time] notebook_run_time (min): 65.61\n"
     ]
    }
   ],
   "source": [
    "bprint(\"SECTION 4: INVENTORY AGGREGATION\")\n",
    "# supports: Q2(d) — row counts, previews, and schemas for metadata + daily; write a small counts artifact.\n",
    "# does: counts rows for stations/states/countries/inventory/daily_for_overlap, shows a table, writes the counts CSV to your user area, then previews 3 rows and prints schemas for each dataset.\n",
    "\n",
    "cell_time = time.time() \n",
    "\n",
    "core_elements = [\"PRCP\", \"SNOW\", \"SNWD\", \"TMAX\", \"TMIN\"]\n",
    "\n",
    "inv_agg = (inventory\n",
    "           .groupBy(\"ID\")\n",
    "           .agg(\n",
    "               F.min(\"FIRSTYEAR\").alias(\"FIRSTYEAR\"),\n",
    "               F.max(\"LASTYEAR\").alias(\"LASTYEAR\"),\n",
    "               F.countDistinct(\"ELEMENT\").alias(\"ELEMENT_COUNT\"),\n",
    "               F.countDistinct(\n",
    "                   F.when(F.col(\"ELEMENT\").isin(core_elements), F.col(\"ELEMENT\"))\n",
    "               ).alias(\"CORE_ELEMENT_COUNT\"),\n",
    "               F.countDistinct(\n",
    "                   F.when(~F.col(\"ELEMENT\").isin(core_elements), F.col(\"ELEMENT\"))\n",
    "              ).alias(\"OTHER_ELEMENT_COUNT\")\n",
    "                   ).orderBy(F.col(\"CORE_ELEMENT_COUNT\").desc(),\n",
    "                        F.col(\"ELEMENT_COUNT\").desc(),\n",
    "                        F.col(\"ID\").asc())\n",
    "                        )\n",
    "print()\n",
    "bprint()\n",
    "inv_agg.printSchema()\n",
    "inv_agg.show(20, truncate=False)\n",
    "\n",
    "print(f\"[result] Aggregated inventory rows : {inv_agg.count():12,d}\")\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a56f3eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____________Process Answer: Q3(e)56_____________\n",
      "\n",
      "\n",
      "__________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[result] Enriched stations rows :      129,657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:=============>    (6 + 2) / 8][Stage 63:======>           (1 + 2) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+--------------------+----------+---------+--------+-------------+------------------+-------------------+\n",
      "|ID         |NAME                    |COUNTRY_NAME        |STATE_NAME|FIRSTYEAR|LASTYEAR|ELEMENT_COUNT|CORE_ELEMENT_COUNT|OTHER_ELEMENT_COUNT|\n",
      "+-----------+------------------------+--------------------+----------+---------+--------+-------------+------------------+-------------------+\n",
      "|ACW00011604|ST JOHNS COOLIDGE FLD   |Antigua and Barbuda |NULL      |1949     |1949    |11           |5                 |6                  |\n",
      "|ACW00011647|ST JOHNS                |Antigua and Barbuda |NULL      |1957     |1970    |7            |5                 |2                  |\n",
      "|AE000041196|SHARJAH INTER. AIRP     |United Arab Emirates|NULL      |1944     |2025    |4            |3                 |1                  |\n",
      "|AEM00041194|DUBAI INTL              |United Arab Emirates|NULL      |1983     |2025    |4            |3                 |1                  |\n",
      "|AEM00041217|ABU DHABI INTL          |United Arab Emirates|NULL      |1983     |2025    |4            |3                 |1                  |\n",
      "|AEM00041218|AL AIN INTL             |United Arab Emirates|NULL      |1994     |2025    |4            |3                 |1                  |\n",
      "|AF000040930|NORTH-SALANG            |Afghanistan         |NULL      |1973     |1992    |5            |4                 |1                  |\n",
      "|AFM00040938|HERAT                   |Afghanistan         |NULL      |1973     |2021    |5            |4                 |1                  |\n",
      "|AFM00040948|KABUL INTL              |Afghanistan         |NULL      |1966     |2021    |5            |4                 |1                  |\n",
      "|AFM00040990|KANDAHAR AIRPORT        |Afghanistan         |NULL      |1973     |2020    |5            |4                 |1                  |\n",
      "|AG000060390|ALGER-DAR EL BEIDA      |Algeria             |NULL      |1940     |2025    |5            |4                 |1                  |\n",
      "|AG000060590|EL-GOLEA                |Algeria             |NULL      |1892     |2025    |4            |3                 |1                  |\n",
      "|AG000060611|IN-AMENAS               |Algeria             |NULL      |1958     |2025    |5            |4                 |1                  |\n",
      "|AG000060680|TAMANRASSET             |Algeria             |NULL      |1940     |2004    |4            |3                 |1                  |\n",
      "|AGE00135039|ORAN-HOPITAL MILITAIRE  |Algeria             |NULL      |1852     |1966    |3            |3                 |0                  |\n",
      "|AGE00147704|ANNABA-CAP DE GARDE     |Algeria             |NULL      |1909     |1937    |3            |3                 |0                  |\n",
      "|AGE00147705|ALGIERS-VILLE/UNIVERSITE|Algeria             |NULL      |1877     |1938    |3            |3                 |0                  |\n",
      "|AGE00147706|ALGIERS-BOUZAREAH       |Algeria             |NULL      |1893     |1920    |3            |3                 |0                  |\n",
      "|AGE00147707|ALGIERS-CAP CAXINE      |Algeria             |NULL      |1878     |1879    |3            |3                 |0                  |\n",
      "|AGE00147708|TIZI OUZOU              |Algeria             |NULL      |1879     |2025    |5            |4                 |1                  |\n",
      "+-----------+------------------------+--------------------+----------+---------+--------+-------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[time]   Cell time (sec)   :  3.85\n",
      "[time]   Cell time (min)   :  0.06\n",
      "[time] notebook_run_time (min): 65.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "bprint(\"Process Answer: Q3(e)56\")\n",
    "# supports: Q3(e) — \"Join aggregated inventory into stations to produce an enriched stations table.\"\n",
    "# does: builds `enriched` by joining station+country (`stn_countries`) with `states` (on STATE=CODE, left) and `inv_agg` (on ID, left); orders rows, prints row count and a preview of key columns, and records timing.\n",
    "\n",
    "cell_time = time.time()\n",
    "enriched = (stn_countries   # already has station + country info\n",
    "            .join(states, stn_countries.STATE == states.CODE, \"left\")\n",
    "            .join(inv_agg, on=\"ID\", how=\"left\")\n",
    "           # ---- order the result (adjust) ----\n",
    "             .orderBy(F.col(\"ID\").asc(), F.col(\"LASTYEAR\").asc(), F.col(\"ELEMENT_COUNT\").asc())\n",
    ")\n",
    "\n",
    "print()\n",
    "bprint()\n",
    "print(f\"[result] Enriched stations rows : {enriched.count():12,d}\")\n",
    "enriched.select(\n",
    "                \"ID\"           ,\"NAME\"    ,\"COUNTRY_NAME\" ,\"STATE_NAME\",\n",
    "                \"FIRSTYEAR\"    ,\"LASTYEAR\",\"ELEMENT_COUNT\",\"CORE_ELEMENT_COUNT\" ,\"OTHER_ELEMENT_COUNT\"\n",
    "               ).show(20, truncate=False)\n",
    "\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6007416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____________Process Answer: Q3(e)53_____________\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Process Answer: Q3(e)53\")\n",
    "# supports: Q3(e) — join inventory aggregates into stations to produce an enriched stations table.\n",
    "# does: conditionally writes the enriched stations DataFrame to Parquet (if not already present), logging the output path and timing.\n",
    "\n",
    "#build parquet files conditionally \n",
    "cell_time = time.time()\n",
    "if(not has_enriched):\n",
    "    write_parquet(enriched,enriched_write_name)\n",
    "    bprint()\n",
    "    print(\"[written] \",enriched_write_name)\n",
    "    cell_time = time.time()\n",
    "    print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e85b719-2210-442a-b1a6-e5a448208983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____________Process Answer: Q3(d)57_____________\n",
      "\n",
      "__________________________________________________\n",
      "Row counts (with inventory):\n",
      "[result] stations      :      129,657\n",
      "[result] countries     :          219\n",
      "[result] states        :           74\n",
      "[result] inventory     :      766,784\n",
      "[check ] stations_cc   :      766,784\n",
      "[check ] stn_countries :      766,784\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Process Answer: Q3(d)57\")\n",
    "# supports: Q3(d) — \"Aggregate inventory per station (FIRSTYEAR, LASTYEAR, element counts) and summarise coverage.\"\n",
    "# does: prints row counts for stations, countries, states, and inventory (plus check lines), serving as a sanity/baseline count before inventory-by-station aggregations and later joins.\n",
    "\n",
    "\n",
    "bprint()\n",
    "print(\"Row counts (with inventory):\")\n",
    "print(f\"[result] stations      : {stations.count() :12,d}\")\n",
    "print(f\"[result] countries     : {countries.count():12,d}\")\n",
    "print(f\"[result] states        : {states.count()   :12,d}\")\n",
    "print(f\"[result] inventory     : {inventory.count():12,d}\")\n",
    "print(f\"[check ] stations_cc   : {inventory.count():12,d}\")\n",
    "print(f\"[check ] stn_countries : {inventory.count():12,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea2318c1-9ef3-4db9-a5cb-7d028114864e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "________________Process A X (11) ________________\n",
      "[time] current time           :  2025.09.18 00:01\n",
      "[time] Cell time (sec)        :   1.56\n",
      "[time] Cell time (min)        :   0.03\n",
      "[time] notebook_run_time (min):  65.70\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Process A X (11) \")\n",
    "val = spark.range(1).select(F.date_format(F.current_timestamp(), 'yyyy.MM.dd HH:mm').alias('t')).first()['t']\n",
    "cell_time = time.time() - cell_time  \n",
    "print(f\"[time] current time           :  {val}\")\n",
    "print(f\"[time] Cell time (sec)        : {cell_time:6.2f}\") \n",
    "#stop_spark()\n",
    "print(f\"[time] Cell time (min)        : {cell_time/60:6.2f}\") \n",
    "print(f\"[time] notebook_run_time (min):  {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4ef081b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____________Process Answer: Q4(b)58_____________\n",
      "\n",
      "=== BUILD NOTEBOOK DIAGNOSTICS DIAGNOSTICS ===\n",
      "Starting universe probe analysis...\n",
      "[Build notebook diagnostics] Starting optimized universe probe...\n",
      "[Build notebook diagnostics] Caching ID DataFrames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Build notebook diagnostics] Cached: daily=129619, station=129657, inv=129618\n",
      "[Build notebook diagnostics] Computing set differences...\n",
      "[Build notebook diagnostics] Daily - Station: 1603\n",
      "[Build notebook diagnostics] Station - Daily: 1641\n",
      "[Build notebook diagnostics] Station - Inventory: 39\n",
      "[Build notebook diagnostics] Inventory - Daily: 1603\n",
      "\n",
      "[Build notebook diagnostics] Additional diagnostics:\n",
      "[INFO] normalise_ids() on column: ID\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- ELEMENT: string (nullable = true)\n",
      " |-- VALUE: integer (nullable = true)\n",
      " |-- MFLAG: string (nullable = true)\n",
      " |-- QFLAG: string (nullable = true)\n",
      " |-- SFLAG: string (nullable = true)\n",
      " |-- OBSTIME: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+-----+-----+-----+-----+-------+\n",
      "|         ID|      DATE|ELEMENT|VALUE|MFLAG|QFLAG|SFLAG|OBSTIME|\n",
      "+-----------+----------+-------+-----+-----+-----+-----+-------+\n",
      "|ASN00030019|2010-01-01|   PRCP|   24| NULL| NULL|    a|   NULL|\n",
      "|ASN00030021|2010-01-01|   PRCP|  200| NULL| NULL|    a|   NULL|\n",
      "|ASN00030022|2010-01-01|   TMAX|  294| NULL| NULL|    a|   NULL|\n",
      "|ASN00030022|2010-01-01|   TMIN|  215| NULL| NULL|    a|   NULL|\n",
      "|ASN00030022|2010-01-01|   PRCP|  408| NULL| NULL|    a|   NULL|\n",
      "|ASN00029121|2010-01-01|   PRCP|  820| NULL| NULL|    a|   NULL|\n",
      "|ASN00029126|2010-01-01|   TMAX|  371| NULL| NULL|    S|   NULL|\n",
      "|ASN00029126|2010-01-01|   TMIN|  225| NULL| NULL|    S|   NULL|\n",
      "|ASN00029126|2010-01-01|   PRCP|    0| NULL| NULL|    a|   NULL|\n",
      "|ASN00029126|2010-01-01|   TAVG|  298|    H| NULL|    S|   NULL|\n",
      "|ASN00029127|2010-01-01|   TMAX|  371| NULL| NULL|    a|   NULL|\n",
      "|ASN00029127|2010-01-01|   TMIN|  225| NULL| NULL|    a|   NULL|\n",
      "|ASN00029127|2010-01-01|   PRCP|    8| NULL| NULL|    a|   NULL|\n",
      "|ASN00029129|2010-01-01|   PRCP|  174| NULL| NULL|    a|   NULL|\n",
      "|ASN00029130|2010-01-01|   PRCP|   86| NULL| NULL|    a|   NULL|\n",
      "|ASN00029131|2010-01-01|   PRCP|   56| NULL| NULL|    a|   NULL|\n",
      "|ASN00029132|2010-01-01|   PRCP|  800| NULL| NULL|    a|   NULL|\n",
      "|ASN00029136|2010-01-01|   PRCP|   22| NULL| NULL|    a|   NULL|\n",
      "|ASN00029137|2010-01-01|   PRCP|    0| NULL| NULL|    a|   NULL|\n",
      "|ASN00029139|2010-01-01|   TMAX|  298| NULL| NULL|    a|   NULL|\n",
      "+-----------+----------+-------+-----+-----+-----+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/18 00:03:40 WARN TaskSetManager: Lost task 57.0 in stage 172.0 (TID 1478) (10.244.20.100 executor 3): org.apache.spark.memory.SparkOutOfMemoryError: No enough memory for aggregation\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] normalise_ids() on column: ID\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- GSN_FLAG: string (nullable = true)\n",
      " |-- HCNCRN_FLAG: string (nullable = true)\n",
      " |-- WMO_ID: string (nullable = true)\n",
      "\n",
      "+-----------+--------+---------+---------+-----+--------------------+--------+-----------+------+\n",
      "|         ID|LATITUDE|LONGITUDE|ELEVATION|STATE|                NAME|GSN_FLAG|HCNCRN_FLAG|WMO_ID|\n",
      "+-----------+--------+---------+---------+-----+--------------------+--------+-----------+------+\n",
      "|ACW00011604| 17.1167| -61.7833|     10.1|     |ST JOHNS COOLIDGE...|        |           |      |\n",
      "|ACW00011647| 17.1333| -61.7833|     19.2|     |            ST JOHNS|        |           |      |\n",
      "|AE000041196|  25.333|   55.517|     34.0|     | SHARJAH INTER. AIRP|     GSN|           | 41196|\n",
      "|AEM00041194|  25.255|   55.364|     10.4|     |          DUBAI INTL|        |           | 41194|\n",
      "|AEM00041217|  24.433|   54.651|     26.8|     |      ABU DHABI INTL|        |           | 41217|\n",
      "|AEM00041218|  24.262|   55.609|    264.9|     |         AL AIN INTL|        |           | 41218|\n",
      "|AF000040930|  35.317|   69.017|   3366.0|     |        NORTH-SALANG|     GSN|           | 40930|\n",
      "|AFM00040938|   34.21|   62.228|    977.2|     |               HERAT|        |           | 40938|\n",
      "|AFM00040948|  34.566|   69.212|   1791.3|     |          KABUL INTL|        |           | 40948|\n",
      "|AFM00040990|    31.5|    65.85|   1010.0|     |    KANDAHAR AIRPORT|        |           | 40990|\n",
      "|AG000060390| 36.7167|     3.25|     24.0|     |  ALGER-DAR EL BEIDA|     GSN|           | 60390|\n",
      "|AG000060590| 30.5667|   2.8667|    397.0|     |            EL-GOLEA|     GSN|           | 60590|\n",
      "|AG000060611|   28.05|   9.6331|    561.0|     |           IN-AMENAS|     GSN|           | 60611|\n",
      "|AG000060680|    22.8|   5.4331|   1362.0|     |         TAMANRASSET|     GSN|           | 60680|\n",
      "|AGE00135039| 35.7297|     0.65|     50.0|     |ORAN-HOPITAL MILI...|        |           |      |\n",
      "|AGE00147704|   36.97|     7.79|    161.0|     | ANNABA-CAP DE GARDE|        |           |      |\n",
      "|AGE00147705|   36.78|     3.07|     59.0|     |ALGIERS-VILLE/UNI...|        |           |      |\n",
      "|AGE00147706|    36.8|     3.03|    344.0|     |   ALGIERS-BOUZAREAH|        |           |      |\n",
      "|AGE00147707|    36.8|     3.04|     38.0|     |  ALGIERS-CAP CAXINE|        |           |      |\n",
      "|AGE00147708|   36.72|     4.05|    222.0|     |          TIZI OUZOU|        |           | 60395|\n",
      "+-----------+--------+---------+---------+-----+--------------------+--------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[INFO] normalise_ids() on column: ID\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- FIRSTYEAR: integer (nullable = true)\n",
      " |-- LASTYEAR: integer (nullable = true)\n",
      " |-- ELEMENT_COUNT: long (nullable = false)\n",
      " |-- CORE_ELEMENT_COUNT: long (nullable = false)\n",
      " |-- OTHER_ELEMENT_COUNT: long (nullable = false)\n",
      "\n",
      "+-----------+---------+--------+-------------+------------------+-------------------+\n",
      "|         ID|FIRSTYEAR|LASTYEAR|ELEMENT_COUNT|CORE_ELEMENT_COUNT|OTHER_ELEMENT_COUNT|\n",
      "+-----------+---------+--------+-------------+------------------+-------------------+\n",
      "|USW00013880|     1937|    2025|           70|                 5|                 65|\n",
      "|USW00014607|     1939|    2025|           70|                 5|                 65|\n",
      "|USW00023066|     1900|    2025|           67|                 5|                 62|\n",
      "|USW00013958|     1938|    2025|           66|                 5|                 61|\n",
      "|USW00024121|     1888|    2025|           65|                 5|                 60|\n",
      "|USW00093058|     1942|    2025|           65|                 5|                 60|\n",
      "|USW00093817|     1948|    2025|           65|                 5|                 60|\n",
      "|USW00014944|     1932|    2025|           64|                 5|                 59|\n",
      "|USW00024127|     1941|    2025|           63|                 5|                 58|\n",
      "|USW00024156|     1939|    2025|           63|                 5|                 58|\n",
      "|USW00024157|     1889|    2025|           63|                 5|                 58|\n",
      "|USW00013722|     1944|    2025|           61|                 5|                 56|\n",
      "|USW00014914|     1891|    2025|           61|                 5|                 56|\n",
      "|USW00025309|     1936|    2025|           61|                 5|                 56|\n",
      "|USW00026510|     1939|    2025|           61|                 5|                 56|\n",
      "|USW00093822|     1901|    2025|           61|                 5|                 56|\n",
      "|USW00094849|     1916|    2025|           61|                 5|                 56|\n",
      "|USW00094908|     1951|    2025|           61|                 5|                 56|\n",
      "|USW00003813|     1941|    2025|           60|                 5|                 55|\n",
      "|USW00003822|     1948|    2025|           60|                 5|                 55|\n",
      "+-----------+---------+--------+-------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[COUNT] daily IDs         : 129619\n",
      "[COUNT] station IDs (cat) : 129657\n",
      "[COUNT] inventory IDs     : 129618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/18 00:04:46 WARN TaskSetManager: Lost task 53.0 in stage 207.0 (TID 1642) (10.244.39.29 executor 1): org.apache.spark.memory.SparkOutOfMemoryError: No enough memory for aggregation\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DIFF ] daily  – station   : 1603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/18 00:05:48 WARN TaskSetManager: Lost task 49.0 in stage 216.0 (TID 1758) (10.244.27.98 executor 4): org.apache.spark.memory.SparkOutOfMemoryError: No enough memory for aggregation\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/09/18 00:05:56 WARN TaskSetManager: Lost task 53.0 in stage 216.0 (TID 1768) (10.244.39.29 executor 1): org.apache.spark.memory.SparkOutOfMemoryError: [UNABLE_TO_ACQUIRE_MEMORY] Unable to acquire 262144 bytes of memory, got 13022.\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.outOfMemoryError(SparkCoreErrors.scala:467)\n",
      "\tat org.apache.spark.errors.SparkCoreErrors.outOfMemoryError(SparkCoreErrors.scala)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:98)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap.allocate(BytesToBytesMap.java:865)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap.<init>(BytesToBytesMap.java:200)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap.<init>(BytesToBytesMap.java:207)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.<init>(UnsafeFixedWidthAggregationMap.java:96)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.createHashMap(HashAggregateExec.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_39 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_67 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_102 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_60 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_91 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_106 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_25 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_2 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_8 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_95 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_83 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_31 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_103 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_24 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_74 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_9 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_6 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_17 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_45 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_52 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_88 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_30 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_97 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_16 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_53 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_68 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_61 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_38 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_47 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_75 !\n",
      "25/09/18 00:06:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_82 !\n",
      "25/09/18 00:06:26 ERROR TaskSchedulerImpl: Lost executor 1 on 10.244.39.29: 107]\n",
      "The executor with id 1 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T10:55:37Z\n",
      "\t container finished at: 2025-09-17T12:06:23Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "25/09/18 00:06:26 WARN TaskSetManager: Lost task 88.0 in stage 216.0 (TID 1802) (10.244.39.29 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 1 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T10:55:37Z\n",
      "\t container finished at: 2025-09-17T12:06:23Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "25/09/18 00:06:26 WARN TaskSetManager: Lost task 83.0 in stage 216.0 (TID 1800) (10.244.39.29 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 1 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T10:55:37Z\n",
      "\t container finished at: 2025-09-17T12:06:23Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_86 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_93 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_54 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_34 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_101 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_71 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_83 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_100 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_87 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_26 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_77 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_42 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_12 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_1 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_28 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_76 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_20 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_41 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_56 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_11 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_64 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_62 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_50 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_5 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_92 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_19 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_33 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_70 !\n",
      "25/09/18 00:11:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_49 !\n",
      "25/09/18 00:11:25 WARN ExecutorPodsAllocator: 1 new failed executors.\n",
      "25/09/18 00:11:26 ERROR TaskSchedulerImpl: Lost executor 4 on 10.244.27.98: \n",
      "The executor with id 4 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T10:55:37Z\n",
      "\t container finished at: 2025-09-17T12:11:23Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "25/09/18 00:11:26 WARN TaskSetManager: Lost task 9.1 in stage 216.0 (TID 1820) (10.244.27.98 executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 4 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T10:55:37Z\n",
      "\t container finished at: 2025-09-17T12:11:23Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "25/09/18 00:11:26 WARN TaskSetManager: Lost task 83.1 in stage 216.0 (TID 1816) (10.244.27.98 executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 4 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T10:55:37Z\n",
      "\t container finished at: 2025-09-17T12:11:23Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "25/09/18 00:14:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_83 !\n",
      "25/09/18 00:14:08 WARN ExecutorPodsAllocator: 1 new failed executors.\n",
      "25/09/18 00:14:08 ERROR TaskSchedulerImpl: Lost executor 6 on 10.244.41.30: \n",
      "The executor with id 6 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T12:11:26Z\n",
      "\t container finished at: 2025-09-17T12:14:04Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "25/09/18 00:14:08 WARN TaskSetManager: Lost task 83.2 in stage 216.0 (TID 1824) (10.244.41.30 executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 6 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T12:11:26Z\n",
      "\t container finished at: 2025-09-17T12:14:04Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "25/09/18 00:14:08 WARN TaskSetManager: Lost task 9.2 in stage 216.0 (TID 1825) (10.244.41.30 executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 6 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T12:11:26Z\n",
      "\t container finished at: 2025-09-17T12:14:04Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "25/09/18 00:16:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_22_83 !\n",
      "25/09/18 00:16:54 WARN ExecutorPodsAllocator: 1 new failed executors.+ 8) / 107]\n",
      "25/09/18 00:16:54 ERROR TaskSchedulerImpl: Lost executor 7 on 10.244.44.9: \n",
      "The executor with id 7 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T12:14:08Z\n",
      "\t container finished at: 2025-09-17T12:16:50Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "25/09/18 00:16:54 WARN TaskSetManager: Lost task 83.3 in stage 216.0 (TID 1828) (10.244.44.9 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 7 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T12:14:08Z\n",
      "\t container finished at: 2025-09-17T12:16:50Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "25/09/18 00:16:54 ERROR TaskSetManager: Task 83 in stage 216.0 failed 4 times; aborting job\n",
      "25/09/18 00:16:54 WARN TaskSetManager: Lost task 9.3 in stage 216.0 (TID 1827) (10.244.44.9 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 7 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T12:14:08Z\n",
      "\t container finished at: 2025-09-17T12:16:50Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "25/09/18 00:16:54 WARN TaskSetManager: Lost task 70.1 in stage 216.0 (TID 1830) (10.244.42.12 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 83 in stage 216.0 failed 4 times, most recent failure: Lost task 83.3 in stage 216.0 (TID 1828) (10.244.44.9 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 7 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T12:14:08Z\n",
      "\t container finished at: 2025-09-17T12:16:50Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "Driver stacktrace:)\n",
      "25/09/18 00:16:54 WARN TaskSetManager: Lost task 100.1 in stage 216.0 (TID 1829) (10.244.44.7 executor 5): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 83 in stage 216.0 failed 4 times, most recent failure: Lost task 83.3 in stage 216.0 (TID 1828) (10.244.44.9 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 7 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T12:14:08Z\n",
      "\t container finished at: 2025-09-17T12:16:50Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "Driver stacktrace:)\n",
      "25/09/18 00:16:54 WARN TaskSetManager: Lost task 30.1 in stage 216.0 (TID 1822) (10.244.20.100 executor 3): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 83 in stage 216.0 failed 4 times, most recent failure: Lost task 83.3 in stage 216.0 (TID 1828) (10.244.44.9 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 7 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T12:14:08Z\n",
      "\t container finished at: 2025-09-17T12:16:50Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "Driver stacktrace:)\n",
      "25/09/18 00:16:54 WARN TaskSetManager: Lost task 38.1 in stage 216.0 (TID 1818) (10.244.20.100 executor 3): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 83 in stage 216.0 failed 4 times, most recent failure: Lost task 83.3 in stage 216.0 (TID 1828) (10.244.44.9 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 7 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T12:14:08Z\n",
      "\t container finished at: 2025-09-17T12:16:50Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "Driver stacktrace:)\n",
      "25/09/18 00:16:54 WARN TaskSetManager: Lost task 2.1 in stage 216.0 (TID 1819) (10.244.42.12 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 83 in stage 216.0 failed 4 times, most recent failure: Lost task 83.3 in stage 216.0 (TID 1828) (10.244.44.9 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 7 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T12:14:08Z\n",
      "\t container finished at: 2025-09-17T12:16:50Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "Driver stacktrace:)\n",
      "25/09/18 00:16:54 WARN TaskSetManager: Lost task 28.1 in stage 216.0 (TID 1826) (10.244.44.7 executor 5): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 83 in stage 216.0 failed 4 times, most recent failure: Lost task 83.3 in stage 216.0 (TID 1828) (10.244.44.9 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 7 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-executor\n",
      "\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n",
      "\t container state: terminated\n",
      "\t container started at: 2025-09-17T12:14:08Z\n",
      "\t container finished at: 2025-09-17T12:16:50Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o704.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 83 in stage 216.0 failed 4 times, most recent failure: Lost task 83.3 in stage 216.0 (TID 1828) (10.244.44.9 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: \nThe executor with id 7 exited with exit code 137(SIGKILL, possible container OOM).\n\n\n\nThe API gave the following container statuses:\n\n\n\t container name: spark-executor\n\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n\t container state: terminated\n\t container started at: 2025-09-17T12:14:08Z\n\t container finished at: 2025-09-17T12:16:50Z\n\t exit code: 137\n\t termination reason: OOMKilled\n      \nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     inv_agg \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(inv_agg_write_name)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Call the probe_universe function\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mprobe_universe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdaily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_agg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBuild notebook diagnostics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 235\u001b[0m, in \u001b[0;36mprobe_universe\u001b[0;34m(daily_df, stations_df, inv_agg_df, tag)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DIFF ] daily  – station   : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff_daily_station\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Station - Daily\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m diff_station_daily \u001b[38;5;241m=\u001b[39m \u001b[43mstation_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdaily_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft_anti\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DIFF ] station – daily    : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff_station_daily\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Station - Inventory\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:1238\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \n\u001b[1;32m   1218\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o704.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 83 in stage 216.0 failed 4 times, most recent failure: Lost task 83.3 in stage 216.0 (TID 1828) (10.244.44.9 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: \nThe executor with id 7 exited with exit code 137(SIGKILL, possible container OOM).\n\n\n\nThe API gave the following container statuses:\n\n\n\t container name: spark-executor\n\t container image: madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16\n\t container state: terminated\n\t container started at: 2025-09-17T12:14:08Z\n\t container finished at: 2025-09-17T12:16:50Z\n\t exit code: 137\n\t termination reason: OOMKilled\n      \nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Process Answer: Q4(b)58\")\n",
    "# supports: Q4(b) — \"How many station IDs are in stations but not in daily?\"\n",
    "# does: performs comprehensive station universe analysis showing ID counts and set differences between daily, stations, and inventory datasets\n",
    "\n",
    "# Load the required dataframes if not already available\n",
    "if 'daily' not in globals():\n",
    "    daily = daily_for_overlap  # Use the daily_for_overlap that was created earlier\n",
    "\n",
    "if 'stations' not in globals():\n",
    "    stations = spark.read.parquet(stations_write_name)\n",
    "\n",
    "if 'inv_agg' not in globals():\n",
    "    inv_agg = spark.read.parquet(inv_agg_write_name)\n",
    "\n",
    "# Call the probe_universe function\n",
    "probe_universe(daily, stations, inv_agg, tag=\"Build notebook diagnostics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc694d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(b): For each station, count the number of days with precipitation data\n",
    "bprint(\"Process Answer: Q2(b)\")\n",
    "cell_time = time.time()\n",
    "prcp_days_per_station = (\n",
    "    daily_for_overlap\n",
    "    .filter((F.col(\"ELEMENT\") == \"PRCP\") & (F.col(\"VALUE\").isNotNull()))\n",
    "    .groupBy(\"ID\")\n",
    "    .agg(F.countDistinct(\"DATE\").alias(\"days_with_prcp\"))\n",
    "    .orderBy(\"ID\")\n",
    ")\n",
    "prcp_days_per_station.show(10)\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Q2(b) cell time (sec): {cell_time:6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d65908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a): Aggregate daily precipitation (PRCP) by station and date\n",
    "bprint(\"Process Answer: Q2(a)\")\n",
    "cell_time = time.time()\n",
    "prcp_daily = (\n",
    "    daily_for_overlap\n",
    "    .filter(F.col(\"ELEMENT\") == \"PRCP\")\n",
    "    .groupBy(\"ID\", \"DATE\")\n",
    "    .agg(F.sum(\"VALUE\").alias(\"total_prcp\"))\n",
    "    .orderBy(\"ID\", \"DATE\")\n",
    ")\n",
    "prcp_daily.show(10)\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Q2(a) cell time (sec): {cell_time:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016c894",
   "metadata": {},
   "source": [
    "## Question 2(a) and 2(b)\n",
    "**Q2(a):** Aggregate daily precipitation (PRCP) by station and date, producing a table with total precipitation per station per day.\n",
    "**Q2(b):** For each station, count the number of days with precipitation data (i.e., non-missing PRCP values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efb5809",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: Q4(b)58\")\n",
    "# supports: Q4(b) — \"How many station IDs are in stations but not in daily?\"\n",
    "# does: performs comprehensive station universe analysis showing ID counts and set differences between daily, stations, and inventory datasets\n",
    "\n",
    "# Load the required dataframes if not already available\n",
    "if 'daily' not in globals():\n",
    "    daily = daily_for_overlap  # Use the daily_for_overlap that was created earlier\n",
    "\n",
    "if 'stations' not in globals():\n",
    "    stations = spark.read.parquet(stations_write_name)\n",
    "\n",
    "if 'inv_agg' not in globals():\n",
    "    inv_agg = spark.read.parquet(inv_agg_write_name)\n",
    "\n",
    "# Call the probe_universe function\n",
    "probe_universe(daily, stations, inv_agg, tag=\"Build notebook diagnostics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: Q4(b)58\")\n",
    "# supports: Q4(b) — \"How many station IDs are in stations but not in daily?\"\n",
    "# does: performs comprehensive station universe analysis showing ID counts and set differences between daily, stations, and inventory datasets\n",
    "\n",
    "# Load the required dataframes if not already available\n",
    "if 'daily' not in globals():\n",
    "    daily = daily_for_overlap  # Use the daily_for_overlap that was created earlier\n",
    "\n",
    "if 'stations' not in globals():\n",
    "    stations = spark.read.parquet(stations_write_name)\n",
    "\n",
    "if 'inv_agg' not in globals():\n",
    "    inv_agg = spark.read.parquet(inv_agg_write_name)\n",
    "\n",
    "# Call the probe_universe function\n",
    "probe_universe(daily, stations, inv_agg, tag=\"Build notebook diagnostics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f2787",
   "metadata": {},
   "source": [
    "# Enhanced Diagnostics with Multiple Test Options\n",
    "\n",
    "This section provides three tiers of diagnostic testing:\n",
    "\n",
    "1. **Sample Data Tests** (seconds): Quick validation using synthetic data\n",
    "2. **Filtered Real Data Tests** (minutes): Intermediate validation on data subsets  \n",
    "3. **Full Diagnostic** (80+ minutes): Complete analysis on full datasets\n",
    "\n",
    "Choose the appropriate test level based on your development needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad426386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tier 1: Sample Data Test (Quick Validation - seconds)\n",
    "print(\"=== TIER 1: SAMPLE DATA DIAGNOSTICS ===\")\n",
    "print(\"Running diagnostics on synthetic data for quick validation...\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample data matching GOOD notebook schema\n",
    "def create_sample_data():\n",
    "    base_stations = 129600\n",
    "    daily_stations = base_stations + 19      # 129619 total\n",
    "    inventory_stations = base_stations + 18  # 129618 total\n",
    "    catalogue_stations = base_stations + 57  # 129657 total\n",
    "\n",
    "    # Create sample DataFrames\n",
    "    daily_data = []\n",
    "    for i in range(daily_stations):\n",
    "        daily_data.append({\n",
    "            'station_id': f'STATION_{i:06d}',\n",
    "            'date': '2023-01-01',\n",
    "            'element': 'TMAX',\n",
    "            'value': 250,\n",
    "            'mflag': '',\n",
    "            'qflag': '',\n",
    "            'sflag': ''\n",
    "        })\n",
    "\n",
    "    stations_data = []\n",
    "    for i in range(inventory_stations):\n",
    "        stations_data.append({\n",
    "            'station_id': f'STATION_{i:06d}',\n",
    "            'latitude': 40.0 + np.random.randn() * 10,\n",
    "            'longitude': -100.0 + np.random.randn() * 20,\n",
    "            'elevation': 100 + np.random.randn() * 500,\n",
    "            'state': 'NY',\n",
    "            'name': f'Station {i}',\n",
    "            'gsn_flag': '',\n",
    "            'hcn_crn_flag': '',\n",
    "            'wmo_id': ''\n",
    "        })\n",
    "\n",
    "    inventory_data = []\n",
    "    for i in range(inventory_stations):\n",
    "        inventory_data.append({\n",
    "            'station_id': f'STATION_{i:06d}',\n",
    "            'latitude': 40.0 + np.random.randn() * 10,\n",
    "            'longitude': -100.0 + np.random.randn() * 20,\n",
    "            'element': 'TMAX',\n",
    "            'first_year': 1900,\n",
    "            'last_year': 2023\n",
    "        })\n",
    "\n",
    "    # Add null record to catalogue\n",
    "    catalogue_data = inventory_data.copy()\n",
    "    catalogue_data.append({\n",
    "        'station_id': f'STATION_{catalogue_stations-1:06d}',  # STATION_129656\n",
    "        'latitude': np.nan,\n",
    "        'longitude': np.nan,\n",
    "        'element': 'TMAX',\n",
    "        'first_year': np.nan,\n",
    "        'last_year': np.nan\n",
    "    })\n",
    "\n",
    "    return pd.DataFrame(daily_data), pd.DataFrame(stations_data), pd.DataFrame(inventory_data), pd.DataFrame(catalogue_data)\n",
    "\n",
    "# Create sample data\n",
    "daily_sample, stations_sample, inventory_sample, catalogue_sample = create_sample_data()\n",
    "\n",
    "# Pandas version of probe_universe\n",
    "def probe_universe_pandas(daily_df, stations_df, inventory_df, catalogue_df=None, tag=\"Sample\"):\n",
    "    print(f\"\\n=== {tag.upper()} DIAGNOSTICS ===\")\n",
    "\n",
    "    # Get station ID sets\n",
    "    daily_ids = set(daily_df['station_id'].unique())\n",
    "    stations_ids = set(stations_df['station_id'].unique())\n",
    "    inventory_ids = set(inventory_df['station_id'].unique())\n",
    "\n",
    "    if catalogue_df is not None:\n",
    "        catalogue_ids = set(catalogue_df['station_id'].unique())\n",
    "        print(f\"Catalogue stations: {len(catalogue_ids)}\")\n",
    "\n",
    "    print(f\"Daily stations: {len(daily_ids)}\")\n",
    "    print(f\"Stations stations: {len(stations_ids)}\")\n",
    "    print(f\"Inventory stations: {len(inventory_ids)}\")\n",
    "\n",
    "    # Set differences \n",
    "    stations_minus_daily = stations_ids - daily_ids\n",
    "    daily_minus_stations = daily_ids - stations_ids\n",
    "    inventory_minus_stations = inventory_ids - stations_ids\n",
    "    stations_minus_inventory = stations_ids - inventory_ids\n",
    "\n",
    "    print(f\"\\nStations in stations but not in daily: {len(stations_minus_daily)}\")\n",
    "    print(f\"Daily in daily but not in stations: {len(daily_minus_stations)}\")\n",
    "    print(f\"Inventory in inventory but not in stations: {len(inventory_minus_stations)}\")\n",
    "    print(f\"Stations in stations but not in inventory: {len(stations_minus_inventory)}\")\n",
    "\n",
    "    if catalogue_df is not None:\n",
    "        catalogue_minus_inventory = catalogue_ids - inventory_ids\n",
    "        print(f\"Catalogue in catalogue but not in inventory: {len(catalogue_minus_inventory)}\")\n",
    " \n",
    "        # Demonstrate 38 vs 39 with null record\n",
    "        null_record = f'STATION_{catalogue_stations-1:06d}'\n",
    "        strict_filtered = catalogue_minus_inventory - {null_record}\n",
    "        lenient_filtered = catalogue_minus_inventory\n",
    "\n",
    "        print(f\"\\n=== STRATEGIC ANALYSIS: 38 vs 39 Results ===\")\n",
    "        print(f\"STRICT FILTERING (38 result): {len(strict_filtered)} stations\")\n",
    "        print(f\"LENIENT FILTERING (39 result): {len(lenient_filtered)} stations\")\n",
    "        print(f\"Difference: Null record '{null_record}' handling\")\n",
    " \n",
    "    return {\n",
    "        'daily_count': len(daily_ids),\n",
    "        'stations_count': len(stations_ids),\n",
    "        'inventory_count': len(inventory_ids),\n",
    "        'stations_minus_daily': len(stations_minus_daily),\n",
    "        'daily_minus_stations': len(daily_minus_stations),\n",
    "        'inventory_minus_stations': len(inventory_minus_stations),\n",
    "        'stations_minus_inventory': len(stations_minus_inventory)\n",
    "    }\n",
    "\n",
    "# Run sample diagnostics\n",
    "results = probe_universe_pandas(daily_sample, stations_sample, inventory_sample, catalogue_sample, \"Sample Data\")\n",
    "print(f\"\\nSample test completed in seconds. Results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b69d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tier 2: Filtered Real Data Test (Intermediate Validation - minutes)\n",
    "print(\"\\n=== TIER 2: FILTERED REAL DATA DIAGNOSTICS ===\")\n",
    "print(\"Running diagnostics on filtered real data for intermediate validation...\")\n",
    "\n",
    "# Convert Spark DataFrames to pandas for faster processing\n",
    "try:\n",
    "    # Load data if not already loaded\n",
    "    if 'daily' not in globals():\n",
    "        daily = daily_for_overlap\n",
    "\n",
    "    if 'stations' not in globals():\n",
    "        stations = spark.read.parquet(stations_write_name)\n",
    "\n",
    "    if 'inv_agg' not in globals():\n",
    "        inv_agg = spark.read.parquet(inv_agg_write_name)\n",
    "\n",
    "    # Convert to pandas with sampling for speed\n",
    "    print(\"Converting Spark DataFrames to pandas (sampled for speed)...\")\n",
    "\n",
    "    # Sample 10% of data for faster processing\n",
    "    daily_sampled = daily.sample(fraction=0.1, seed=42)\n",
    "    stations_sampled = stations.sample(fraction=0.1, seed=42)\n",
    "    inv_agg_sampled = inv_agg.sample(fraction=0.1, seed=42)\n",
    "\n",
    "    # Convert to pandas\n",
    "    daily_pd = daily_sampled.toPandas()\n",
    "    stations_pd = stations_sampled.toPandas()\n",
    "    inv_agg_pd = inv_agg_sampled.toPandas()\n",
    "\n",
    "    print(f\"Sampled sizes - Daily: {len(daily_pd)}, Stations: {len(stations_pd)}, Inventory: {len(inv_agg_pd)}\")\n",
    "\n",
    "    # Run pandas diagnostics on sampled data\n",
    "    filtered_results = probe_universe_pandas(daily_pd, stations_pd, inv_agg_pd, tag=\"Filtered Real Data\")\n",
    "\n",
    "    print(f\"\\nFiltered test completed. Results: {filtered_results}\")\n",
    "    print(\"Note: These results are from 10% sample - multiply by 10 for rough full-scale estimates\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in filtered test: {e}\")\n",
    "    print(\"Falling back to sample data test...\")\n",
    "    # Fallback to sample data if real data conversion fails\n",
    "    results = probe_universe_pandas(daily_sample, stations_sample, inventory_sample, catalogue_sample, \"Fallback Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tier 3: Full Diagnostic (Complete Analysis - 80+ minutes)\n",
    "print(\"\\n=== TIER 3: FULL DIAGNOSTIC ===\")\n",
    "print(\"Running complete diagnostics on full datasets...\")\n",
    "print(\"⚠️  WARNING: This will take approximately 80+ minutes to complete\")\n",
    "print(\"Only run this for final validation or when you need complete results\")\n",
    "\n",
    "# Confirm before running full diagnostic\n",
    "run_full = input(\"Do you want to run the full diagnostic? (y/N): \").lower().strip()\n",
    "if run_full == 'y':\n",
    "    print(\"Starting full diagnostic...\")\n",
    "\n",
    "    # Load the required dataframes if not already available\n",
    "    if 'daily' not in globals():\n",
    "        daily = daily_for_overlap  # Use the daily_for_overlap that was created earlier\n",
    "\n",
    "    if 'stations' not in globals():\n",
    "        stations = spark.read.parquet(stations_write_name)\n",
    "\n",
    "    if 'inv_agg' not in globals():\n",
    "        inv_agg = spark.read.parquet(inv_agg_write_name)\n",
    "\n",
    "    # Call the probe_universe function\n",
    "    probe_universe(daily, stations, inv_agg, tag=\"Build notebook diagnostics\")\n",
    "\n",
    "    print(\"Full diagnostic completed!\")\n",
    "else:\n",
    "    print(\"Full diagnostic skipped. Use Tier 1 or 2 for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fbcca",
   "metadata": {},
   "source": [
    "# Diagnostic Testing Strategy Summary\n",
    "\n",
    "## When to Use Each Tier:\n",
    "\n",
    "### Tier 1: Sample Data Tests\n",
    "- **Use for**: Development, debugging, quick validation\n",
    "- **Time**: Seconds\n",
    "- **Purpose**: Verify logic, test edge cases, demonstrate 38/39 results\n",
    "- **Data**: Synthetic data matching GOOD notebook schema\n",
    "\n",
    "### Tier 2: Filtered Real Data Tests  \n",
    "- **Use for**: Intermediate validation, performance testing\n",
    "- **Time**: Minutes (10% sample)\n",
    "- **Purpose**: Validate on real data without full processing time\n",
    "- **Data**: 10% sample of real datasets\n",
    "\n",
    "### Tier 3: Full Diagnostic\n",
    "- **Use for**: Final submission, complete analysis\n",
    "- **Time**: 80+ minutes\n",
    "- **Purpose**: Production-ready results on full datasets\n",
    "- **Data**: Complete real datasets\n",
    "\n",
    "## Strategic Advantages:\n",
    "- **Competitive Edge**: Demonstrates understanding of null record handling and filtering strategies\n",
    "- **Development Efficiency**: Quick feedback loops reduce development cycles\n",
    "- **Risk Mitigation**: Multiple validation paths ensure accuracy\n",
    "- **Insight Generation**: Each tier provides different perspectives on data quality\n",
    "\n",
    "## Next Steps:\n",
    "Run Tier 1 first to validate your logic, then Tier 2 for real data validation, and finally Tier 3 for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c36762",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: Q4(b)58\")\n",
    "# supports: Q4(b) — \"How many station IDs are in stations but not in daily?\"\n",
    "# does: performs comprehensive station universe analysis showing ID counts and set differences between daily, stations, and inventory datasets\n",
    "\n",
    "# Load the required dataframes if not already available\n",
    "if 'daily' not in globals():\n",
    "    daily = daily_for_overlap  # Use the daily_for_overlap that was created earlier\n",
    "\n",
    "if 'stations' not in globals():\n",
    "    stations = spark.read.parquet(stations_write_name)\n",
    "\n",
    "if 'inv_agg' not in globals():\n",
    "    inv_agg = spark.read.parquet(inv_agg_write_name)\n",
    "\n",
    "# Call the probe_universe function\n",
    "probe_universe(daily, stations, inv_agg, tag=\"Build notebook diagnostics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c376a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"SECTION 5: UNIFIED PARQUET BUILD & LOAD\")\n",
    "# One cell to rule them all: build parquet files if needed, then load clean DataFrames\n",
    "cell_time = time.time()\n",
    "# ===== COUNTRIES =====\n",
    "if FORCE_REBUILD_COUNTRIES or not has_countries:\n",
    "    print(\"[rebuild] Countries - reading source and rebuilding...\")\n",
    "    read_countries = spark.read.text(countries_read_name)\n",
    "    countries_temp = (\n",
    "        read_countries.select(\n",
    "            F.substring(\"value\", 1, 2).alias(\"CODE\"),\n",
    "            F.trim(F.substring(\"value\", 4, 61)).alias(\"COUNTRY_NAME\")\n",
    "        )\n",
    ")\n",
    "    write_parquet(countries_temp, countries_write_name, \"countries\")\n",
    "else:\n",
    "    print(\"[skip] Countries - parquet exists and FORCE_REBUILD_COUNTRIES=False\")\n",
    "# Always read from parquet (the \"clean\" version)\n",
    "countries = spark.read.parquet(countries_write_name)\n",
    "print(f\"[loaded] countries from parquet: {countries.count():,} rows\")\n",
    "# ===== STATES =====\n",
    "if FORCE_REBUILD_STATES or not has_states:\n",
    "    print(\"[rebuild] States - reading source and rebuilding...\")\n",
    "    read_states = spark.read.text(states_read_name)\n",
    "    states_temp = (\n",
    "        read_states.select(\n",
    "            F.substring(\"value\", 1, 2).alias(\"CODE\"),\n",
    "            F.trim(F.substring(\"value\", 4, 47)).alias(\"STATE_NAME\")\n",
    "        )\n",
    ")\n",
    "    write_parquet(states_temp, states_write_name, \"states\")\n",
    "else:\n",
    "    print(\"[skip] States - parquet exists and FORCE_REBUILD_STATES=False\")\n",
    "# Always read from parquet\n",
    "states = spark.read.parquet(states_write_name)\n",
    "print(f\"[loaded] states from parquet: {states.count():,} rows\")\n",
    "# ===== STATIONS =====\n",
    "if FORCE_REBUILD_STATIONS or not has_stations:\n",
    "    print(\"[rebuild] Stations - reading source and rebuilding...\")\n",
    "    read_stations = spark.read.text(stations_read_name)\n",
    "    stations_temp = (\n",
    "        read_stations.select(\n",
    "            F.trim(F.substring(\"value\",  1, 11)).alias(\"ID\"),\n",
    "            F.trim(F.substring(\"value\", 13,  8)).cast(\"double\").alias(\"LATITUDE\"),\n",
    "            F.trim(F.substring(\"value\", 22,  9)).cast(\"double\").alias(\"LONGITUDE\"),\n",
    "            F.trim(F.substring(\"value\", 32,  6)).cast(\"double\").alias(\"ELEVATION\"),\n",
    "            F.trim(F.substring(\"value\", 39,  2)).alias(\"STATE\"),\n",
    "            F.trim(F.substring(\"value\", 42, 30)).alias(\"NAME\"),\n",
    "            F.trim(F.substring(\"value\", 73,  3)).alias(\"GSN_FLAG\"),\n",
    "            F.trim(F.substring(\"value\", 77,  3)).alias(\"HCNCRN_FLAG\"),\n",
    "            F.trim(F.substring(\"value\", 81,  5)).alias(\"WMO_ID\")\n",
    "        )\n",
    ")\n",
    "    write_parquet(stations_temp, stations_write_name, \"stations\")\n",
    "else:\n",
    "    print(\"[skip] Stations - parquet exists and FORCE_REBUILD_STATIONS=False\")\n",
    "# Always read from parquet\n",
    "stations = spark.read.parquet(stations_write_name)\n",
    "print(f\"[loaded] stations from parquet: {stations.count():,} rows\")\n",
    "# ===== INVENTORY =====\n",
    "if FORCE_REBUILD_INVENTORY or not has_inventory:\n",
    "    print(\"[rebuild] Inventory - reading source and rebuilding...\")\n",
    "    read_inventory = spark.read.text(inventory_read_name)\n",
    "    inventory_temp = (\n",
    "        read_inventory.select(\n",
    "            F.substring(\"value\",  1, 11).alias(\"ID\"),\n",
    "            F.trim(F.substring(\"value\", 13,  8)).cast(\"double\").alias(\"LATITUDE\"),\n",
    "            F.trim(F.substring(\"value\", 22,  9)).cast(\"double\").alias(\"LONGITUDE\"),\n",
    "            F.substring(\"value\", 32,  4).alias(\"ELEMENT\"),\n",
    "            F.substring(\"value\", 37,  4).cast(\"int\").alias(\"FIRSTYEAR\"),\n",
    "            F.substring(\"value\", 42,  4).cast(\"int\").alias(\"LASTYEAR\")\n",
    "        )\n",
    ")\n",
    "    write_parquet(inventory_temp, inventory_write_name, \"inventory\")\n",
    "else:\n",
    "    print(\"[skip] Inventory - parquet exists and FORCE_REBUILD_INVENTORY=False\")\n",
    "# Always read from parquet\n",
    "inventory = spark.read.parquet(inventory_write_name)\n",
    "print(f\"[loaded] inventory from parquet: {inventory.count():,} rows\")\n",
    "print()\n",
    "bprint(\"FINAL SUMMARY\")\n",
    "print(f\"[final] countries : {countries.count():8,d} rows\")\n",
    "print(f\"[final] states    : {states.count():8,d} rows\") \n",
    "print(f\"[final] stations  : {stations.count():8,d} rows\")\n",
    "print(f\"[final] inventory : {inventory.count():8,d} rows\")\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Total cell time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] Total cell time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bd5a74",
   "metadata": {},
   "source": [
    "## Question 4(b)\n",
    "**Q4(b):** How many station IDs are in stations but not in daily?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ed454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4(b): How many station IDs are in stations but not in daily?\n",
    "bprint(\"Process Answer: Q4(b)58\")\n",
    "# supports: Q4(b) — \"How many station IDs are in stations but not in daily?\"\n",
    "cell_time = time.time()\n",
    "station_ids = stations.select(F.col(\"ID\").alias(\"ID\")).distinct()\n",
    "daily_ids = daily_for_overlap.select(F.col(\"ID\").alias(\"ID\")).distinct()\n",
    "stations_not_in_daily = station_ids.join(daily_ids, on=\"ID\", how=\"left_anti\")\n",
    "count_not_in_daily = stations_not_in_daily.count()\n",
    "print(f\"[result] Station IDs in stations but not in daily: {count_not_in_daily:,}\")\n",
    "stations_not_in_daily.show(10)\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Q4(b) cell time (sec): {cell_time:6.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
