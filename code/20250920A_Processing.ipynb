{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "467d2c76",
   "metadata": {},
   "source": [
    "## Q1(a): Data structure and compression\n",
    "\n",
    "**Assignment Question:**\n",
    "How is the data structured? Are any of the datasets compressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825cee97",
   "metadata": {},
   "source": [
    "## Q1(b): Years in daily and data size changes\n",
    "\n",
    "**Assignment Question:**\n",
    "How many years are contained in daily, and how does the size of the data change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c20bc6",
   "metadata": {},
   "source": [
    "## Q1(c): Total data size\n",
    "\n",
    "**Assignment Question:**\n",
    "What is the total size of all of the data, and how much of that is daily?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2c8f7",
   "metadata": {},
   "source": [
    "## Q2(a): Define schema for daily\n",
    "\n",
    "**Assignment Question:**\n",
    "Define a schema for daily based on the description above or in the GHCN Daily README, using the types defined in pyspark.sql. What do you think is the best way to load the DATE and OBSERVATION TIME columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f50988",
   "metadata": {},
   "source": [
    "## Q2(b): Load daily with schema\n",
    "\n",
    "**Assignment Question:**\n",
    "Modify the spark.read.csv command to load a subset of the most recent year of daily into Spark so that it uses the schema that you defined in step (a). Did anything go wrong when you tried to use the schema? What data types did you end up using and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376e67e8",
   "metadata": {},
   "source": [
    "## Q2(d): Row counts in metadata tables\n",
    "\n",
    "**Assignment Question:**\n",
    "How many rows are there in each of the metadata tables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be54e21",
   "metadata": {},
   "source": [
    "## Q2(e): Row count in daily\n",
    "\n",
    "**Assignment Question:**\n",
    "How many rows are there in daily?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a7df1",
   "metadata": {},
   "source": [
    "## Q4(a): Join daily and stations\n",
    "\n",
    "**Assignment Question:**\n",
    "LEFT JOIN a subset of daily and your stations table from Q3 step (e). How expensive do you think it would be to join all of daily and stations? Can you think of an efficient way to check if there are any stations in stations that are not in daily at all without using LEFT JOIN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260cb883",
   "metadata": {},
   "source": [
    "## Q4(b): Stations not in daily\n",
    "\n",
    "**Assignment Question:**\n",
    "Based on step (a) count the total number of stations in stations that are not in daily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db545270-fabc-4cf7-a32a-e4f4d6fef18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "global azure_account_name\n",
    "global azure_data_container_name\n",
    "global azure_user_container_name\n",
    "global azure_user_token\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "azure_user_token = r\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(f\"fs.azure.sas.{azure_user_container_name}.{azure_account_name}.blob.core.windows.net\",  azure_user_token)\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bbb004-5a22-4f65-b400-cb2bc3edf217",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e2f6d-f8f0-4150-8eec-e5bb2b0471e4",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06343f5-a638-4f7f-ade5-5f564a068bc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/10/01 19:22:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4040\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1759299776697</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>32</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-5fbaae6ca4304df78d25254a2781f4db</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1759299776857</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-3094c0999e703456</td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2025-09-21T20:54:03Z&se=2026-12-31T04:09:03Z&spr=https&sv=2024-11-04&sr=c&sig=5V91JeJe9mD%2FuPKUQ3LCErJh%2FwP0gNYoyl8MMx5pdkM%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=4, executor_cores=2, worker_memory=4, master_memory=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7556669f-ee60-4328-8b8f-29d26d48be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from IPython.display     import display  # calls between environments\n",
    "from math                import acos, atan2, cos, radians, sin, sqrt\n",
    "from matplotlib.ticker   import FuncFormatter, MaxNLocator\n",
    "from pathlib             import Path\n",
    "from pyspark.sql         import DataFrame\n",
    "from pyspark.sql         import DataFrame as SparkDF\n",
    "from pyspark.sql         import functions as F, types as T\n",
    "from pyspark.sql.types   import *\n",
    "from pyspark.sql.utils   import AnalysisException\n",
    "from pyspark.sql.window  import Window\n",
    "from typing              import List, Optional, Tuple\n",
    "from rich.tree           import Tree\n",
    "from rich.console        import Console\n",
    "import itertools         as it\n",
    "import matplotlib.dates  as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "onsole = Console()\n",
    "\n",
    "import math, os, platform, re\n",
    "import subprocess, sys, time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cda5876-8608-4f13-8dba-fa5b7898d0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________HELPER / DIAGNOSTIC FUNCTIONS___________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"_\" * 35 + \"HELPER / DIAGNOSTIC FUNCTIONS\" + \"_\" * 35)\n",
    "\n",
    "notebook_run_time = time.time()\n",
    "\n",
    "def df_as_html(df, n: int = 5, right_align: bool = False, show_index: bool = False):\n",
    "    \"\"\"\n",
    "    HTML preview via pandas with no truncation. If right_align=True,\n",
    "    only numeric columns are right-justified; everything else is \n",
    "    explicitly left-aligned.\n",
    "    \"\"\"\n",
    "    \n",
    "    pdf = df.limit(n).toPandas()\n",
    "    print(\"[INFO] Converting Spark â†’ pandas for HTML display (rows:\", len(pdf), \")\")\n",
    "    print(\"[INFO] right_align (numeric columns):\", right_align)\n",
    "\n",
    "    with pd.option_context(\n",
    "        \"display.max_colwidth\", None,   \n",
    "        \"display.max_columns\", None,    \n",
    "        \"display.width\", None            \n",
    "    ):\n",
    "        styler = pdf.style if show_index else pdf.style.hide(axis=\"index\")\n",
    "\n",
    "        #   table alignment: left for both headers and cells\n",
    "        styler = styler.set_table_styles(\n",
    "            [\n",
    "                {\"selector\": \"th\", \"props\": [(\"text-align\", \"left\")]},\n",
    "                {\"selector\": \"td\", \"props\": [(\"text-align\", \"left\")]},\n",
    "            ],\n",
    "            overwrite=True,  # make this the baseline\n",
    "        )\n",
    "         \n",
    "        if right_align:\n",
    "            numeric_cols = list(pdf.select_dtypes(include=[\"number\"]).columns)\n",
    "            print(\"[INFO] Right-aligning numeric columns:\", numeric_cols)\n",
    "            if numeric_cols:\n",
    "                styler = styler.set_properties(subset=numeric_cols,\n",
    "                                               **{\"text-align\": \"right\"})\n",
    "        display(styler)\n",
    "\n",
    "def _normalise_dir(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure trailing slash so we point to\n",
    "    the dataset directory (not a file)\n",
    "    \"\"\"\n",
    "    return s if s.endswith(\"/\") else s + \"/\"\n",
    "\n",
    "def ensure_dir(path: str) -> str:\n",
    "    \"\"\"\n",
    "    ensures that path is a path \n",
    "    and not representing a file;\n",
    "    add trailing slash if needed\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        raise ValueError(\"Path is None\")\n",
    "    path = _normalise_dir(path)\n",
    "#   print(\"ensure_dir -> \",path)\n",
    "    return path\n",
    "    \n",
    "def show_df(df, n: int = 10, name: str = \"\", right_align: bool = False):\n",
    "    \"\"\"\n",
    "    Print schema, \n",
    "    show an HTML sample,\n",
    "    and row count.\n",
    "    \"\"\"\n",
    "    bprint()\n",
    "    print(\"name : \",name)\n",
    "    df.printSchema()\n",
    "    print(\"[check] sample:\")\n",
    "    df_as_html(df, n=n, right_align=right_align)\n",
    "  \n",
    "def write_parquet(df, dir_as_path: str, df_name:str = \"\"):    \n",
    "    funct_time = time.time()\n",
    "    path = _normalise_dir(dir_as_path)\n",
    "    print(f\"[file] write_parquet  : {path}\")\n",
    "    try:      \n",
    "        show_df(df,df_name)\n",
    "    except Exception as e:\n",
    "        print(\"[cathch] sample failed:\", e)\n",
    "        os.system(f'hdfs dfs -rm -r -f \"{path}\"')   # idempotent cleanup\n",
    "    df.write.mode(\"overwrite\").format(\"parquet\").save(path)\n",
    "    os.system(f'hdfs dfs -ls -R \"{path}\"')\n",
    "    funct_time = time.time() - funct_time \n",
    "    print(f\"[time] write_parquet (min)   : {funct_time/60:5.2f}\")\n",
    "    print(f\"[time] write_parquet (sec)   : {funct_time:5.2f}\")\n",
    "  \n",
    "def has_parquet(dir_as_path: str) -> bool:\n",
    "    path   = _normalise_dir( dir_as_path)\n",
    "    marker = path + '_SUCCESS'\n",
    "    #print(\"\\n[check] dir_path:\", dir_path)\n",
    "    #print(\"\\n[check] path    :\", path)\n",
    "    print(\"\\n[check] marker  :\", marker)\n",
    "    rc = os.system(f'hdfs dfs -test -e \"{marker}\"')\n",
    "    print(\"[check] rc:\", rc, \"->\", (\"exists\" if rc == 0 else \"missing\"))\n",
    "    return (rc == 0)\n",
    "    \n",
    "def _to_spark(df_like, schema=None):\n",
    "    \"\"\"\n",
    "    Return a Spark DataFrame  .\n",
    "    \"\"\"\n",
    "    if isinstance(df_like, SparkDF):\n",
    "        return df_like\n",
    "    return spark.createDataFrame(df_like, schema=schema) if schema else spark.createDataFrame(df_like)\n",
    "\n",
    "def _success_exists(target_dir: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check for the Hadoop/Spark _SUCCESS marker;  \n",
    "    \"\"\"\n",
    "    jvm = spark._jvm\n",
    "    hconf = spark._jsc.hadoopConfiguration()\n",
    "    try:\n",
    "        uri = jvm.java.net.URI(target_dir)\n",
    "        fs = jvm.org.apache.hadoop.fs.FileSystem.get(uri, hconf)\n",
    "        success = jvm.org.apache.hadoop.fs.Path(target_dir + \"_SUCCESS\")\n",
    "        exists = fs.exists(success)\n",
    "        print(f\"[status] _SUCCESS check at: {target_dir}_SUCCESS -> {exists}\")\n",
    "        return bool(exists)\n",
    "    except Exception as e:\n",
    "        print(f\"[status] _SUCCESS check failed ({e}); attempting read-probe â€¦\")\n",
    "        try:\n",
    "            spark.read.parquet(target_dir).limit(1).count()\n",
    "            print(f\"[dewstatus59] read-probe succeeded at: {target_dir}\")\n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"[status] read-probe failed ({e2}); treating as not existing.\")\n",
    "            return False\n",
    " \n",
    "\n",
    "def normalise_ids(df: DataFrame, col: str = \"ID\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    # Single source of truth for ID normalisation \n",
    "    Upper + trim + distinct on the given ID column.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] normalise_ids() on column: {col}\")\n",
    "    df.printSchema()\n",
    "    df.show(20)\n",
    "    return df.select(F.upper(F.trim(F.col(col))).alias(\"ID\")).distinct()\n",
    "    df.printSchema()\n",
    "    df.show(20)\n",
    "# hack \n",
    "_ids       = normalise_ids\n",
    "canon_ids  = normalise_ids\n",
    "_canon_ids = normalise_ids\n",
    " \n",
    "def probe_universe(daily_df, stations_df, inv_agg_df, tag=\"\"):\n",
    "    \"\"\"\n",
    "    DIAGNOSTIC\n",
    "    \"\"\"\n",
    "    # quick previews\n",
    "    daily_df.show(20)\n",
    "    stations_df.show(20)\n",
    "    inv_agg_df.show(20)\n",
    "\n",
    "    print(tag)\n",
    "    daily_df.printSchema()\n",
    "    stations_df.printSchema()\n",
    "    inv_agg_df.printSchema()\n",
    "    print(tag)\n",
    "\n",
    "    print(\"\\n\" + \"_\"*70)\n",
    "    print(f\"[PROBE] Station universe check :: {tag}\")\n",
    "\n",
    "    # id universes\n",
    "    daily_ids   = _ids(daily_df)\n",
    "    station_ids = _ids(stations_df)\n",
    "    inv_ids     = _ids(inv_agg_df)\n",
    "\n",
    "    # counts\n",
    "    print(\"[COUNT] daily IDs         :\", daily_ids.count())\n",
    "    print(\"[COUNT] station IDs (cat) :\", station_ids.count())\n",
    "    print(\"[COUNT] inventory IDs     :\", inv_ids.count())\n",
    "\n",
    "    # set differences\n",
    "    print(\"[DIFF ] daily - station   :\", daily_ids.join(station_ids, \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] station - daily   :\", station_ids.join(daily_ids, \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] station - inv     :\", station_ids.join(inv_ids,    \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] inv - daily       :\", inv_ids.join(daily_ids,      \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] inv - station     :\", inv_ids.join(station_ids,    \"ID\", \"left_anti\").count())\n",
    "\n",
    "    bprint(\"[done] probe_universe\")\n",
    "\n",
    "\n",
    "def _count_unique_ids(df: DataFrame) -> int:\n",
    "    return normalise_ids(df).count()\n",
    "\n",
    "def pick_unfiltered_daily(preferred_path: str = None) -> DataFrame:\n",
    "    \"\"\"Return an unfiltered daily DF (~129k unique station IDs).\"\"\"\n",
    "    cand_names = [\"daily\", \"read_daily\", \"daily_df\", \"daily_all\", \"ghcnd_daily\"]\n",
    "    print(\"[INFO] Candidate DataFrames:\", [n for n in cand_names if n in globals()])\n",
    "    for name in cand_names:\n",
    "        obj = globals().get(name)\n",
    "        if isinstance(obj, DataFrame):\n",
    "            try:\n",
    "                n = normalise_ids(obj).count()\n",
    "                print(f\"[CHECK] {name} unique station IDs:\", n)\n",
    "                if n >= 120_000:\n",
    "                    print(f\"[INFO] Using {name} as the unfiltered daily.\")\n",
    "                    return obj\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not inspect {name}:\", repr(e))\n",
    "    if preferred_path:\n",
    "        print(f\"[INFO] Trying preferred_path: {preferred_path}\")\n",
    "        df = spark.read.parquet(str(preferred_path))\n",
    "        n = normalise_ids(df).count()\n",
    "        print(\"[CHECK] preferred_path unique station IDs:\", n)\n",
    "        if n >= 120_000:\n",
    "            print(\"[INFO] Using preferred_path as the unfiltered daily.\")\n",
    "            return df\n",
    "    for var in [\"DAILY_READ_NAME\",\"DAILY_WRITE_NAME\",\"daily_read_name\",\"daily_write_name\",\"DAILY_NAME\"]:\n",
    "        if var in globals():\n",
    "            path = globals()[var]\n",
    "            try:\n",
    "                print(f\"[INFO] Trying {var} = {path}\")\n",
    "                df = spark.read.parquet(str(path))\n",
    "                n = normalise_ids(df).count()\n",
    "                print(f\"[CHECK] {var} unique station IDs:\", n)\n",
    "                if n >= 120_000:\n",
    "                    print(f\"[INFO] Using {var} as the unfiltered daily.\")\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not read {var}:\", repr(e))\n",
    "    raise SystemExit(\"[FATAL] Could not find an unfiltered daily dataset (expected ~129k unique station IDs).\")\n",
    "\n",
    "def bprint(text: str=\"\", l=50):\n",
    "    n = len(text)\n",
    "    n = abs(n - l)//2\n",
    "    \n",
    "    print(\"\\n\" + \"_\" * n + text + \"_\" * n)\n",
    "\n",
    "# Back-compat aliases hack to account for non-disciplined naming un-convention\n",
    "_ids       = normalise_ids\n",
    "canon_ids  = normalise_ids\n",
    "_canon_ids = normalise_ids\n",
    "\n",
    "#print(\"[TEST] Using _canon_ids:\", _canon_ids(stations).count())\n",
    "#print(\"[TEST] Using canon_ids :\", canon_ids(stations).count())\n",
    "#print(\"[TEST] Using _ids      :\", _ids(stations).count())\n",
    "\n",
    "\n",
    "# : pairwise city distances in km using Spark built-ins \n",
    "def pairwise_city_distances_spark(cities, radius_km=6371.0):\n",
    "    \"\"\"\n",
    "    cities: list[tuple[str, float, float]] -> [(name, lat_deg, lon_deg), ...]\n",
    "    returns: Spark DataFrame with columns:\n",
    "             city_a, city_b, haversine_km, slc_km, delta_km, delta_pct\n",
    "    \"\"\"\n",
    "  #  from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        raise RuntimeError(\"No active Spark session.\")\n",
    "\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"city\", T.StringType(), False),\n",
    "        T.StructField(\"lat\",  T.DoubleType(), False),\n",
    "        T.StructField(\"lon\",  T.DoubleType(), False),\n",
    "        ])\n",
    "    df = spark.createDataFrame(cities, schema)\n",
    "\n",
    "    a, b = df.alias(\"a\"), df.alias(\"b\")\n",
    "    pairs = (a.join(b, F.col(\"a.city\") < F.col(\"b.city\"))\n",
    "               .select(F.col(\"a.city\").alias(\"city_a\"),\n",
    "                       F.col(\"b.city\").alias(\"city_b\"),\n",
    "                       F.col(\"a.lat\").alias(\"lat1\"),\n",
    "                       F.col(\"a.lon\").alias(\"lon1\"),\n",
    "                       F.col(\"b.lat\").alias(\"lat2\"),\n",
    "                       F.col(\"b.lon\").alias(\"lon2\")))\n",
    "\n",
    "    R = F.lit(float(radius_km))\n",
    "    lat1 = F.radians(F.col(\"lat1\"));  lat2 = F.radians(F.col(\"lat2\"))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = F.radians(F.col(\"lon2\") - F.col(\"lon1\"))\n",
    "\n",
    "    a_term = F.sin(dlat/2)**2 + F.cos(lat1)*F.cos(lat2)*F.sin(dlon/2)**2\n",
    "    c_term = 2*F.atan2(F.sqrt(a_term), F.sqrt(1 - a_term))\n",
    "    hav_km = R * c_term\n",
    "\n",
    "    cos_val = F.sin(lat1)*F.sin(lat2) + F.cos(lat1)*F.cos(lat2)*F.cos(dlon)\n",
    "    cos_val = F.greatest(F.lit(-1.0), F.least(F.lit(1.0), cos_val))\n",
    "    slc_km = R * F.acos(cos_val)\n",
    "\n",
    "    delta_km  = F.abs(hav_km - slc_km)\n",
    "    delta_pct = F.when(hav_km == 0, F.lit(0.0)).otherwise(delta_km / hav_km * 100.0)\n",
    "\n",
    "    out_df = (pairs\n",
    "              .withColumn(\"haversine_km\", F.round(hav_km, 2))\n",
    "              .withColumn(\"slc_km\",       F.round(slc_km, 2))\n",
    "              .withColumn(\"delta_km\",     F.round(delta_km, 4))\n",
    "              .withColumn(\"delta_pct\",    F.round(delta_pct, 6))\n",
    "              .select(\"city_a\", \"city_b\", \"haversine_km\", \"slc_km\", \"delta_km\", \"delta_pct\")\n",
    "              .orderBy(\"haversine_km\"))\n",
    "    return out_df\n",
    "\n",
    "\n",
    "# --- Timing helpers for Spark & pure Python (no extra deps)\n",
    " \n",
    "\n",
    "def benchmark_python_distances(cities, radius_km=6371.0, repeats=50000):\n",
    "    \"\"\"\n",
    "    benchmark:\n",
    "    cities: [(name, lat_deg, lon_deg), ...]  (3 cities => 3 pairs)\n",
    "    repeats: loop count to make timings stable\n",
    "    returns: dict with seconds for haversine/slc\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for i in range(len(cities)):\n",
    "        for j in range(i+1, len(cities)):\n",
    "            (_, lat1, lon1), (_, lat2, lon2) = cities[i], cities[j]\n",
    "            pairs.append((lat1, lon1, lat2, lon2))\n",
    "\n",
    "    # haversine\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        for lat1, lon1, lat2, lon2 in pairs:\n",
    "            Ï†1, Î»1, Ï†2, Î»2 = map(radians, (lat1, lon1, lat2, lon2))\n",
    "            dÏ†, dÎ» = (Ï†2 - Ï†1), (Î»2 - Î»1)\n",
    "            a = sin(dÏ†/2)**2 + cos(Ï†1)*cos(Ï†2)*sin(dÎ»/2)**2\n",
    "            c = 2*atan2(sqrt(a), sqrt(1 - a))\n",
    "            _ = radius_km * c\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # spherical law of cosines (SLC)\n",
    "    t2 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        for lat1, lon1, lat2, lon2 in pairs:\n",
    "            Ï†1, Î»1, Ï†2, Î»2 = map(radians, (lat1, lon1, lat2, lon2))\n",
    "            cosv = sin(Ï†1)*sin(Ï†2) + cos(Ï†1)*cos(Ï†2)*cos(Î»2 - Î»1)\n",
    "            cosv = max(-1.0, min(1.0, cosv))\n",
    "            _ = radius_km * acos(cosv)\n",
    "    t3 = perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"python_haversine_sec\": t1 - t0,\n",
    "        \"python_slc_sec\":       t3 - t2,\n",
    "        \"repeats\": repeats,\n",
    "        \"pairs\": len(pairs),\n",
    "    }\n",
    "\n",
    "\n",
    "def _parse_ls_bytes(line): \n",
    "    parts = line.split()\n",
    "    if len(parts) < 8:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[4])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def _parse_du_bytes(line):\n",
    "    parts = line.split()\n",
    "    if len(parts) < 2:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[0])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def du_bytes(path):\n",
    "    lines = get_ipython().getoutput(f'hdfs dfs -du \"{path}\"')\n",
    "    total = 0\n",
    "    for ln in lines:\n",
    "        parts = ln.split()\n",
    "        if len(parts) >= 2:\n",
    "            try:\n",
    "                total += int(parts[0])\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return total\n",
    "    \n",
    "def benchmark_spark_distances(cities, radius_km=6368.6, repeats=3):\n",
    "    \"\"\"\n",
    "    Uses Spark built-ins only. Measures full execution\n",
    "    time by forcing an action.\n",
    "    \n",
    "    returns: dict with seconds for haversine/slc and\n",
    "    row counts used.\n",
    "    \n",
    "    For the radius:\n",
    "    \n",
    "    The Earth is slightly flattened, so the geocentric \n",
    "    radius depends on latitude.  For context: \n",
    "    \n",
    "    * equatorial radius = 6,378.137 km; \n",
    "    * polar radius      = 6,356.752 km \n",
    "    \n",
    "    Across New Zealandâ€™s latitudes (â‰ˆ36â€“47Â°S), using the\n",
    "    WGS-84 ellipsoid, you get roughly:\n",
    "\n",
    "    Auckland (37Â°S):       ~6,370.4 km\n",
    "    Christchurch (43.5Â°S): ~6,368.0 km\n",
    "    Dunedin (45.9Â°S):      ~6,367.2 km\n",
    "    __________________________________\n",
    "    Wellington (41Â°S):     ~6,369.0 km\n",
    "    mean                  â‰ˆ 6,368.6 km\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        from pyspark.sql import SparkSession, functions as F, types as T\n",
    "    except Exception:\n",
    "        return None  # no Spark therefore save cannot run in vs code\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        return None\n",
    "\n",
    "    # build pairs once and cache\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"city\", T.StringType(), False),\n",
    "        T.StructField(\"lat\",  T.DoubleType(), False),\n",
    "        T.StructField(\"lon\",  T.DoubleType(), False),\n",
    "    ])\n",
    "    df = spark.createDataFrame(cities, schema)\n",
    "    a, b = df.alias(\"a\"), df.alias(\"b\")\n",
    "    pairs = (a.join(b, F.col(\"a.city\") < F.col(\"b.city\"))\n",
    "               .select(F.col(\"a.lat\").alias(\"lat1\"),\n",
    "                       F.col(\"a.lon\").alias(\"lon1\"),\n",
    "                       F.col(\"b.lat\").alias(\"lat2\"),\n",
    "                       F.col(\"b.lon\").alias(\"lon2\"))\n",
    "               .cache())\n",
    "    _ = pairs.count()\n",
    "\n",
    "    R = F.lit(float(radius_km))\n",
    "    lat1 = F.radians(F.col(\"lat1\")); lat2 = F.radians(F.col(\"lat2\"))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = F.radians(F.col(\"lon2\") - F.col(\"lon1\"))\n",
    "\n",
    "    # Haversine expr\n",
    "    a_term = F.sin(dlat/2)**2 + F.cos(lat1)*F.cos(lat2)*F.sin(dlon/2)**2\n",
    "    c_term = 2*F.atan2(F.sqrt(a_term), F.sqrt(1 - a_term))\n",
    "    hav    = R * c_term\n",
    "\n",
    "    # SLC expr\n",
    "    cosv = F.sin(lat1)*F.sin(lat2) + F.cos(lat1)*F.cos(lat2)*F.cos(dlon)\n",
    "    cosv = F.greatest(F.lit(-1.0), F.least(F.lit(1.0), cosv))\n",
    "    slc = R * F.acos(cosv)\n",
    "\n",
    "    # time Haversine\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(hav.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # time SLC\n",
    "    t2 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(slc.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t3 = perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"spark_pairs\": pairs.count(),\n",
    "        \"spark_repeats\": repeats,\n",
    "        \"spark_haversine_sec\": t1 - t0,\n",
    "        \"spark_slc_sec\":       t3 - t2,\n",
    "    }\n",
    "\n",
    "def visualise_directory_tree(root_path, max_depth=1):\n",
    "    \"\"\"\n",
    "    Visualise directory tree structure using Rich library for enhanced display.\n",
    "    Falls back to simple print if Rich is not available.\n",
    "    Shows immediate children with * for subdirectories.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'Tree' not in globals() or Tree is None or 'console' not in globals() or console is None:\n",
    "            print(f\"[INFO] Rich library not available. Directory root: {root_path}\")\n",
    "            return\n",
    "    except NameError:\n",
    "        print(f\"[INFO] Rich library not available. Directory root: {root_path}\")\n",
    "        return\n",
    "    \n",
    "    def build_tree(path, tree, depth=0):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "        try:\n",
    "            # For remote paths, use hdfs dfs -ls\n",
    "            if path.startswith(\"wasbs://\"):\n",
    "                cmd = f\"hdfs dfs -ls {path}\"\n",
    "                result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "                if result.returncode == 0:\n",
    "                    lines = result.stdout.strip().split('\\n')\n",
    "                    for line in lines:\n",
    "                        if line:\n",
    "                            parts = line.split()\n",
    "                            if len(parts) >= 8:\n",
    "                                item_type = parts[0][0]  # d for directory, - for file\n",
    "                                full_name = ' '.join(parts[7:])\n",
    "                                # Extract relative name from full path\n",
    "                                if full_name.startswith(path):\n",
    "                                    name = full_name[len(path):].lstrip('/')\n",
    "                                else:\n",
    "                                    name = full_name\n",
    "                                if item_type == 'd':\n",
    "                                    tree.add(f\"ðŸ“ {name}/*\")\n",
    "                                else:\n",
    "                                    tree.add(f\"ðŸ“„ {name}\")\n",
    "                else:\n",
    "                    tree.add(f\"[ERROR] Could not list {path}\")\n",
    "            else:\n",
    "                # For local paths\n",
    "                for item in os.listdir(path):\n",
    "                    item_path = os.path.join(path, item)\n",
    "                    if os.path.isdir(item_path):\n",
    "                        tree.add(f\"ðŸ“ {item}/*\")\n",
    "                    else:\n",
    "                        tree.add(f\"ðŸ“„ {item}\")\n",
    "        except Exception as e:\n",
    "            tree.add(f\"[ERROR] {str(e)}\")\n",
    "    \n",
    "    tree = Tree(f\"ðŸ“ {root_path}\")\n",
    "    build_tree(root_path, tree)\n",
    "    console.print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf14ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b263df7f-2c3c-49f7-b921-8be60dc8f623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___________SECTION 1: ENVIRONMENT SETUP___________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__________________________________________________\n",
      "[time] current time           :  2025.10.01 19:23\n",
      "\n",
      "__________________________________________________\n",
      "\n",
      "___________________ENVIRONMENT___________________\n",
      "Spark       : 3.5.1\n",
      "Python tuple: (3, 8, 10)\n",
      "username    : dew59\n",
      "\n",
      "\n",
      "_________________DEEBUG BOOLEANS_________________\n",
      "[status] FORCE_REBUILD_ENRICHED  : True\n",
      "[status] FORCE_REBUILD_INV_AGG   : True\n",
      "[status] FORCE_REBUILD_STATIONS  : True\n",
      "[status] FORCE_REBUILD_INVENTORY : True\n",
      "[status] FORCE_REBUILD_STATES    : True\n",
      "[status] FORCE_REBUILD_COUNTRIES : True\n",
      "[status] FORCE_REBUILD_OVERLAP   : True\n",
      "[status] FORCE_REBUILD_PRECIP    : True\n",
      "\n",
      "__________________SOURCE FOLDERS__________________\n",
      "\n",
      "azure_account_name        : madsstorage002\n",
      "azure_data_container_name : campus-data\n",
      "azure_user_container_name : campus-user\n",
      "previous_year             : 2024\n",
      "most_recent_year          : 2025\n",
      "\n",
      "data_root           : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/\n",
      "user_root           : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/\n",
      "\n",
      "__________________________________________________\n",
      "daily_root          : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/\n",
      "\n",
      "aux_root    : ../auxiliary/\n",
      "reports_dir : ../auxiliary/reports/\n",
      "images_dir  : ../auxiliary/images/\n",
      "figures_dir : ../auxiliary/figures/\n",
      "\n",
      "\n",
      "___________________SOURCE FILES___________________\n",
      "stations_read_name  : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-stations.txt\n",
      "inventory_read_name : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-inventory.txt\n",
      "countries_read_name : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-countries.txt\n",
      "states_read_name    : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-states.txt\n",
      "\n",
      "previous_csvgz_path  : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2024.csv.gz\n",
      "current_csvgz_path   : wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2025.csv.gz\n",
      "\n",
      "\n",
      "___________________USER FOLDERS___________________\n",
      "\n",
      "stations_write_name  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations.parquet/\n",
      "inventory_write_name : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet/\n",
      "countries_write_name : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/countries.parquet/\n",
      "states_write_name    : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/states.parquet/\n",
      "inv_agg_write_name   : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inv_agg.parquet/\n",
      "\n",
      "enriched_write_name : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/enriched_write_name.parquet/\n",
      "enriched_write_name : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/enriched_write_name.parquet/\n",
      "inv_agg_write_name  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inv_agg.parquet/\n",
      "\n",
      "\n",
      "stations_write_name : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations.parquet/\n",
      "overlap_counts_name : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/q1b32_overlap_counts.parquet/\n",
      "overlap_write_name  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/q1b32_overlap_counts.parquet/\n",
      "precip_write_name   : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/q2a-agg-precipitation.parquet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "bprint(\"SECTION 1: ENVIRONMENT SETUP\")\n",
    "# supports: Assignment Setup â€” \"Configure global variables and paths required for GHCN data processing\"\n",
    "# does: initializes runtime tracking, configures Azure storage paths, defines data locations,\n",
    "#       establishes file paths, and sets debug flags for conditional processing\n",
    "\n",
    "notebook_run_time = time.time()\n",
    "val               = spark.range(1).select(F.date_format(F.current_timestamp(), 'yyyy.MM.dd HH:mm').alias('t')).first()['t']\n",
    "bprint()\n",
    "print(f\"[time] current time           :  {val}\")\n",
    "bprint()\n",
    "\n",
    "\n",
    "bprint(\"ENVIRONMENT\") \n",
    "print(\"Spark       :\", spark.version)\n",
    "print(\"Python tuple:\", sys.version_info[:3]) \n",
    "print(\"username    :\", username)\n",
    "print()\n",
    "\n",
    "bprint(\"DEEBUG BOOLEANS\")\n",
    "#FORCE_OVERWRITE = False  # False means that if the file exists then we wont re-write it \n",
    "#FORCE_OVERWRITE = True   # True means overwrite all resultant files\n",
    "FORCE_REBUILD_ENRICHED  = True   #has_parquet(enriched_write_name)\n",
    "FORCE_REBUILD_INV_AGG = True    # has_parquet(inv_agg_write_name)\n",
    "\n",
    "FORCE_REBUILD_STATIONS  = True    #has_parquet(stations_write_name)\n",
    "FORCE_REBUILD_INVENTORY = True    # has_parquet(inventory_write_name)\n",
    "FORCE_REBUILD_STATES    = True    #has_parquet(states_write_name)\n",
    "FORCE_REBUILD_COUNTRIES = True    #has_parquet(countries_write_name)\n",
    "FORCE_REBUILD_INV_AGG = True    # has_parquet(inv_agg_write_name)\n",
    "\n",
    "FORCE_REBUILD_OVERLAP   = True    #has_parquet(overlap_write_name)\n",
    "FORCE_REBUILD_PRECIP    = True    #has_parquet(precip_write_path)\n",
    "\n",
    "print(f\"[status] FORCE_REBUILD_ENRICHED  :\", FORCE_REBUILD_ENRICHED)\n",
    "print(f\"[status] FORCE_REBUILD_INV_AGG   :\", FORCE_REBUILD_INV_AGG)\n",
    "print(f\"[status] FORCE_REBUILD_STATIONS  :\", FORCE_REBUILD_STATIONS)\n",
    "print(f\"[status] FORCE_REBUILD_INVENTORY :\", FORCE_REBUILD_INVENTORY)\n",
    "print(f\"[status] FORCE_REBUILD_STATES    :\", FORCE_REBUILD_STATES)\n",
    "print(f\"[status] FORCE_REBUILD_COUNTRIES :\", FORCE_REBUILD_COUNTRIES)\n",
    "\n",
    "print(f\"[status] FORCE_REBUILD_OVERLAP   :\", FORCE_REBUILD_OVERLAP)\n",
    "print(f\"[status] FORCE_REBUILD_PRECIP    :\", FORCE_REBUILD_PRECIP)\n",
    "\n",
    "bprint(\"SOURCE FOLDERS\")\n",
    "print()\n",
    "\n",
    "azure_account_name        = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "previous_year             = 2024  # full hear\n",
    "most_recent_year          = 2025  # currently building\n",
    "\n",
    "\n",
    "print(\"azure_account_name        :\", azure_account_name)\n",
    "print(\"azure_data_container_name :\", azure_data_container_name)\n",
    "print(\"azure_user_container_name :\", azure_user_container_name)\n",
    "print(\"previous_year             :\", previous_year)\n",
    "print(\"most_recent_year          :\", most_recent_year)\n",
    "print()\n",
    "\n",
    "data_root      = f\"wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/\"\n",
    "user_root      = f\"wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/{username}/\"\n",
    " \n",
    "data_root      = ensure_dir(data_root)\n",
    "user_root      = ensure_dir(user_root) \n",
    "\n",
    "print(\"data_root           :\", data_root) \n",
    "print(\"user_root           :\", user_root)\n",
    "bprint()\n",
    "\n",
    "daily_root     = ensure_dir(f\"{data_root}daily/\")\n",
    "\n",
    "print(\"daily_root          :\", daily_root)\n",
    "print()\n",
    "\n",
    "aux_root = \"../auxiliary/\"\n",
    "aux_root = ensure_dir(aux_root)\n",
    "\n",
    "reports_dir  = ensure_dir(f\"{aux_root}reports/\")\n",
    "images_dir   = ensure_dir(f\"{aux_root}images/\")\n",
    "figures_dir  = ensure_dir(f\"{aux_root}figures/\") \n",
    "\n",
    "\n",
    "print(\"aux_root    :\", aux_root)\n",
    "print(\"reports_dir :\", reports_dir)\n",
    "print(\"images_dir  :\", images_dir)\n",
    "print(\"figures_dir :\", figures_dir)\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "bprint(\"SOURCE FILES\")\n",
    "stations_read_name   = f'{data_root}ghcnd-stations.txt'\n",
    "inventory_read_name  = f'{data_root}ghcnd-inventory.txt'\n",
    "countries_read_name  = f'{data_root}ghcnd-countries.txt'\n",
    "states_read_name     = f'{data_root}ghcnd-states.txt'\n",
    "\n",
    "\n",
    "print(\"stations_read_name  :\", stations_read_name)\n",
    "print(\"inventory_read_name :\", inventory_read_name)\n",
    "print(\"countries_read_name :\", countries_read_name)\n",
    "print(\"states_read_name    :\", states_read_name)\n",
    "print()\n",
    "\n",
    "previous_csvgz_path  = f'{daily_root}2024.csv.gz' \n",
    "current_csvgz_path   = f'{daily_root}2025.csv.gz' \n",
    "\n",
    "\n",
    "print(\"previous_csvgz_path  :\", previous_csvgz_path)\n",
    "print(\"current_csvgz_path   :\", current_csvgz_path)\n",
    "print()\n",
    "bprint(\"USER FOLDERS\")\n",
    "  \n",
    "stations_write_name  =  ensure_dir(f'{user_root}stations.parquet')      #parquest file referenced by folder\n",
    "inventory_write_name =  ensure_dir(f'{user_root}inventory.parquet')\n",
    "countries_write_name =  ensure_dir(f'{user_root}countries.parquet')\n",
    "states_write_name    =  ensure_dir(f'{user_root}states.parquet') \n",
    "\n",
    "inv_agg_write_name   = ensure_dir(f'{user_root}inv_agg.parquet')\n",
    "enriched_write_name  = ensure_dir(f'{user_root}enriched_write_name.parquet')\n",
    "\n",
    "print()\n",
    "print(\"stations_write_name  :\", stations_write_name)\n",
    "print(\"inventory_write_name :\", inventory_write_name)\n",
    "print(\"countries_write_name :\", countries_write_name)\n",
    "print(\"states_write_name    :\", states_write_name)\n",
    "print(\"inv_agg_write_name   :\", inv_agg_write_name)\n",
    "print()\n",
    "print(\"enriched_write_name :\", enriched_write_name)\n",
    "print(\"enriched_write_name :\", enriched_write_name)\n",
    "\n",
    "#overlap_write_pathh  = ensure_dir(f'{user_root}q1b32_overlap_counts.parquet')\n",
    "#precip_write_path    = ensure_dir(f'{user_root}q2a-agg-precipitation.parquet')\n",
    " \n",
    "print(\"inv_agg_write_name  :\", inv_agg_write_name)\n",
    "station_date_element = ensure_dir(f\"{user_root}q2a_station_date_element.parquet\")\n",
    "overlap_counts_name  = ensure_dir(f'{user_root}q1b32_overlap_counts.parquet')\n",
    "overlap_write_name   = ensure_dir(f'{user_root}q1b32_overlap_counts.parquet')\n",
    "precip_write_name    = ensure_dir(f'{user_root}q2a-agg-precipitation.parquet') \n",
    "print()\n",
    "\n",
    "inv_agg_write_name   = ensure_dir(f'{user_root}inv_agg.parquet') \n",
    "\n",
    "print()\n",
    "print(\"stations_write_name :\", stations_write_name)\n",
    "print(\"overlap_counts_name :\", overlap_counts_name)\n",
    "print(\"overlap_write_name  :\", overlap_write_name)\n",
    "print(\"precip_write_name   :\", precip_write_name) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45324309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Rich library not available. Directory root: wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/\n",
      "[INFO] Rich library not available. Directory root: wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Visualise directory trees for data and user roots\n",
    "visualise_directory_tree(data_root)\n",
    "visualise_directory_tree(user_root) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de89a186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__________SECTION 1.1: DATA PREPARATION__________\n",
      "\n",
      "[check] marker  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations.parquet/_SUCCESS\n",
      "[check] rc: 0 -> exists\n",
      "\n",
      "[check] marker  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet/_SUCCESS\n",
      "[check] rc: 0 -> exists\n",
      "\n",
      "[check] marker  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/states.parquet/_SUCCESS\n",
      "[check] rc: 0 -> exists\n",
      "\n",
      "[check] marker  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/countries.parquet/_SUCCESS\n",
      "[check] rc: 0 -> exists\n",
      "\n",
      "[check] marker  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inv_agg.parquet/_SUCCESS\n",
      "[check] rc: 0 -> exists\n",
      "\n",
      "[check] marker  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/enriched_write_name.parquet/_SUCCESS\n",
      "[check] rc: 0 -> exists\n"
     ]
    }
   ],
   "source": [
    "bprint(\"SECTION 1.1: DATA PREPARATION\")\n",
    "# does: builds the has_parquet() variables) \n",
    " \n",
    "has_stations  = has_parquet(stations_write_name)\n",
    "has_inventory = has_parquet(inventory_write_name)\n",
    "has_states    = has_parquet(states_write_name)\n",
    "has_countries = has_parquet(countries_write_name)\n",
    "\n",
    "has_inv_agg   = has_parquet(inv_agg_write_name)\n",
    "has_enriched  = has_parquet(enriched_write_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4de0a779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____SECTION 5: UNIFIED PARQUET BUILD & LOAD_____\n",
      "[rebuild] Countries - reading source and rebuilding...\n",
      "[file] write_parquet  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/countries.parquet/\n",
      "\n",
      "__________________________________________________\n",
      "name :  \n",
      "root\n",
      " |-- CODE: string (nullable = true)\n",
      " |-- COUNTRY_NAME: string (nullable = true)\n",
      "\n",
      "[check] sample:\n",
      "[cathch] sample failed: An error occurred while calling o279.limit. Trace:\n",
      "py4j.Py4JException: Method limit([class java.lang.String]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 19:23:22,698 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/countries.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/01 19:23:25 WARN AzureFileSystemThreadPoolExecutor: Disabling threads for Delete operation as thread count 0 is <= 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 dew59 supergroup          0 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/countries.parquet/_SUCCESS\n",
      "-rw-r--r--   1 dew59 supergroup       4080 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/countries.parquet/part-00000-8f156b4b-aa1c-4be3-91a3-7c83c789b824-c000.snappy.parquet\n",
      "[time] write_parquet (min)   :  0.11\n",
      "[time] write_parquet (sec)   :  6.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loaded] countries from parquet: 219 rows\n",
      "[rebuild] States - reading source and rebuilding...\n",
      "[file] write_parquet  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/states.parquet/\n",
      "\n",
      "__________________________________________________\n",
      "name :  \n",
      "root\n",
      " |-- CODE: string (nullable = true)\n",
      " |-- STATE_NAME: string (nullable = true)\n",
      "\n",
      "[check] sample:\n",
      "[cathch] sample failed: An error occurred while calling o302.limit. Trace:\n",
      "py4j.Py4JException: Method limit([class java.lang.String]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 19:23:31,712 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/states.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/01 19:23:33 WARN AzureFileSystemThreadPoolExecutor: Disabling threads for Delete operation as thread count 0 is <= 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 dew59 supergroup          0 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/states.parquet/_SUCCESS\n",
      "-rw-r--r--   1 dew59 supergroup       1879 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/states.parquet/part-00000-1f4f2ca0-126c-45f5-af93-cf83b06027b9-c000.snappy.parquet\n",
      "[time] write_parquet (min)   :  0.08\n",
      "[time] write_parquet (sec)   :  4.81\n",
      "[loaded] states from parquet: 74 rows\n",
      "[rebuild] Stations - reading source and rebuilding...\n",
      "[file] write_parquet  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations.parquet/\n",
      "\n",
      "__________________________________________________\n",
      "name :  \n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- GSN_FLAG: string (nullable = true)\n",
      " |-- HCNCRN_FLAG: string (nullable = true)\n",
      " |-- WMO_ID: string (nullable = true)\n",
      "\n",
      "[check] sample:\n",
      "[cathch] sample failed: An error occurred while calling o357.limit. Trace:\n",
      "py4j.Py4JException: Method limit([class java.lang.String]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 19:23:37,296 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/01 19:23:39 WARN AzureFileSystemThreadPoolExecutor: Disabling threads for Delete operation as thread count 0 is <= 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 dew59 supergroup          0 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations.parquet/_SUCCESS\n",
      "-rw-r--r--   1 dew59 supergroup    1330337 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations.parquet/part-00001-01bc187e-7c7b-48ad-be77-9f78c3be4cf3-c000.snappy.parquet\n",
      "-rw-r--r--   1 dew59 supergroup     909343 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations.parquet/part-00002-01bc187e-7c7b-48ad-be77-9f78c3be4cf3-c000.snappy.parquet\n",
      "-rw-r--r--   1 dew59 supergroup    1320264 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations.parquet/part-00000-01bc187e-7c7b-48ad-be77-9f78c3be4cf3-c000.snappy.parquet\n",
      "[time] write_parquet (min)   :  0.09\n",
      "[time] write_parquet (sec)   :  5.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loaded] stations from parquet: 129,657 rows\n",
      "[rebuild] Inventory - reading source and rebuilding...\n",
      "[file] write_parquet  : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet/\n",
      "\n",
      "__________________________________________________\n",
      "name :  \n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEMENT: string (nullable = true)\n",
      " |-- FIRSTYEAR: integer (nullable = true)\n",
      " |-- LASTYEAR: integer (nullable = true)\n",
      "\n",
      "[check] sample:\n",
      "[cathch] sample failed: An error occurred while calling o397.limit. Trace:\n",
      "py4j.Py4JException: Method limit([class java.lang.String]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 19:23:44,592 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/01 19:23:47 WARN AzureFileSystemThreadPoolExecutor: Disabling threads for Delete operation as thread count 0 is <= 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 dew59 supergroup    1009903 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet/part-00001-a6ffa03e-5c0c-4344-b7d0-2a32e61afee0-c000.snappy.parquet\n",
      "-rw-r--r--   1 dew59 supergroup     470447 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet/part-00006-a6ffa03e-5c0c-4344-b7d0-2a32e61afee0-c000.snappy.parquet\n",
      "-rw-r--r--   1 dew59 supergroup     998728 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet/part-00000-a6ffa03e-5c0c-4344-b7d0-2a32e61afee0-c000.snappy.parquet\n",
      "-rw-r--r--   1 dew59 supergroup     961163 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet/part-00003-a6ffa03e-5c0c-4344-b7d0-2a32e61afee0-c000.snappy.parquet\n",
      "-rw-r--r--   1 dew59 supergroup      71677 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet/part-00007-a6ffa03e-5c0c-4344-b7d0-2a32e61afee0-c000.snappy.parquet\n",
      "-rw-r--r--   1 dew59 supergroup     940273 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet/part-00002-a6ffa03e-5c0c-4344-b7d0-2a32e61afee0-c000.snappy.parquet\n",
      "-rw-r--r--   1 dew59 supergroup          0 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet/_SUCCESS\n",
      "-rw-r--r--   1 dew59 supergroup     473226 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet/part-00004-a6ffa03e-5c0c-4344-b7d0-2a32e61afee0-c000.snappy.parquet\n",
      "-rw-r--r--   1 dew59 supergroup     475699 2025-10-01 19:23 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/inventory.parquet/part-00005-a6ffa03e-5c0c-4344-b7d0-2a32e61afee0-c000.snappy.parquet\n",
      "[time] write_parquet (min)   :  0.11\n",
      "[time] write_parquet (sec)   :  6.87\n",
      "[loaded] inventory from parquet: 766,784 rows\n",
      "\n",
      "\n",
      "__________________FINAL SUMMARY__________________\n",
      "[final] countries :      219 rows\n",
      "[final] states    :       74 rows\n",
      "[final] stations  :  129,657 rows\n",
      "[final] inventory :  766,784 rows\n",
      "[time] Total cell time (sec):  30.57\n",
      "[time] Total cell time (min):   0.51\n",
      "[time] notebook_run_time (min):  0.78\n"
     ]
    }
   ],
   "source": [
    "bprint(\"SECTION 5: UNIFIED PARQUET BUILD & LOAD\")\n",
    "# One cell to rule them all: build parquet files if needed, then load clean DataFrames\n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "# ===== COUNTRIES =====\n",
    "if FORCE_REBUILD_COUNTRIES or not has_countries:\n",
    "    print(\"[rebuild] Countries - reading source and rebuilding...\")\n",
    "    read_countries = spark.read.text(countries_read_name)\n",
    "    countries_temp = (\n",
    "        read_countries.select(\n",
    "            F.substring(\"value\", 1, 2).alias(\"CODE\"),\n",
    "            F.trim(F.substring(\"value\", 4, 61)).alias(\"COUNTRY_NAME\")\n",
    "        )\n",
    "    )\n",
    "    write_parquet(countries_temp, countries_write_name, \"countries\")\n",
    "else:\n",
    "    print(\"[skip] Countries - parquet exists and FORCE_REBUILD_COUNTRIES=False\")\n",
    "\n",
    "# Always read from parquet (the \"clean\" version)\n",
    "countries = spark.read.parquet(countries_write_name)\n",
    "print(f\"[loaded] countries from parquet: {countries.count():,} rows\")\n",
    "\n",
    "# ===== STATES =====\n",
    "if FORCE_REBUILD_STATES or not has_states:\n",
    "    print(\"[rebuild] States - reading source and rebuilding...\")\n",
    "    read_states = spark.read.text(states_read_name)\n",
    "    states_temp = (\n",
    "        read_states.select(\n",
    "            F.substring(\"value\", 1, 2).alias(\"CODE\"),\n",
    "            F.trim(F.substring(\"value\", 4, 47)).alias(\"STATE_NAME\")\n",
    "        )\n",
    "    )\n",
    "    write_parquet(states_temp, states_write_name, \"states\")\n",
    "else:\n",
    "    print(\"[skip] States - parquet exists and FORCE_REBUILD_STATES=False\")\n",
    "\n",
    "# Always read from parquet\n",
    "states = spark.read.parquet(states_write_name)\n",
    "print(f\"[loaded] states from parquet: {states.count():,} rows\")\n",
    "\n",
    "# ===== STATIONS =====\n",
    "if FORCE_REBUILD_STATIONS or not has_stations:\n",
    "    print(\"[rebuild] Stations - reading source and rebuilding...\")\n",
    "    read_stations = spark.read.text(stations_read_name)\n",
    "    stations_temp = (\n",
    "        read_stations.select(\n",
    "            F.trim(F.substring(\"value\",  1, 11)).alias(\"ID\"),\n",
    "            F.trim(F.substring(\"value\", 13,  8)).cast(\"double\").alias(\"LATITUDE\"),\n",
    "            F.trim(F.substring(\"value\", 22,  9)).cast(\"double\").alias(\"LONGITUDE\"),\n",
    "            F.trim(F.substring(\"value\", 32,  6)).cast(\"double\").alias(\"ELEVATION\"),\n",
    "            F.trim(F.substring(\"value\", 39,  2)).alias(\"STATE\"),\n",
    "            F.trim(F.substring(\"value\", 42, 30)).alias(\"NAME\"),\n",
    "            F.trim(F.substring(\"value\", 73,  3)).alias(\"GSN_FLAG\"),\n",
    "            F.trim(F.substring(\"value\", 77,  3)).alias(\"HCNCRN_FLAG\"),\n",
    "            F.trim(F.substring(\"value\", 81,  5)).alias(\"WMO_ID\")\n",
    "        )\n",
    "    )\n",
    "    write_parquet(stations_temp, stations_write_name, \"stations\")\n",
    "else:\n",
    "    print(\"[skip] Stations - parquet exists and FORCE_REBUILD_STATIONS=False\")\n",
    "\n",
    "# Always read from parquet\n",
    "stations = spark.read.parquet(stations_write_name)\n",
    "print(f\"[loaded] stations from parquet: {stations.count():,} rows\")\n",
    "\n",
    "# ===== INVENTORY =====\n",
    "if FORCE_REBUILD_INVENTORY or not has_inventory:\n",
    "    print(\"[rebuild] Inventory - reading source and rebuilding...\")\n",
    "    read_inventory = spark.read.text(inventory_read_name)\n",
    "    inventory_temp = (\n",
    "        read_inventory.select(\n",
    "            F.substring(\"value\",  1, 11).alias(\"ID\"),\n",
    "            F.trim(F.substring(\"value\", 13,  8)).cast(\"double\").alias(\"LATITUDE\"),\n",
    "            F.trim(F.substring(\"value\", 22,  9)).cast(\"double\").alias(\"LONGITUDE\"),\n",
    "            F.substring(\"value\", 32,  4).alias(\"ELEMENT\"),\n",
    "            F.substring(\"value\", 37,  4).cast(\"int\").alias(\"FIRSTYEAR\"),\n",
    "            F.substring(\"value\", 42,  4).cast(\"int\").alias(\"LASTYEAR\")\n",
    "        )\n",
    "    )\n",
    "    write_parquet(inventory_temp, inventory_write_name, \"inventory\")\n",
    "else:\n",
    "    print(\"[skip] Inventory - parquet exists and FORCE_REBUILD_INVENTORY=False\")\n",
    "\n",
    "# Always read from parquet\n",
    "inventory = spark.read.parquet(inventory_write_name)\n",
    "print(f\"[loaded] inventory from parquet: {inventory.count():,} rows\")\n",
    " \n",
    "\n",
    "print()\n",
    "bprint(\"FINAL SUMMARY\")\n",
    "print(f\"[final] countries : {countries.count():8,d} rows\")\n",
    "print(f\"[final] states    : {states.count():8,d} rows\") \n",
    "print(f\"[final] stations  : {stations.count():8,d} rows\")\n",
    "print(f\"[final] inventory : {inventory.count():8,d} rows\")\n",
    "\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Total cell time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] Total cell time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc446981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "______________Process Answer: Q2(b)______________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'daily_for_overlap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m bprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess Answer: Q2(b)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m cell_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m prcp_days_per_station \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mdaily_for_overlap\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter((F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mELEMENT\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRCP\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m&\u001b[39m (F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull()))\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39magg(F\u001b[38;5;241m.\u001b[39mcountDistinct(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdays_with_prcp\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m prcp_days_per_station\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     12\u001b[0m cell_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m cell_time\n",
      "\u001b[0;31mNameError\u001b[0m: name 'daily_for_overlap' is not defined"
     ]
    }
   ],
   "source": [
    "# Q2(b): For each station, count the number of days with precipitation data\n",
    "bprint(\"Process Answer: Q2(b)\")\n",
    "cell_time = time.time()\n",
    "prcp_days_per_station = (\n",
    "    daily_for_overlap\n",
    "    .filter((F.col(\"ELEMENT\") == \"PRCP\") & (F.col(\"VALUE\").isNotNull()))\n",
    "    .groupBy(\"ID\")\n",
    "    .agg(F.countDistinct(\"DATE\").alias(\"days_with_prcp\"))\n",
    "    .orderBy(\"ID\")\n",
    ")\n",
    "prcp_days_per_station.show(10)\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Q2(b) cell time (sec): {cell_time:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc157611",
   "metadata": {},
   "source": [
    "## Q2(b): For each station, count the number of days with precipitation data\n",
    "\n",
    "**Assignment Question:**\n",
    "For each station, count the number of days with precipitation data (i.e., non-missing PRCP values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a): Aggregate daily precipitation (PRCP) by station and date\n",
    "bprint(\"Process Answer: Q2(a)\")\n",
    "cell_time = time.time()\n",
    "prcp_daily = (\n",
    "    daily_for_overlap\n",
    "    .filter(F.col(\"ELEMENT\") == \"PRCP\")\n",
    "    .groupBy(\"ID\", \"DATE\")\n",
    "    .agg(F.sum(\"VALUE\").alias(\"total_prcp\"))\n",
    "    .orderBy(\"ID\", \"DATE\")\n",
    ")\n",
    "prcp_daily.show(10)\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Q2(a) cell time (sec): {cell_time:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f55dae3",
   "metadata": {},
   "source": [
    "## Q2(a): Aggregate daily precipitation (PRCP) by station and date\n",
    "\n",
    "**Assignment Question:**\n",
    "Aggregate daily precipitation (PRCP) by station and date, producing a table with total precipitation per station per day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee4148",
   "metadata": {},
   "source": [
    "## Question 2(a) and 2(b)\n",
    "**Q2(a):** Aggregate daily precipitation (PRCP) by station and date, producing a table with total precipitation per station per day.\n",
    "**Q2(b):** For each station, count the number of days with precipitation data (i.e., non-missing PRCP values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a08dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INV_AGG (requires inventory) =====\n",
    "if FORCE_REBUILD_INV_AGG or not has_inv_agg:\n",
    "    print(\"[rebuild] Inv_agg - building aggregation from inventory...\")\n",
    "    core_elements = [\"PRCP\", \"SNOW\", \"SNWD\", \"TMAX\", \"TMIN\"]\n",
    "    inv_agg_temp = (inventory\n",
    "                   .groupBy(\"ID\")\n",
    "                   .agg(\n",
    "                       F.min(\"FIRSTYEAR\").alias(\"FIRSTYEAR\"),\n",
    "                       F.max(\"LASTYEAR\").alias(\"LASTYEAR\"),\n",
    "                       F.countDistinct(\"ELEMENT\").alias(\"ELEMENT_COUNT\"),\n",
    "                       F.countDistinct(\n",
    "                           F.when(F.col(\"ELEMENT\").isin(core_elements), F.col(\"ELEMENT\"))\n",
    "                       ).alias(\"CORE_ELEMENT_COUNT\"),\n",
    "                       F.countDistinct(\n",
    "                           F.when(~F.col(\"ELEMENT\").isin(core_elements), F.col(\"ELEMENT\"))\n",
    "                       ).alias(\"OTHER_ELEMENT_COUNT\")\n",
    "                   ).orderBy(F.col(\"CORE_ELEMENT_COUNT\").desc(),\n",
    "                            F.col(\"ELEMENT_COUNT\").desc(),\n",
    "                            F.col(\"ID\").asc())\n",
    "                   )\n",
    "    write_parquet(inv_agg_temp, inv_agg_write_name, \"inv_agg\")\n",
    "else:\n",
    "    print(\"[skip] Inv_agg - parquet exists and FORCE_REBUILD_INV_AGG=False\")\n",
    "\n",
    "# Always read from parquet\n",
    "inv_agg = spark.read.parquet(inv_agg_write_name)\n",
    "print(f\"[loaded] inv_agg from parquet: {inv_agg.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c467c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"SECTION 5: ENANCED PARQUET BUILD & LOAD\")\n",
    "# One cell to rule them all: build parquet files if needed, then load clean DataFrames\n",
    "\n",
    "if FORCE_REBUILD_ENRICHED or not has_enriched:\n",
    "    print(\"[rebuild] Enriched - building enriched stations from joins...\")\n",
    "    \n",
    "    # Derive country code from station ID\n",
    "    stations_cc = stations.withColumn(\"COUNTRY_CODE\", F.substring(\"ID\", 1, 2))\n",
    "    \n",
    "    # Join stations with countries\n",
    "    stn_countries = (\n",
    "        stations_cc\n",
    "        .join(countries, stations_cc.COUNTRY_CODE == countries.CODE, \"left\")\n",
    "        .drop(countries.CODE)   # keep COUNTRY_CODE from stations, drop duplicate\n",
    "    )\n",
    "    \n",
    "    # Build enriched by joining with states and inventory aggregation\n",
    "    enriched_temp = (\n",
    "        stn_countries   # already has station + country info\n",
    "        .join(states, stn_countries.STATE == states.CODE, \"left\")\n",
    "        .join(inv_agg, on=\"ID\", how=\"left\")\n",
    "        .orderBy(F.col(\"ID\").asc(), F.col(\"LASTYEAR\").asc(), F.col(\"ELEMENT_COUNT\").asc())\n",
    "    )\n",
    "    \n",
    "    write_parquet(enriched_temp, enriched_write_name, \"enriched\")\n",
    "else:\n",
    "    print(\"[skip] Enriched - parquet exists and FORCE_REBUILD_ENRICHED=False\")\n",
    "\n",
    "# Always read from parquet\n",
    "enriched = spark.read.parquet(enriched_write_name)\n",
    "print(f\"[loaded] enriched from parquet: {enriched.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acbfcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"CSV AND FILE HANDLING\")\n",
    "\n",
    "# Schema for reading CSV files\n",
    "daily_schema = T.StructType([\n",
    "    T.StructField(\"ID\",       T.StringType(), True),\n",
    "    T.StructField(\"DATE\",     T.StringType(), True),  # parsed to DateType below\n",
    "    T.StructField(\"ELEMENT\",  T.StringType(), True),\n",
    "    T.StructField(\"VALUE\",    T.IntegerType(), True),\n",
    "    T.StructField(\"MFLAG\",    T.StringType(), True),\n",
    "    T.StructField(\"QFLAG\",    T.StringType(), True),\n",
    "    T.StructField(\"SFLAG\",    T.StringType(), True),\n",
    "    T.StructField(\"OBSTIME\",  T.StringType(), True),\n",
    "])\n",
    "\n",
    "# CSV reading function\n",
    "def read_csv_with_schema(path: str, schema=None) -> DataFrame:\n",
    "    \"\"\"Read a CSV file using the specified schema.\"\"\"\n",
    "    return (spark.read.csv(\n",
    "        path,\n",
    "        schema=schema or daily_schema,\n",
    "        header=False,\n",
    "        mode=\"PERMISSIVE\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586fca6e-683f-4426-b354-175f8eeb5000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bprint(\"Process A1(a)1\")\n",
    " \n",
    "\n",
    "#Q1(a): Describe the structure of the GHCN-Daily dataset in storage â€” identify the container and the top-level items (e.g., daily/, inventory, stations, countries, states), note naming patterns, and briefly state what each contains (with an example path or two).\n",
    "\n",
    "\n",
    "#!hdfs dfs -ls -h {data_root}\n",
    "notebook_run_time = time.time() \n",
    "print(\"daily_root -> \", daily_root)\n",
    "!hdfs dfs -ls -h {data_root} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e35147-add4-42c6-83e5-7c235be0d4d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bprint(\"Process A1(a)2\")\n",
    "\n",
    "\n",
    "#Q1(a): Describe the structure of the GHCN-Daily dataset in storage â€” identify the container and the top-level items (e.g., daily/, inventory, stations, countries, states), note naming patterns, and briefly state what each contains (with an example path or two).\n",
    "#!hdfs dfs -du -s -h {daily_root} \n",
    "print(f\"daily_root -> {daily_root}\")\n",
    "!hdfs dfs -du -s -h {daily_root} \n",
    "!hdfs dfs -ls    -h {daily_root} \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c741463-9dfb-4c39-ba88-1d35afcecedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Plot A1(c).pie â€” total vs daily\")\n",
    "# supports: Q1(c). Q1(c) (verbatim): \"What is the total size of all of the data, and how much of that is daily?\"\n",
    "\n",
    "#Daily folder size, meta-data size \n",
    "bprint(\"Process A1(c)\")\n",
    "cell_time = time.time() \n",
    "result = get_ipython().getoutput(f\"hdfs dfs -du -s {daily_root}\")\n",
    "print(\"Raw result:\", result)\n",
    "print()\n",
    "daily_size_MByte = int(result[0].split()[0])\n",
    "daily_size_MByte = daily_size_MByte/ (1024**2)\n",
    "daily_size_Bytes = int(result[0].split()[0])\n",
    "print(\"Daily size (bytes):\", daily_size_Bytes)\n",
    "print(\"Daily size (MB)   :\", daily_size_MByte)\n",
    " \n",
    "lines = get_ipython().getoutput(f\"hdfs dfs -ls {data_root}\")\n",
    "print(lines)\n",
    "meta_size_Bytes  = 0\n",
    "other_size_MByte = 0\n",
    "other_size_Mbyte = meta_size_Bytes / (1024**2)\n",
    "for line in lines:\n",
    "    parts = line.split()\n",
    "    if len(parts) >= 6 and parts[0].startswith('-'):   # file, not directory\n",
    "        size = int(parts[2])                           # file size is parts[2]  \n",
    "        meta_size_Bytes += size\n",
    "        \n",
    "print()\n",
    "bprint() \n",
    "print(f\"[result] daily size (bytes): {daily_size_Bytes:,d}\")\n",
    "print(f\"[result] daily size (MB)   : {daily_size_MByte:.2f}\")\n",
    "print(f\"[result] meta-data (bytes) : {meta_size_Bytes:,d}\")\n",
    "print(f\"[result] meta-data (MB)    : {meta_size_Bytes / (1024**2):.2f}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\") \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46ac03-a6e2-4ade-aa56-be09f15807a2",
   "metadata": {},
   "outputs": [],
   "source": [
    " bprint(\"Plot A1(c).pie â€” total vs daily\") \n",
    "# supports: Q1(c). Q1(c) (verbatim): \"What is the total size of all of the data, and how much of that is daily?\"\n",
    "\n",
    "\n",
    "cell_time = time.time()  \n",
    "print(\"current_csvgz_path :\",current_csvgz_path)\n",
    "cmd        = f\"hdfs dfs -ls {current_csvgz_path       }\"\n",
    "result     = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "lines      = result.stdout.strip().split(\"\\n\")\n",
    "rows       = []\n",
    "#print(lines)\n",
    "for line in lines:\n",
    "    print(line)\n",
    "    parts = line.split()\n",
    "    #print(parts)\n",
    "    if len(parts) < 6:\n",
    "        #print(\"continue\")\n",
    "        continue\n",
    "    size = int(parts[2])\n",
    "    path = parts[-1]\n",
    "    if path.endswith(\".csv.gz\"):\n",
    "        year = int(path.split(\"/\")[-1].replace(\".csv.gz\", \"\"))\n",
    "        rows.append((year, size))\n",
    "        #print(year)\n",
    "        \n",
    " \n",
    "bprint()\n",
    "print(\"Sample parsed rows:\", rows[:5])\n",
    "print(\"rows :\",rows)\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad253ff-b9e0-45ba-a3a4-618c071edcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: 1(b)8 indirectly\")\n",
    "# supports: Q1(b). Q1(b) (verbatim): \"How many years are contained in daily, and how does the size of the data change?\" \n",
    "\n",
    "\n",
    "# Build Spark DataFrame with exactly the 2 integer columns  \n",
    "\n",
    "cell_time = time.time()  \n",
    "# Define schema \n",
    "schema = StructType([\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"compressed_bytes\", IntegerType(), True)\n",
    " ])\n",
    "\n",
    "# Create Spark DataFrame with schema\n",
    "year_sizes_df = spark.createDataFrame(rows, schema)\n",
    "\n",
    "print(\"Schema:\")\n",
    "year_sizes_df.printSchema() \n",
    "\n",
    "cell_time =(time.time() - cell_time)  \n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82399ee-c86a-4840-be91-67149fce50d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: 1(b)4\") \n",
    "#  (â€œHow many years are contained in daily, and how does the size of the data change?â€).\n",
    "# Build Spark DataFrame with exactly the 2 integer columns  \n",
    "cell_time = time.time() \n",
    "year_sizes_df.show(10, truncate=False)\n",
    "print(\"Row count:\", year_sizes_df.count())\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c14ad-23d1-4adb-9919-9829c3eff9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"SECTION 2: DATA INGESTION\")\n",
    "# supports: Q1(b). Q1(b) (verbatim): \"How many years are contained in daily, and how does the size of the data change?\"\n",
    "\n",
    "cell_time = time.time()  \n",
    "# â€” build FULL `daily` from all years (wildcard)\n",
    "# Ensure the schema exists (uses your column names, incl. OBSTIME)\n",
    "if \"daily_schema\" not in globals():\n",
    "    daily_schema = T.StructType([\n",
    "        T.StructField(\"ID\",       T.StringType(), True),\n",
    "        T.StructField(\"DATE\",     T.StringType(), True),  # parsed to DateType below\n",
    "        T.StructField(\"ELEMENT\",  T.StringType(), True),\n",
    "        T.StructField(\"VALUE\",    T.IntegerType(), True),\n",
    "        T.StructField(\"MFLAG\",    T.StringType(), True),\n",
    "        T.StructField(\"QFLAG\",    T.StringType(), True),\n",
    "        T.StructField(\"SFLAG\",    T.StringType(), True),\n",
    "        T.StructField(\"OBSTIME\",  T.StringType(), True),\n",
    "    ])\n",
    "\n",
    "print(\"Reading all years:\", f\"{daily_root}*.csv.gz\")\n",
    "print()\n",
    "\n",
    "_df = spark.read.csv(\n",
    "    f\"{daily_root}*.csv.gz\",\n",
    "    schema=daily_schema,\n",
    "    header=False,            # flip to True if your files have a header row\n",
    "    mode=\"PERMISSIVE\"\n",
    ")\n",
    "\n",
    "# Some dumps use STATION instead of ID\n",
    "if \"STATION\" in _df.columns and \"ID\" not in _df.columns:\n",
    "    _df = _df.withColumnRenamed(\"STATION\", \"ID\")\n",
    "\n",
    "daily_for_overlap = (\n",
    "    _df.withColumn(\n",
    "        \"DATE\",\n",
    "        F.coalesce(F.to_date(\"DATE\", \"yyyy-MM-dd\"),\n",
    "                   F.to_date(\"DATE\", \"yyyyMMdd\"))\n",
    "    )\n",
    "    .withColumn(\"ID\", F.upper(F.trim(F.col(\"ID\"))))\n",
    "    .select(\"ID\", \"DATE\", \"ELEMENT\", \"VALUE\", \"MFLAG\", \"QFLAG\", \"SFLAG\", \"OBSTIME\")\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "# Touch to materialise cache\n",
    "_ = daily_for_overlap.limit(1).count()\n",
    "\n",
    "show_df(daily_for_overlap.limit(10), name=\"daily (full, wildcard)\")\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8cade-6a39-48f2-a050-dffecad17f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: 1(b)6\")\n",
    "# supports: Q1(b) (robust HDFS parsing variant, \"NEW -ER\", for year/size extraction). Q1(b) (verbatim): \"How many years are contained in daily, and how does the size of the data change?\"\n",
    "\n",
    "\n",
    "cell_time = time.time() \n",
    "rows        = []\n",
    "# NOTE:  -du with a files-only --- size + path are stable (behaves like the GOOD run)\n",
    "lines       = get_ipython().getoutput(f'hdfs dfs -du \"{data_root}/ghcnd-*.txt\"')\n",
    "print(lines)\n",
    "for line in lines:                 # <-- was lines[15:] (skipped everything)\n",
    "    #print()\n",
    "    parts = line.split()\n",
    "    #print(line)\n",
    "    #print(parts)\n",
    "    #print(len(parts))\n",
    "    #print(parts[0])\n",
    "\n",
    "    if len(parts) >= 2:\n",
    "        size = int(parts[0])                 # bytes from `hdfs dfs -du`\n",
    "        path = parts[-1].strip()             # full path\n",
    "        #print(\"size:\",size)\n",
    "        print(path)\n",
    "        # if not path.startswith(daily_root):   # files-only glob excludes /daily already\n",
    "        rows.append((path, size))             # not compressed\n",
    "\n",
    "print(\"\\nMetadata file count:\", len(rows))\n",
    "print(\"Sample parsed rows:\", rows[:5])\n",
    "# Spark schema\n",
    "schema = StructType([\n",
    "    StructField(\"path\", StringType(), False),\n",
    "    StructField(\"uncompressed_bytes\", LongType(), False),\n",
    "])\n",
    "\n",
    "metadata_files_df = spark.createDataFrame(rows, schema)\n",
    "\n",
    "print(\"\\n[spark] other_files_df schema:\")\n",
    "metadata_files_df.printSchema()\n",
    "print(\"[spark] sample:\")\n",
    "metadata_files_df.show( truncate=False)\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dba4c5-b87a-4f37-9939-a2c89d5417a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Q1(b)25\") \n",
    "\n",
    "has_enriched  = has_parquet(enriched_write_name)\n",
    "has_stations  = has_parquet(stations_write_name)\n",
    "has_inventory = has_parquet(inventory_write_name)\n",
    "has_states    = has_parquet(states_write_name)\n",
    "has_countries = has_parquet(countries_write_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a6fb6-ac8c-46c5-a30c-cee9f136b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: 1(c)2 â€” dataset sizes (HDFS) + est. uncompressed daily\")\n",
    "# supports: Q1(c). Q1(c) (verbatim): \"What is the total size of all of the data, and how much of that is daily?\"\n",
    "\n",
    "cell_time = time.time()\n",
    "# \n",
    "print(f\"daily_root          : {daily_root}\")\n",
    "print(f\"inventory_read_name : {inventory_read_name}\")\n",
    "print(f\"stations_read_name  : {stations_read_name}\")\n",
    "print(f\"countries_read_name : {countries_read_name}\")\n",
    "print(f\"states_read_name    : {states_read_name}\")\n",
    "sizes = {\n",
    "    \"daily (folder)\":      du_bytes(daily_root),\n",
    "    \"ghcnd-inventory.txt\": du_bytes(inventory_read_name),\n",
    "    \"ghcnd-stations.txt\":  du_bytes(stations_read_name),\n",
    "    \"ghcnd-countries.txt\": du_bytes(countries_read_name),\n",
    "    \"ghcnd-states.txt\":    du_bytes(states_read_name),\n",
    "}\n",
    "total_bytes = sum(sizes.values())\n",
    "\n",
    "# Simple gzip expansion estimate  \n",
    "gzip_expansion_factor = 3.3\n",
    "est_uncomp_daily = int(sizes[\"daily (folder)\"] * gzip_expansion_factor)\n",
    "\n",
    "print(\"[status] gzip_expansion_factor ->\", gzip_expansion_factor)\n",
    "print(\"[status] sizes (bytes) ->\", sizes)\n",
    "print(\"[status] total (bytes)  ->\", total_bytes)\n",
    "\n",
    "# Present as a small Spark table (sizes in MB for readability)\n",
    "to_mb = 1024**2\n",
    "rows = []\n",
    "for k, v in sizes.items():\n",
    "    rows.append((k, round(v/to_mb, 2)))\n",
    "rows.append((\"TOTAL\", round(total_bytes/to_mb, 2)))\n",
    "\n",
    "sizes_df = spark.createDataFrame(rows, [\"dataset\", \"size_mb\"])\n",
    "sizes_df.show(truncate=False)\n",
    "\n",
    "print(f\"[status] estimated uncompressed daily (MB): {est_uncomp_daily/to_mb:,.2f}\")\n",
    "\n",
    " \n",
    "user_out = f\"wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/processing\"\n",
    "spark.createDataFrame(\n",
    "    [(k, v, round(v/to_mb,2)) for k, v in sizes.items()] + [(\"TOTAL\", total_bytes, round(total_bytes/to_mb, 2))]\n",
    "    , [\"dataset\",\"size_bytes\",\"size_mb\"]\n",
    ").coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(f\"{user_out}/dew59_sizes_mb_csv\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777255a-e6d1-46bc-8413-83de590650e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: Q2(c)71\")\n",
    "# supports: Q2(c) â€” [awaiting verbatim text] load fixed-width metadata into Spark and extract columns using substring by character ranges.\n",
    "# does: reads the fixed-width STATIONS text via spark.read.text and extracts ID, LATITUDE, LONGITUDE, ELEVATION, STATE, NAME, GSN_FLAG, HCNCRN_FLAG, WMO_ID with F.substring; prints schema/sample.\n",
    "\n",
    "read_stations = spark.read.text(stations_read_name)\n",
    "\n",
    "stations = (\n",
    "    read_stations.select(\n",
    "        F.trim(F.substring(\"value\",  1, 11)).alias(\"ID\"),                 # 1â€“11\n",
    "        F.trim(F.substring(\"value\", 13,  8)).cast(\"double\").alias(\"LATITUDE\"),   # 13â€“20\n",
    "        F.trim(F.substring(\"value\", 22,  9)).cast(\"double\").alias(\"LONGITUDE\"),  # 22â€“30\n",
    "        F.trim(F.substring(\"value\", 32,  6)).cast(\"double\").alias(\"ELEVATION\"),  # 32â€“37\n",
    "        F.trim(F.substring(\"value\", 39,  2)).alias(\"STATE\"),                     # 39â€“40\n",
    "        F.trim(F.substring(\"value\", 42, 30)).alias(\"NAME\"),                      # 42â€“71\n",
    "        F.trim(F.substring(\"value\", 73,  3)).alias(\"GSN_FLAG\"),                  # 73â€“75\n",
    "        F.trim(F.substring(\"value\", 77,  3)).alias(\"HCNCRN_FLAG\"),               # 77â€“79\n",
    "        F.trim(F.substring(\"value\", 81,  5)).alias(\"WMO_ID\")                     # 81â€“85\n",
    "    )\n",
    ")\n",
    "print(\"stations\")\n",
    "stations.printSchema()\n",
    "stations.show(10, truncate=False)\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145003c4",
   "metadata": {},
   "source": [
    "## Q2(c): Extract columns from fixed-width metadata\n",
    "\n",
    "**Assignment Question:**\n",
    "Load fixed-width metadata into Spark and extract columns using substring by character ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb52e415-e5fc-434f-a73b-70d3513db1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"SECTION 3: DATA PROCESSING\")\n",
    "# supports: Q3(aâ€“c) â€” \"Derive country_code and join stations with countries/states.\"\n",
    "# does: parses COUNTRIES fixed-width into [CODE, COUNTRY_NAME]; derives COUNTRY_CODE from station IDs; left-joins stationsâ†”countries to create stn_countries and previews results for verification.\n",
    "\n",
    "# countries\n",
    "read_countries = spark.read.text(countries_read_name)\n",
    "countries = (\n",
    "    read_countries.select(\n",
    "        F.substring(\"value\", 1, 2).alias(\"CODE\"),                # 1â€“2\n",
    "        F.trim(F.substring(\"value\", 4, 61)).alias(\"COUNTRY_NAME\")# 4â€“64\n",
    "    )\n",
    ")\n",
    "countries.show()\n",
    "# derive country code \n",
    "stations_cc = stations.withColumn(\"COUNTRY_CODE\", F.substring(\"ID\", 1, 2))\n",
    "# join country code \n",
    "stn_countries = (\n",
    "    stations_cc\n",
    "    .join(countries, stations_cc.COUNTRY_CODE == countries.CODE, \"left\")\n",
    "    .drop(countries.CODE)   # keep COUNTRY_CODE from stations, drop duplicate\n",
    ")\n",
    "stations_cc.show()\n",
    "stn_countries.show()\n",
    "stn_countries.select(\"ID\",\"NAME\",\"COUNTRY_CODE\",\"COUNTRY_NAME\").show(20, False)\n",
    "\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab41721",
   "metadata": {},
   "source": [
    "## Q3(aâ€“c): Derive country code and join stations with countries/states\n",
    "\n",
    "**Assignment Question:**\n",
    "Derive country_code from station IDs, join stations with countries and states, and preview the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ccb34-1c8a-4478-b0ea-14154463ee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: Q3(aâ€“c)62\")\n",
    "# supports: Q3(aâ€“c) â€” \"Derive country_code and join stations with countries/states.\"\n",
    "# does: reads STATES fixed-width from states_read_name, extracts CODE and STATE_NAME to build the states DataFrame, then prints schema and a sample; preparation for the stationsâ†”states join.\n",
    "\n",
    "# states\n",
    "cell_time = time.time()  \n",
    "read_states = spark.read.text(states_read_name)\n",
    "\n",
    "states = (\n",
    "    read_states.select(\n",
    "        F.substring(\"value\", 1, 2).alias(\"CODE\"),                 # 1â€“2\n",
    "        F.trim(F.substring(\"value\", 4, 47)).alias(\"STATE_NAME\")   # 4â€“50  (length = 47)\n",
    "    )\n",
    ")\n",
    "\n",
    "states.printSchema()\n",
    "states.show(20, truncate=False)\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e313a575-ad27-4a3b-bc91-9d2cf0c58126",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0e93a-329a-43be-b71c-ad795a38ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: Q3(d)60\")  # indirectly\n",
    "# supports: Q3(d) â€” aggregate inventory per station (FIRSTYEAR/LASTYEAR, element counts).\n",
    "# does: parses the fixed-width inventory file (inventory_read_name) into columns [ID, LATITUDE, LONGITUDE, ELEMENT, FIRSTYEAR, LASTYEAR], then prints schema and a sample to verify ingestion before aggregation.\n",
    "\n",
    "\n",
    "cell_time = time.time()   \n",
    "read_inventory = spark.read.text(inventory_read_name)\n",
    "\n",
    "inventory = (\n",
    "    read_inventory.select(\n",
    "        F.substring(\"value\",  1, 11).alias(\"ID\"),                  # 1â€“11\n",
    "        F.trim(F.substring(\"value\", 13,  8)).cast(\"double\").alias(\"LATITUDE\"),   # 13â€“20\n",
    "        F.trim(F.substring(\"value\", 22,  9)).cast(\"double\").alias(\"LONGITUDE\"),  # 22â€“30\n",
    "        F.substring(\"value\", 32,  4).alias(\"ELEMENT\"),             # 32â€“35\n",
    "        F.substring(\"value\", 37,  4).cast(\"int\").alias(\"FIRSTYEAR\"),# 37â€“40\n",
    "        F.substring(\"value\", 42,  4).cast(\"int\").alias(\"LASTYEAR\")  # 42â€“45\n",
    "    )\n",
    ")\n",
    "\n",
    "inventory.printSchema()\n",
    "inventory.show(20, truncate=False)\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44d7e9",
   "metadata": {},
   "source": [
    "## Q3(d): Aggregate inventory per station\n",
    "\n",
    "**Assignment Question:**\n",
    "Aggregate inventory per station (FIRSTYEAR/LASTYEAR, element counts), and verify ingestion before aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3671b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: Q3(aâ€“c)49 \") # indirectly\n",
    "# supports: Q3(aâ€“c) â€” derive country_code and join stations with countries/states.\n",
    "# does: persists the countries table to Parquet (if missing) at countries_write_name, so itâ€™s available for the upcoming stationsâ†”countries/states joins; prints the write path and timing.\n",
    "\n",
    "cell_time = time.time()\n",
    "if(not has_countries):\n",
    "    write_parquet(countries,countries_write_name)\n",
    "    bprint()\n",
    "    print(\"[written] \",countries_write_name)\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: Q3(aâ€“c)50\")\n",
    "# supports: Q3(aâ€“c) â€” derive country_code and join stations with countries/states.\n",
    "# does: writes the STATES table to Parquet at states_write_name when missing (has_states is False) so itâ€™s ready for the upcoming stationsâ†”states joins; prints the write path and timing.\n",
    "\n",
    "cell_time = time.time()\n",
    "if(not has_states):\n",
    "    write_parquet(states,states_write_name)\n",
    "    bprint()\n",
    "    print(\"[written] \",states_write_name)\n",
    "    \n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc66bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    " bprint(\"Process Answer: Q3(d)51\")\n",
    "# supports: Q3(d) â€” aggregate inventory per station (first/last year, element counts) as input to enrichment.\n",
    "# does: writes the INVENTORY table to Parquet at inventory_write_name when missing (has_inventory is False), ensuring inventory is available for the Q3(d) aggregations and later joins.\n",
    "\n",
    "cell_time = time.time() \n",
    "\n",
    "if(not has_inventory):\n",
    "    write_parquet(inventory,inventory_write_name)\n",
    "    bprint()\n",
    "    print(\"[written] \",inventory_write_name)    \n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: Q3(aâ€“c)52\")\n",
    "# supports: Q3(aâ€“c) â€” \"Derive country_code and join stations with countries/states.\"\n",
    "# does: writes the STATIONS table to Parquet at stations_write_name when missing (has_stations is False), ensuring the base stations dataset is persisted for the upcoming joins.\n",
    "\n",
    "cell_time = time.time()\n",
    "if(not has_stations):\n",
    "    write_parquet(stations,stations_write_name)\n",
    "    bprint()\n",
    "    print(\"[written] \",stations_write_name)    \n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] cell_time (sec): {cell_time:6.2f}\")\n",
    "print(f\"[time] cell_time (min): {cell_time/60:6.2f}\")\n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"SECTION 4: INVENTORY AGGREGATION\")\n",
    "# supports: Q2(d) â€” row counts, previews, and schemas for metadata + daily; write a small counts artifact.\n",
    "# does: counts rows for stations/states/countries/inventory/daily_for_overlap, shows a table, writes the counts CSV to your user area, then previews 3 rows and prints schemas for each dataset.\n",
    "\n",
    "cell_time = time.time() \n",
    "\n",
    "core_elements = [\"PRCP\", \"SNOW\", \"SNWD\", \"TMAX\", \"TMIN\"]\n",
    "\n",
    "inv_agg = (inventory\n",
    "           .groupBy(\"ID\")\n",
    "           .agg(\n",
    "               F.min(\"FIRSTYEAR\").alias(\"FIRSTYEAR\"),\n",
    "               F.max(\"LASTYEAR\").alias(\"LASTYEAR\"),\n",
    "               F.countDistinct(\"ELEMENT\").alias(\"ELEMENT_COUNT\"),\n",
    "               F.countDistinct(\n",
    "                   F.when(F.col(\"ELEMENT\").isin(core_elements), F.col(\"ELEMENT\"))\n",
    "               ).alias(\"CORE_ELEMENT_COUNT\"),\n",
    "               F.countDistinct(\n",
    "                   F.when(~F.col(\"ELEMENT\").isin(core_elements), F.col(\"ELEMENT\"))\n",
    "              ).alias(\"OTHER_ELEMENT_COUNT\")\n",
    "                   ).orderBy(F.col(\"CORE_ELEMENT_COUNT\").desc(),\n",
    "                        F.col(\"ELEMENT_COUNT\").desc(),\n",
    "                        F.col(\"ID\").asc())\n",
    "                        )\n",
    "print()\n",
    "bprint()\n",
    "inv_agg.printSchema()\n",
    "inv_agg.show(20, truncate=False)\n",
    "\n",
    "print(f\"[result] Aggregated inventory rows : {inv_agg.count():12,d}\")\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93555ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4(b): How many station IDs are in stations but not in daily?\n",
    "bprint(\"Process Answer: Q4(b)58\")\n",
    "# supports: Q4(b) â€” \"How many station IDs are in stations but not in daily?\"\n",
    "cell_time = time.time()\n",
    "station_ids = stations.select(F.col(\"ID\").alias(\"ID\")).distinct()\n",
    "daily_ids = daily_for_overlap.select(F.col(\"ID\").alias(\"ID\")).distinct()\n",
    "stations_not_in_daily = station_ids.join(daily_ids, on=\"ID\", how=\"left_anti\")\n",
    "count_not_in_daily = stations_not_in_daily.count()\n",
    "print(f\"[result] Station IDs in stations but not in daily: {count_not_in_daily:,}\")\n",
    "stations_not_in_daily.show(10)\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time] Q4(b) cell time (sec): {cell_time:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24350146",
   "metadata": {},
   "source": [
    "## Q4(b): How many station IDs are in stations but not in daily?\n",
    "\n",
    "**Assignment Question:**\n",
    "How many station IDs are in stations but not in daily?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a820d84d",
   "metadata": {},
   "source": [
    "## Q2(d): Inventory aggregation and row counts\n",
    "\n",
    "**Assignment Question:**\n",
    "Aggregate inventory per station (FIRSTYEAR/LASTYEAR, element counts) and provide row counts, previews, and schemas for metadata and daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f3eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: Q3(e)56\")\n",
    "# supports: Q3(e) â€” \"Join aggregated inventory into stations to produce an enriched stations table.\"\n",
    "# does: builds `enriched` by joining station+country (`stn_countries`) with `states` (on STATE=CODE, left) and `inv_agg` (on ID, left); orders rows, prints row count and a preview of key columns, and records timing.\n",
    "\n",
    "cell_time = time.time()\n",
    "enriched = (stn_countries   # already has station + country info\n",
    "            .join(states, stn_countries.STATE == states.CODE, \"left\")\n",
    "            .join(inv_agg, on=\"ID\", how=\"left\")\n",
    "           # ---- order the result (adjust) ----\n",
    "             .orderBy(F.col(\"ID\").asc(), F.col(\"LASTYEAR\").asc(), F.col(\"ELEMENT_COUNT\").asc())\n",
    ")\n",
    "\n",
    "print()\n",
    "bprint()\n",
    "print(f\"[result] Enriched stations rows : {enriched.count():12,d}\")\n",
    "enriched.select(\n",
    "                \"ID\"           ,\"NAME\"    ,\"COUNTRY_NAME\" ,\"STATE_NAME\",\n",
    "                \"FIRSTYEAR\"    ,\"LASTYEAR\",\"ELEMENT_COUNT\",\"CORE_ELEMENT_COUNT\" ,\"OTHER_ELEMENT_COUNT\"\n",
    "               ).show(20, truncate=False)\n",
    "\n",
    "cell_time = time.time() - cell_time \n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")  \n",
    "print(f\"[time] notebook_run_time (min): {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: Q3(e)53\")\n",
    "# supports: Q3(e) â€” join inventory aggregates into stations to produce an enriched stations table.\n",
    "# does: conditionally writes the enriched stations DataFrame to Parquet (if not already present), logging the output path and timing.\n",
    "\n",
    "#build parquet files conditionally \n",
    "cell_time = time.time()\n",
    "if(not has_enriched):\n",
    "    write_parquet(enriched,enriched_write_name)\n",
    "    bprint()\n",
    "    print(\"[written] \",enriched_write_name)\n",
    "    cell_time = time.time()\n",
    "    print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e85b719-2210-442a-b1a6-e5a448208983",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process Answer: Q3(d)57\")\n",
    "# supports: Q3(d) â€” \"Aggregate inventory per station (FIRSTYEAR, LASTYEAR, element counts) and summarise coverage.\"\n",
    "# does: prints row counts for stations, countries, states, and inventory (plus check lines), serving as a sanity/baseline count before inventory-by-station aggregations and later joins.\n",
    "\n",
    "\n",
    "bprint()\n",
    "print(\"Row counts (with inventory):\")\n",
    "print(f\"[result] stations      : {stations.count() :12,d}\")\n",
    "print(f\"[result] countries     : {countries.count():12,d}\")\n",
    "print(f\"[result] states        : {states.count()   :12,d}\")\n",
    "print(f\"[result] inventory     : {inventory.count():12,d}\")\n",
    "print(f\"[check ] stations_cc   : {inventory.count():12,d}\")\n",
    "print(f\"[check ] stn_countries : {inventory.count():12,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2318c1-9ef3-4db9-a5cb-7d028114864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Process A X (11) \")\n",
    "val = spark.range(1).select(F.date_format(F.current_timestamp(), 'yyyy.MM.dd HH:mm').alias('t')).first()['t']\n",
    "cell_time = time.time() - cell_time  \n",
    "print(f\"[time] current time           :  {val}\")\n",
    "print(f\"[time] Cell time (sec)        : {cell_time:6.2f}\") \n",
    "stop_spark()\n",
    "print(f\"[time] Cell time (min)        : {cell_time/60:6.2f}\") \n",
    "print(f\"[time] notebook_run_time (min):  {(time.time() - notebook_run_time)/60:5.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
